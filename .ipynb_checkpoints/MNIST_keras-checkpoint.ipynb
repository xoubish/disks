{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN with keras \n",
    "\n",
    "What I think would be the optimal choice for my problem! Except that this example is a three color and for now I will only need gray scale?! This example is a modified version of goldesel github bird generation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Reshape\n",
    "from keras.layers import Flatten, BatchNormalization, Dense, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is interesting, how much warning info to be shown. ranging from 0 show all to 3 show almost none\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here is where we will load the dataset stored in dataset_path. In this script\n",
    "# we will use the Caltech-UCSD Birds-200-2011 dataset which includes 11788\n",
    "# images from 200 different birds. We will feed the images without applying\n",
    "# the provided bounding boxes from the dataset. The data will only be resized\n",
    "# and normalized. Keras ImageDataGenerator will be used for loading the dataset\n",
    "def load_dataset(dataset_path, batch_size, image_shape):\n",
    "    dataset_generator = ImageDataGenerator()\n",
    "    dataset_generator = dataset_generator.flow_from_directory(\n",
    "        dataset_path, target_size=(image_shape[0], image_shape[1]),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None)\n",
    "\n",
    "    return dataset_generator\n",
    "\n",
    "\n",
    "# Creates the discriminator model. This model tries to classify images as real\n",
    "# or fake.\n",
    "def construct_discriminator(image_shape):\n",
    "\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Conv2D(filters=64, kernel_size=(5, 5),\n",
    "                             strides=(2, 2), padding='same',\n",
    "                             data_format='channels_last',\n",
    "                             kernel_initializer='glorot_uniform',\n",
    "                             input_shape=(image_shape)))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "\n",
    "    discriminator.add(Conv2D(filters=128, kernel_size=(5, 5),\n",
    "                             strides=(2, 2), padding='same',\n",
    "                             data_format='channels_last',\n",
    "                             kernel_initializer='glorot_uniform'))\n",
    "    discriminator.add(BatchNormalization(momentum=0.5))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "\n",
    "    discriminator.add(Conv2D(filters=256, kernel_size=(5, 5),\n",
    "                             strides=(2, 2), padding='same',\n",
    "                             data_format='channels_last',\n",
    "                             kernel_initializer='glorot_uniform'))\n",
    "    discriminator.add(BatchNormalization(momentum=0.5))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "\n",
    "    discriminator.add(Conv2D(filters=512, kernel_size=(5, 5),\n",
    "                             strides=(2, 2), padding='same',\n",
    "                             data_format='channels_last',\n",
    "                             kernel_initializer='glorot_uniform'))\n",
    "    discriminator.add(BatchNormalization(momentum=0.5))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "\n",
    "    discriminator.add(Flatten())\n",
    "    discriminator.add(Dense(1))\n",
    "    discriminator.add(Activation('sigmoid'))\n",
    "\n",
    "    optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "    discriminator.compile(loss='binary_crossentropy',\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=None)\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "\n",
    "# Creates the generator model. This model has an input of random noise and\n",
    "# generates an image that will try mislead the discriminator.\n",
    "def construct_generator():\n",
    "\n",
    "    generator = Sequential()\n",
    "\n",
    "    generator.add(Dense(units=4 * 4 * 512,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        input_shape=(1, 1, 100)))\n",
    "    generator.add(Reshape(target_shape=(4, 4, 512)))\n",
    "    generator.add(BatchNormalization(momentum=0.5))\n",
    "    generator.add(Activation('relu'))\n",
    "\n",
    "    generator.add(Conv2DTranspose(filters=256, kernel_size=(5, 5),\n",
    "                                  strides=(2, 2), padding='same',\n",
    "                                  data_format='channels_last',\n",
    "                                  kernel_initializer='glorot_uniform'))\n",
    "    generator.add(BatchNormalization(momentum=0.5))\n",
    "    generator.add(Activation('relu'))\n",
    "\n",
    "    generator.add(Conv2DTranspose(filters=128, kernel_size=(5, 5),\n",
    "                                  strides=(2, 2), padding='same',\n",
    "                                  data_format='channels_last',\n",
    "                                  kernel_initializer='glorot_uniform'))\n",
    "    generator.add(BatchNormalization(momentum=0.5))\n",
    "    generator.add(Activation('relu'))\n",
    "\n",
    "    generator.add(Conv2DTranspose(filters=64, kernel_size=(5, 5),\n",
    "                                  strides=(2, 2), padding='same',\n",
    "                                  data_format='channels_last',\n",
    "                                  kernel_initializer='glorot_uniform'))\n",
    "    generator.add(BatchNormalization(momentum=0.5))\n",
    "    generator.add(Activation('relu'))\n",
    "\n",
    "    generator.add(Conv2DTranspose(filters=3, kernel_size=(5, 5),\n",
    "                                  strides=(2, 2), padding='same',\n",
    "                                  data_format='channels_last',\n",
    "                                  kernel_initializer='glorot_uniform'))\n",
    "    generator.add(Activation('tanh'))\n",
    "\n",
    "    optimizer = Adam(lr=0.00015, beta_1=0.5)\n",
    "    generator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=None)\n",
    "\n",
    "    return generator\n",
    "\n",
    "\n",
    "# Displays a figure of the generated images and saves them in as .png image\n",
    "def save_generated_images(generated_images, epoch, batch_number):\n",
    "\n",
    "    plt.figure(figsize=(8, 8), num=2)\n",
    "    gs1 = gridspec.GridSpec(8, 8)\n",
    "    gs1.update(wspace=0, hspace=0)\n",
    "\n",
    "    for i in range(64):\n",
    "        ax1 = plt.subplot(gs1[i])\n",
    "        ax1.set_aspect('equal')\n",
    "        image = generated_images[i, :, :, :]\n",
    "        image += 1\n",
    "        image *= 127.5\n",
    "        fig = plt.imshow(image.astype(np.uint8))\n",
    "        plt.axis('off')\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_name = 'generated images/generatedSamples_epoch' + str(\n",
    "        epoch + 1) + '_batch' + str(batch_number + 1) + '.png'\n",
    "\n",
    "    plt.savefig(save_name, bbox_inches='tight', pad_inches=0)\n",
    "    plt.pause(0.0000000001)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Main train function\n",
    "def train_dcgan(batch_size, epochs, image_shape, dataset_path):\n",
    "    # Build the adversarial model that consists in the generator output\n",
    "    # connected to the discriminator\n",
    "    generator = construct_generator()\n",
    "    discriminator = construct_discriminator(image_shape)\n",
    "\n",
    "    gan = Sequential()\n",
    "    # Only false for the adversarial model\n",
    "    discriminator.trainable = False\n",
    "    gan.add(generator)\n",
    "    gan.add(discriminator)\n",
    "\n",
    "    optimizer = Adam(lr=0.00015, beta_1=0.5)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "                metrics=None)\n",
    "\n",
    "    # Create a dataset Generator with help of keras\n",
    "    dataset_generator = load_dataset(dataset_path, batch_size, image_shape)\n",
    "\n",
    "    # 11788 is the total number of images on the bird dataset\n",
    "    number_of_batches = int(11788 / batch_size)\n",
    "\n",
    "    # Variables that will be used to plot the losses from the discriminator and\n",
    "    # the adversarial models\n",
    "    adversarial_loss = np.empty(shape=1)\n",
    "    discriminator_loss = np.empty(shape=1)\n",
    "    batches = np.empty(shape=1)\n",
    "\n",
    "    # Allo plot updates inside for loop\n",
    "    plt.ion()\n",
    "\n",
    "    current_batch = 0\n",
    "\n",
    "    # Let's train the DCGAN for n epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(\"Epoch \" + str(epoch+1) + \"/\" + str(epochs) + \" :\")\n",
    "\n",
    "        for batch_number in range(number_of_batches):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Get the current batch and normalize the images between -1 and 1\n",
    "            real_images = dataset_generator.next()\n",
    "            real_images /= 127.5\n",
    "            real_images -= 1\n",
    "\n",
    "            # The last batch is smaller than the other ones, so we need to\n",
    "            # take that into account\n",
    "            current_batch_size = real_images.shape[0]\n",
    "\n",
    "            # Generate noise\n",
    "            noise = np.random.normal(0, 1,\n",
    "                                     size=(current_batch_size,) + (1, 1, 100))\n",
    "\n",
    "            # Generate images\n",
    "            generated_images = generator.predict(noise)\n",
    "\n",
    "            # Add some noise to the labels that will be\n",
    "            # fed to the discriminator\n",
    "            real_y = (np.ones(current_batch_size) -\n",
    "                      np.random.random_sample(current_batch_size) * 0.2)\n",
    "            fake_y = np.random.random_sample(current_batch_size) * 0.2\n",
    "\n",
    "            # Let's train the discriminator\n",
    "            discriminator.trainable = True\n",
    "\n",
    "            d_loss = discriminator.train_on_batch(real_images, real_y)\n",
    "            d_loss += discriminator.train_on_batch(generated_images, fake_y)\n",
    "\n",
    "            discriminator_loss = np.append(discriminator_loss, d_loss)\n",
    "\n",
    "            # Now it's time to train the generator\n",
    "            discriminator.trainable = False\n",
    "\n",
    "            noise = np.random.normal(0, 1,\n",
    "                                     size=(current_batch_size * 2,) +\n",
    "                                     (1, 1, 100))\n",
    "\n",
    "            # We try to mislead the discriminator by giving the opposite labels\n",
    "            fake_y = (np.ones(current_batch_size * 2) -\n",
    "                      np.random.random_sample(current_batch_size * 2) * 0.2)\n",
    "\n",
    "            g_loss = gan.train_on_batch(noise, fake_y)\n",
    "            adversarial_loss = np.append(adversarial_loss, g_loss)\n",
    "            batches = np.append(batches, current_batch)\n",
    "\n",
    "            # Each 50 batches show and save images\n",
    "            if((batch_number + 1) % 50 == 0 and\n",
    "               current_batch_size == batch_size):\n",
    "                save_generated_images(generated_images, epoch, batch_number)\n",
    "\n",
    "            time_elapsed = time.time() - start_time\n",
    "\n",
    "            # Display and plot the results\n",
    "            print(\"     Batch \" + str(batch_number + 1) + \"/\" +\n",
    "                  str(number_of_batches) +\n",
    "                  \" generator loss | discriminator loss : \" +\n",
    "                  str(g_loss) + \" | \" + str(d_loss) + ' - batch took ' +\n",
    "                  str(time_elapsed) + ' s.')\n",
    "\n",
    "            current_batch += 1\n",
    "\n",
    "        # Save the model weights each 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            discriminator.trainable = True\n",
    "            generator.save('models/generator_epoch' + str(epoch) + '.hdf5')\n",
    "            discriminator.save('models/discriminator_epoch' +\n",
    "                               str(epoch) + '.hdf5')\n",
    "\n",
    "        # Each epoch update the loss graphs\n",
    "        plt.figure(1)\n",
    "        plt.plot(batches, adversarial_loss, color='green',\n",
    "                 label='Generator Loss')\n",
    "        plt.plot(batches, discriminator_loss, color='blue',\n",
    "                 label='Discriminator Loss')\n",
    "        plt.title(\"DCGAN Train\")\n",
    "        plt.xlabel(\"Batch Iteration\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        if epoch == 0:\n",
    "            plt.legend()\n",
    "        plt.pause(0.0000000001)\n",
    "        plt.show()\n",
    "        plt.savefig('trainingLossPlot.png')\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset_path = '/home/tfreitas/Datasets/CUB_200_2011/CUB_200_2011/images/'\n",
    "    batch_size = 64\n",
    "    image_shape = (64, 64, 3)\n",
    "    epochs = 190\n",
    "    train_dcgan(batch_size, epochs,\n",
    "                image_shape, dataset_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
