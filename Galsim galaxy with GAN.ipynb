{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a trained GAN on galsim test objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import galsim\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from photutils import create_matching_kernel\n",
    "\n",
    "import astropy.io.fits as pyfits\n",
    "import numpy as np\n",
    "\n",
    "from skimage.transform import downscale_local_mean\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### percentile normalization function borrowed from Miladious! #####\n",
    "\n",
    "def percentile_normalization(data, percentile_low = 1.5, percentile_high = 1.5, p_low_feed = None, p_high_feed = None, scale_coef = 1):\n",
    "    \n",
    "    p_low  = np.percentile(data, percentile_low)\n",
    "    p_high = np.percentile(data, 100 - percentile_high)\n",
    "    \n",
    "    # Artificially set p_low and p_high\n",
    "    if p_low_feed:\n",
    "        p_low = p_low_feed\n",
    "        \n",
    "    if p_high_feed:\n",
    "        p_high = p_high_feed\n",
    "    \n",
    "    # Bound values between q_min and q_max\n",
    "    normalized = np.clip(data, p_low, p_high)\n",
    "    # Shift the zero to prevent negative vlaues\n",
    "    normalized = normalized - np.min(normalized)\n",
    "    # Normalize so the max is 1\n",
    "    normalized /= np.max(normalized)\n",
    "    # Scale\n",
    "    normalized *= scale_coef\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def galsim_clumpy_disk(maxclumps,plot_it = 'yes'):\n",
    "    '''Returns a 64*64 image array of a disk galaxy similar to HST ACS'''\n",
    "    random_seed = np.random.randint(1,10000)\n",
    "    sky_level = 1.e6*np.abs(np.random.normal(1,0.02))     # ADU / arcsec^2\n",
    "    pixel_scale = 0.06                                   # arcsec / pixel  (size units in input catalog are pixels)\n",
    "    gal_flux = 1.e6*np.abs(np.random.normal(1,10))       # arbitrary choice, makes nice (not too) noisy images\n",
    "    gal_g1 = np.random.normal(0.01,0.02)                \n",
    "    gal_g2 = np.random.normal(0.01,0.02)                   \n",
    "\n",
    "\n",
    "    bulge_frac       = np.abs(np.random.normal(0.1,0.05))\n",
    "    disk_frac        = 1.0 - bulge_frac\n",
    "    knot_frac        = np.abs(np.random.normal(0.3,0.05))\n",
    "    smooth_disk_frac = disk_frac-knot_frac\n",
    "    n_knots = np.abs(np.random.randint(1,maxclumps))\n",
    "    hlr = np.abs(np.random.normal(0.3,0.1))\n",
    "    hlr_knots = np.abs(hlr - np.abs(np.random.normal(0.08,0.02)))\n",
    "    hlr_bulge = np.abs(np.random.normal(0.05,0.03))\n",
    "\n",
    "    rng = galsim.BaseDeviate(random_seed+1)\n",
    "    psf = galsim.Moffat(beta=3.5, fwhm=0.2, trunc=5.70)\n",
    "\n",
    "\n",
    "    smooth_disk = galsim.Exponential(flux=smooth_disk_frac, half_light_radius=hlr)\n",
    "    knots = galsim.RandomWalk(n_knots, half_light_radius=hlr_knots, flux=knot_frac, rng=rng)\n",
    "    disk = galsim.Add([smooth_disk, knots])\n",
    "    disk = disk.shear(e1=np.random.uniform(-0.5,0.5), e2=np.random.uniform(-0.5,0.5))\n",
    "    bulge = galsim.DeVaucouleurs(flux=bulge_frac, half_light_radius=hlr_bulge)\n",
    "    bulge = bulge.shear(e1=np.random.normal(0.0,0.1), e2=np.random.normal(0,0.1))\n",
    "    gal = galsim.Add([disk, bulge])\n",
    "    gal = gal.withFlux(gal_flux)\n",
    "    gal = gal.shear(g1=gal_g1, g2=gal_g2)\n",
    "\n",
    "    final = galsim.Convolve([psf, gal])\n",
    "\n",
    "    # Draw the profile\n",
    "    image = galsim.ImageF(64, 64)\n",
    "    final.drawImage(image, scale=pixel_scale)\n",
    "    image.addNoise(galsim.PoissonNoise(rng, sky_level * pixel_scale**2))\n",
    "    arr = np.log10(image.array)\n",
    "    arr[np.isnan(arr)]=0\n",
    "    arr = percentile_normalization(arr)\n",
    "    if plot_it == 'yes':\n",
    "        plt.imshow(arr,origin='lower')\n",
    "        plt.colorbar()\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shemmati/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:40: RuntimeWarning: invalid value encountered in log10\n",
      "/Users/shemmati/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'netG_epoch_490.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9ad72bf29aeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnetS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mShoobygen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mnetS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'netG_epoch_490.pth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m### read in an image with HST resoluion and downgrade (resample, PSF convolve, add noise) to HSC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'netG_epoch_490.pth'"
     ]
    }
   ],
   "source": [
    "## making the CUDA Kernel transfering HST to Subaru HSC###\n",
    "\n",
    "im = galsim_clumpy_disk(10,plot_it='no')\n",
    "im = (255.0 / (im.max()+0.5) * (im)).astype(np.uint8)\n",
    "\n",
    "#### images need to be normalized before seeing GAN#########\n",
    "# from torchvision import transforms\n",
    "# transforms.normalize(0.5,0.5)(gals)\n",
    "tfms = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
    "im = tfms(im)\n",
    "############################################################\n",
    "#converting to pytorch tensor with the right size\n",
    "gals = np.repeat(im[:,:,:,np.newaxis],1,axis = 3)\n",
    "gals =torch.tensor(gals, device=\"cpu\").float()\n",
    "gals = gals.permute(3,0,1,2)\n",
    "\n",
    "\n",
    "nc = 1\n",
    "device = \"cpu\"\n",
    "ngf = 64\n",
    "ngpu = 3\n",
    "\n",
    "class Shoobygen(nn.Module):\n",
    "\n",
    "    def __init__(self,ngpu):\n",
    "        super(Shoobygen, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose2d( 1, ngf * 8, 4, 1, 2, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 2, 2, 2, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "  \n",
    "            nn.ConvTranspose2d(ngf*4, nc, 2, 2, 4, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "            output1 = output[:,:,:,:]\n",
    "\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "            output1 = output[:,:,:,:]\n",
    "\n",
    "        return output1\n",
    "\n",
    "netS = Shoobygen(ngpu).to(device)\n",
    "netS.load_state_dict(torch.load('netG_epoch_490.pth',map_location='cpu'))\n",
    "\n",
    "### read in an image with HST resoluion and downgrade (resample, PSF convolve, add noise) to HSC \n",
    "inputs = gals\n",
    "real_cpu = inputs.to(device)\n",
    "ajab = real_cpu.detach()\n",
    "\n",
    "\n",
    "## Make Kernel\n",
    "psf = pyfits.getdata('psf_i.fits')\n",
    "psf = downscale_local_mean(psf,(3,3))\n",
    "psf = psf[7:-8,7:-8]#[22:-22,22:-22]\n",
    "\n",
    "psf_hsc = pyfits.getdata('psf-calexp-s16a_wide-HSC-I-15827-7,2-236.00000-42.00000.fits')\n",
    "psf_hsc = psf_hsc[0:41,1:42]\n",
    "\n",
    "kern = create_matching_kernel(psf,psf_hsc)\n",
    "psfh = np.repeat(kern[:,:, np.newaxis], 1, axis=2)\n",
    "psfh = np.repeat(psfh[:,:,:,np.newaxis],1,axis = 3)\n",
    "kernel = torch.Tensor(psfh)\n",
    "kernel = kernel.permute(2,3,0,1)\n",
    "kernel =  kernel.float()\n",
    "kernel = kernel.to(device)\n",
    "\n",
    "## add noise, downsample, convolve with kernel ---> give to GAN\n",
    "im = real_cpu+0.25*torch.rand_like(real_cpu)\n",
    "downsampled = F.upsample(im,scale_factor=1/3,mode='bilinear')\n",
    "img = F.conv2d(downsampled, kernel,padding=int(((kernel.shape[3])-1)/2))\n",
    "img = img[:,:,:,:]\n",
    "\n",
    "fake = netS(img)\n",
    "fd = fake.detach()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(ajab[0,0,:,:],origin='lower')\n",
    "plt.text(1,1,'Original (Galsim)',color='y')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(img[0,0,:,:],origin='lower')\n",
    "plt.text(1,1,'downgrade',color='y')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(fd[0,0,:,:],origin='lower')\n",
    "plt.text(1,1,'recovered',color='y')\n",
    "\n",
    "vutils.save_image(fake.detach(),'fake_samples_test.png',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
