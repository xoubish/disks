{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  4080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11b325070>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing parameters:\n",
    "\n",
    "dataroot='gals/'\n",
    "device = torch.device(\"cpu\") # If GPU then use \"cuda:0\"\n",
    "ngpu = 0 #number of GPUs to use \n",
    "nz = 100 #size of the latent z vector\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "workers = 2 #number of data loading workers\n",
    "batchSize = 64 #input batch size\n",
    "imageSize = 64 #the height / width of the input image to network\n",
    "niter = 25 #number of epochs to train for\n",
    "lr = 0.0002 #learning rate, default=0.0002\n",
    "beta1 = 0.5 #beta1 for adam. default=0.5\n",
    "outf='.' #folder to output images and model checkpoints\n",
    "\n",
    "\n",
    "manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: gals/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting gals/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Using downloaded and verified file: gals/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting gals/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Using downloaded and verified file: gals/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting gals/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Using downloaded and verified file: gals/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting gals/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = dset.MNIST(root=dataroot, download=True,\n",
    "                     transform=transforms.Compose([transforms.Resize(imageSize),transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,)),]))\n",
    "nc=1\n",
    "\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize,\n",
    "                                         shuffle=True, num_workers=int(workers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "#if opt.netG != '':\n",
    "#    netG.load_state_dict(torch.load(opt.netG))\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "#if opt.netD != '':\n",
    "#    netD.load_state_dict(torch.load(opt.netD))\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][0/94] Loss_D: 0.1326 Loss_G: 9.9808 D(x): 0.9862 D(G(z)): 0.0969 / 0.0001\n",
      "[0/25][1/94] Loss_D: 0.0081 Loss_G: 9.1335 D(x): 0.9997 D(G(z)): 0.0077 / 0.0002\n",
      "[0/25][2/94] Loss_D: 0.0092 Loss_G: 8.3716 D(x): 0.9991 D(G(z)): 0.0082 / 0.0003\n",
      "[0/25][3/94] Loss_D: 0.0149 Loss_G: 8.2982 D(x): 0.9962 D(G(z)): 0.0107 / 0.0003\n",
      "[0/25][4/94] Loss_D: 0.0162 Loss_G: 10.1412 D(x): 0.9996 D(G(z)): 0.0155 / 0.0001\n",
      "[0/25][5/94] Loss_D: 0.0025 Loss_G: 10.1993 D(x): 0.9994 D(G(z)): 0.0019 / 0.0001\n",
      "[0/25][6/94] Loss_D: 0.0224 Loss_G: 8.1376 D(x): 0.9886 D(G(z)): 0.0024 / 0.0005\n",
      "[0/25][7/94] Loss_D: 0.0312 Loss_G: 12.4638 D(x): 0.9996 D(G(z)): 0.0297 / 0.0000\n",
      "[0/25][8/94] Loss_D: 0.0057 Loss_G: 11.5756 D(x): 0.9952 D(G(z)): 0.0004 / 0.0000\n",
      "[0/25][9/94] Loss_D: 0.0010 Loss_G: 9.1521 D(x): 0.9999 D(G(z)): 0.0010 / 0.0002\n",
      "[0/25][10/94] Loss_D: 0.0124 Loss_G: 10.5373 D(x): 0.9999 D(G(z)): 0.0121 / 0.0000\n",
      "[0/25][11/94] Loss_D: 0.0035 Loss_G: 10.6063 D(x): 0.9999 D(G(z)): 0.0034 / 0.0000\n",
      "[0/25][12/94] Loss_D: 0.0051 Loss_G: 10.5187 D(x): 0.9998 D(G(z)): 0.0048 / 0.0000\n",
      "[0/25][13/94] Loss_D: 0.0078 Loss_G: 11.0348 D(x): 0.9977 D(G(z)): 0.0054 / 0.0000\n",
      "[0/25][14/94] Loss_D: 0.0030 Loss_G: 10.7789 D(x): 0.9999 D(G(z)): 0.0029 / 0.0000\n",
      "[0/25][15/94] Loss_D: 0.0051 Loss_G: 11.5092 D(x): 0.9998 D(G(z)): 0.0049 / 0.0000\n",
      "[0/25][16/94] Loss_D: 0.0028 Loss_G: 11.1026 D(x): 0.9996 D(G(z)): 0.0024 / 0.0000\n",
      "[0/25][17/94] Loss_D: 0.0047 Loss_G: 11.6658 D(x): 0.9991 D(G(z)): 0.0038 / 0.0000\n",
      "[0/25][18/94] Loss_D: 0.0094 Loss_G: 10.0562 D(x): 0.9933 D(G(z)): 0.0023 / 0.0001\n",
      "[0/25][19/94] Loss_D: 0.0178 Loss_G: 19.1448 D(x): 1.0000 D(G(z)): 0.0172 / 0.0000\n",
      "[0/25][20/94] Loss_D: 0.0001 Loss_G: 20.1764 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][21/94] Loss_D: 0.0003 Loss_G: 16.1117 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][22/94] Loss_D: 0.0014 Loss_G: 9.8290 D(x): 0.9988 D(G(z)): 0.0002 / 0.0001\n",
      "[0/25][23/94] Loss_D: 0.0673 Loss_G: 29.7016 D(x): 0.9989 D(G(z)): 0.0628 / 0.0000\n",
      "[0/25][24/94] Loss_D: 0.4347 Loss_G: 26.1873 D(x): 0.7560 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][25/94] Loss_D: 0.0247 Loss_G: 17.4368 D(x): 0.9876 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][26/94] Loss_D: 0.0938 Loss_G: 21.7397 D(x): 1.0000 D(G(z)): 0.0822 / 0.0000\n",
      "[0/25][27/94] Loss_D: 0.0006 Loss_G: 17.0048 D(x): 1.0000 D(G(z)): 0.0006 / 0.0000\n",
      "[0/25][28/94] Loss_D: 1.0660 Loss_G: 35.5648 D(x): 1.0000 D(G(z)): 0.5739 / 0.0000\n",
      "[0/25][29/94] Loss_D: 0.0131 Loss_G: 41.0895 D(x): 0.9884 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][30/94] Loss_D: 3.9476 Loss_G: 36.9895 D(x): 0.2552 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][31/94] Loss_D: 0.0002 Loss_G: 34.6342 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][32/94] Loss_D: 0.0000 Loss_G: 33.5381 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][33/94] Loss_D: 0.0000 Loss_G: 32.5008 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][34/94] Loss_D: 0.0000 Loss_G: 32.0697 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][35/94] Loss_D: 0.0000 Loss_G: 31.3566 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][36/94] Loss_D: 0.0000 Loss_G: 30.6461 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][37/94] Loss_D: 0.0000 Loss_G: 28.2332 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][38/94] Loss_D: 0.0000 Loss_G: 20.1449 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][39/94] Loss_D: 2.7716 Loss_G: 31.3489 D(x): 1.0000 D(G(z)): 0.7727 / 0.0000\n",
      "[0/25][40/94] Loss_D: 0.0004 Loss_G: 37.2727 D(x): 0.9996 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][41/94] Loss_D: 0.3315 Loss_G: 39.0899 D(x): 0.9392 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][42/94] Loss_D: 0.0246 Loss_G: 39.8267 D(x): 0.9834 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][43/94] Loss_D: 0.0687 Loss_G: 39.8369 D(x): 0.9614 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][44/94] Loss_D: 0.1093 Loss_G: 40.0654 D(x): 0.9717 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][45/94] Loss_D: 0.0206 Loss_G: 39.7355 D(x): 0.9870 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][46/94] Loss_D: 0.0209 Loss_G: 39.6818 D(x): 0.9849 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][47/94] Loss_D: 0.0010 Loss_G: 39.6148 D(x): 0.9990 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][48/94] Loss_D: 0.0037 Loss_G: 39.6417 D(x): 0.9966 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][49/94] Loss_D: 0.0001 Loss_G: 39.7034 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][50/94] Loss_D: 0.0000 Loss_G: 39.4435 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][51/94] Loss_D: 0.0001 Loss_G: 39.7912 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][52/94] Loss_D: 0.0000 Loss_G: 39.6051 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][53/94] Loss_D: 0.0000 Loss_G: 39.6608 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][54/94] Loss_D: 0.0822 Loss_G: 39.5324 D(x): 0.9841 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][55/94] Loss_D: 0.0004 Loss_G: 39.2711 D(x): 0.9996 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][56/94] Loss_D: 0.0019 Loss_G: 39.4281 D(x): 0.9982 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][57/94] Loss_D: 0.0000 Loss_G: 39.2862 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][58/94] Loss_D: 0.0004 Loss_G: 39.4692 D(x): 0.9996 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][59/94] Loss_D: 0.0100 Loss_G: 39.4618 D(x): 0.9924 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][60/94] Loss_D: 0.0001 Loss_G: 39.0383 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][61/94] Loss_D: 0.0002 Loss_G: 39.4062 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][62/94] Loss_D: 0.0462 Loss_G: 39.2353 D(x): 0.9822 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][63/94] Loss_D: 0.0005 Loss_G: 38.7347 D(x): 0.9995 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][64/94] Loss_D: 0.0004 Loss_G: 39.0653 D(x): 0.9996 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][65/94] Loss_D: 0.0000 Loss_G: 39.0538 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][66/94] Loss_D: 0.0000 Loss_G: 38.9765 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][67/94] Loss_D: 0.0122 Loss_G: 39.1770 D(x): 0.9915 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][68/94] Loss_D: 0.0000 Loss_G: 38.8575 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][69/94] Loss_D: 0.0001 Loss_G: 38.8762 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][70/94] Loss_D: 0.0000 Loss_G: 38.8584 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][71/94] Loss_D: 0.0000 Loss_G: 39.0759 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][72/94] Loss_D: 0.0001 Loss_G: 38.9320 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][73/94] Loss_D: 0.0000 Loss_G: 38.9434 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][74/94] Loss_D: 0.0000 Loss_G: 38.7604 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][75/94] Loss_D: 0.0007 Loss_G: 38.9141 D(x): 0.9993 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][76/94] Loss_D: 0.0000 Loss_G: 38.9119 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][77/94] Loss_D: 0.0000 Loss_G: 38.9776 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][78/94] Loss_D: 0.0000 Loss_G: 38.8198 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][79/94] Loss_D: 0.0000 Loss_G: 39.0695 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][80/94] Loss_D: 0.0002 Loss_G: 38.9061 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][81/94] Loss_D: 0.0000 Loss_G: 38.8426 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][82/94] Loss_D: 0.0000 Loss_G: 38.7141 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][83/94] Loss_D: 0.0000 Loss_G: 38.8278 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][84/94] Loss_D: 0.0000 Loss_G: 38.7600 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][85/94] Loss_D: 0.0003 Loss_G: 39.0425 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][86/94] Loss_D: 0.0002 Loss_G: 38.8363 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][87/94] Loss_D: 0.0000 Loss_G: 38.7136 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][88/94] Loss_D: 0.0004 Loss_G: 38.7897 D(x): 0.9996 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][89/94] Loss_D: 0.0003 Loss_G: 39.0010 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][90/94] Loss_D: 0.0000 Loss_G: 38.9275 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][91/94] Loss_D: 0.0001 Loss_G: 39.1387 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][92/94] Loss_D: 0.0000 Loss_G: 39.0446 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][93/94] Loss_D: 0.0000 Loss_G: 38.8180 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][0/94] Loss_D: 0.0001 Loss_G: 38.7880 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][1/94] Loss_D: 0.0000 Loss_G: 38.7906 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][2/94] Loss_D: 0.0000 Loss_G: 38.7315 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][3/94] Loss_D: 0.0003 Loss_G: 38.7229 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][4/94] Loss_D: 0.0002 Loss_G: 39.0100 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][5/94] Loss_D: 0.0056 Loss_G: 38.9330 D(x): 0.9953 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][6/94] Loss_D: 0.0002 Loss_G: 38.9123 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][7/94] Loss_D: 0.0000 Loss_G: 38.8294 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][8/94] Loss_D: 0.0000 Loss_G: 38.8505 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][9/94] Loss_D: 0.0000 Loss_G: 39.0764 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][10/94] Loss_D: 0.0001 Loss_G: 38.7993 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][11/94] Loss_D: 0.0000 Loss_G: 39.0417 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][12/94] Loss_D: 0.0001 Loss_G: 38.9520 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][13/94] Loss_D: 0.0000 Loss_G: 39.0171 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][14/94] Loss_D: 0.0000 Loss_G: 38.6338 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][15/94] Loss_D: 0.0000 Loss_G: 38.6787 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][16/94] Loss_D: 0.0000 Loss_G: 38.7672 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][17/94] Loss_D: 0.0000 Loss_G: 38.7107 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][18/94] Loss_D: 0.0000 Loss_G: 38.7562 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][19/94] Loss_D: 0.0000 Loss_G: 38.6825 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][20/94] Loss_D: 0.0000 Loss_G: 38.7688 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][21/94] Loss_D: 0.0000 Loss_G: 38.8280 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][22/94] Loss_D: 0.0000 Loss_G: 38.7398 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][23/94] Loss_D: 0.0000 Loss_G: 38.9585 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][24/94] Loss_D: 0.0000 Loss_G: 38.9091 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][25/94] Loss_D: 0.0000 Loss_G: 38.7894 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][26/94] Loss_D: 0.0000 Loss_G: 38.8090 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][27/94] Loss_D: 0.0000 Loss_G: 38.9785 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][28/94] Loss_D: 0.0000 Loss_G: 38.9123 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][29/94] Loss_D: 0.0000 Loss_G: 38.8648 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][30/94] Loss_D: 0.0000 Loss_G: 38.6979 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][31/94] Loss_D: 0.0000 Loss_G: 38.8645 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][32/94] Loss_D: 0.0001 Loss_G: 38.8678 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][33/94] Loss_D: 0.0000 Loss_G: 38.8811 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][34/94] Loss_D: 0.0001 Loss_G: 38.8844 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][35/94] Loss_D: 0.0010 Loss_G: 38.7458 D(x): 0.9990 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][36/94] Loss_D: 0.0000 Loss_G: 38.9216 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][37/94] Loss_D: 0.0000 Loss_G: 38.7259 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][38/94] Loss_D: 0.0000 Loss_G: 38.7627 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][39/94] Loss_D: 0.0000 Loss_G: 38.8164 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][40/94] Loss_D: 0.0000 Loss_G: 38.6621 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][41/94] Loss_D: 0.0001 Loss_G: 38.6167 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][42/94] Loss_D: 0.0000 Loss_G: 38.6898 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][43/94] Loss_D: 0.0000 Loss_G: 38.9056 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][44/94] Loss_D: 0.0000 Loss_G: 38.7784 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][45/94] Loss_D: 0.0002 Loss_G: 38.8475 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][46/94] Loss_D: 0.0000 Loss_G: 38.7651 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][47/94] Loss_D: 0.0002 Loss_G: 38.6330 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][48/94] Loss_D: 0.0002 Loss_G: 38.8458 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][49/94] Loss_D: 0.0000 Loss_G: 38.7952 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][50/94] Loss_D: 0.0000 Loss_G: 38.8456 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][51/94] Loss_D: 0.0000 Loss_G: 38.7251 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][52/94] Loss_D: 0.0000 Loss_G: 39.0320 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][53/94] Loss_D: 0.0000 Loss_G: 38.6398 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][54/94] Loss_D: 0.0000 Loss_G: 38.8692 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][55/94] Loss_D: 0.0003 Loss_G: 38.7969 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][56/94] Loss_D: 0.0000 Loss_G: 38.5486 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][57/94] Loss_D: 0.0000 Loss_G: 38.5925 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][58/94] Loss_D: 0.0001 Loss_G: 38.7384 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][59/94] Loss_D: 0.0001 Loss_G: 38.8767 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][60/94] Loss_D: 0.0000 Loss_G: 38.6860 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][61/94] Loss_D: 0.0001 Loss_G: 38.7587 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][62/94] Loss_D: 0.0000 Loss_G: 38.7237 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][63/94] Loss_D: 0.0000 Loss_G: 38.7263 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][64/94] Loss_D: 0.0000 Loss_G: 38.8681 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][65/94] Loss_D: 0.0002 Loss_G: 38.7672 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][66/94] Loss_D: 0.0000 Loss_G: 38.8383 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][67/94] Loss_D: 0.0408 Loss_G: 38.4872 D(x): 0.9854 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][68/94] Loss_D: 0.0000 Loss_G: 38.6110 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][69/94] Loss_D: 0.0000 Loss_G: 38.3637 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][70/94] Loss_D: 0.0000 Loss_G: 38.3994 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][71/94] Loss_D: 0.0000 Loss_G: 38.6057 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][72/94] Loss_D: 0.0000 Loss_G: 38.6357 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][73/94] Loss_D: 0.0000 Loss_G: 38.5234 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][74/94] Loss_D: 0.0000 Loss_G: 38.5946 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][75/94] Loss_D: 0.0000 Loss_G: 38.6598 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][76/94] Loss_D: 0.0001 Loss_G: 38.3643 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][77/94] Loss_D: 0.0000 Loss_G: 38.3787 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][78/94] Loss_D: 0.0000 Loss_G: 38.5662 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][79/94] Loss_D: 0.0000 Loss_G: 38.6070 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][80/94] Loss_D: 0.0000 Loss_G: 38.6243 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][81/94] Loss_D: 0.0000 Loss_G: 38.4461 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][82/94] Loss_D: 0.0000 Loss_G: 38.5347 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][83/94] Loss_D: 0.0000 Loss_G: 38.4289 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][84/94] Loss_D: 0.0000 Loss_G: 38.7597 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][85/94] Loss_D: 0.0000 Loss_G: 38.4023 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][86/94] Loss_D: 0.0000 Loss_G: 38.4840 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][87/94] Loss_D: 0.0000 Loss_G: 38.3830 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][88/94] Loss_D: 0.0005 Loss_G: 38.5941 D(x): 0.9995 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][89/94] Loss_D: 0.0001 Loss_G: 38.5370 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][90/94] Loss_D: 0.0000 Loss_G: 38.8107 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][91/94] Loss_D: 0.0000 Loss_G: 38.6571 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][92/94] Loss_D: 0.0000 Loss_G: 38.6144 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[1/25][93/94] Loss_D: 0.0000 Loss_G: 38.3654 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][0/94] Loss_D: 0.0000 Loss_G: 38.3344 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][1/94] Loss_D: 0.0000 Loss_G: 38.4938 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][2/94] Loss_D: 0.0000 Loss_G: 38.3968 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][3/94] Loss_D: 0.0000 Loss_G: 38.6234 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][4/94] Loss_D: 0.0000 Loss_G: 38.7409 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][5/94] Loss_D: 0.0000 Loss_G: 38.4982 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][6/94] Loss_D: 0.0000 Loss_G: 38.6107 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][7/94] Loss_D: 0.0000 Loss_G: 38.6017 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][8/94] Loss_D: 0.0000 Loss_G: 38.4883 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][9/94] Loss_D: 0.0000 Loss_G: 38.4819 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][10/94] Loss_D: 0.0000 Loss_G: 38.6960 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][11/94] Loss_D: 0.0000 Loss_G: 38.8281 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][12/94] Loss_D: 0.0000 Loss_G: 38.5505 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][13/94] Loss_D: 0.0000 Loss_G: 38.6337 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][14/94] Loss_D: 0.0000 Loss_G: 38.5223 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][15/94] Loss_D: 0.0000 Loss_G: 38.5119 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][16/94] Loss_D: 0.0000 Loss_G: 38.2534 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][17/94] Loss_D: 0.0001 Loss_G: 38.2563 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][18/94] Loss_D: 0.0000 Loss_G: 38.6839 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][19/94] Loss_D: 0.0000 Loss_G: 38.4152 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][20/94] Loss_D: 0.0000 Loss_G: 38.6931 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][21/94] Loss_D: 0.0000 Loss_G: 38.7696 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][22/94] Loss_D: 0.0000 Loss_G: 38.6303 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][23/94] Loss_D: 0.0000 Loss_G: 38.3867 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][24/94] Loss_D: 0.0000 Loss_G: 38.5281 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][25/94] Loss_D: 0.0000 Loss_G: 38.4923 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][26/94] Loss_D: 0.0000 Loss_G: 38.5842 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][27/94] Loss_D: 0.0000 Loss_G: 38.4474 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][28/94] Loss_D: 0.0000 Loss_G: 38.2715 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][29/94] Loss_D: 0.0000 Loss_G: 38.4807 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][30/94] Loss_D: 0.0000 Loss_G: 38.5344 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][31/94] Loss_D: 0.0001 Loss_G: 38.6097 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][32/94] Loss_D: 0.0000 Loss_G: 38.2797 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][33/94] Loss_D: 0.0000 Loss_G: 38.4784 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][34/94] Loss_D: 0.0000 Loss_G: 38.3865 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][35/94] Loss_D: 0.0000 Loss_G: 38.4137 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][36/94] Loss_D: 0.0000 Loss_G: 38.6197 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][37/94] Loss_D: 0.0000 Loss_G: 38.6416 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][38/94] Loss_D: 0.0000 Loss_G: 38.5213 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][39/94] Loss_D: 0.0002 Loss_G: 38.3291 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][40/94] Loss_D: 0.0000 Loss_G: 38.7927 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][41/94] Loss_D: 0.0000 Loss_G: 38.5422 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][42/94] Loss_D: 0.0000 Loss_G: 38.4870 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][43/94] Loss_D: 0.0002 Loss_G: 38.4222 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][44/94] Loss_D: 0.0000 Loss_G: 38.4986 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][45/94] Loss_D: 0.0000 Loss_G: 38.4270 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][46/94] Loss_D: 0.0000 Loss_G: 38.6315 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][47/94] Loss_D: 0.0000 Loss_G: 38.2984 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][48/94] Loss_D: 0.0000 Loss_G: 38.3761 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][49/94] Loss_D: 0.0001 Loss_G: 38.6696 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][50/94] Loss_D: 0.0000 Loss_G: 38.6589 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][51/94] Loss_D: 0.0000 Loss_G: 38.4223 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][52/94] Loss_D: 0.0000 Loss_G: 38.3676 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][53/94] Loss_D: 0.0000 Loss_G: 38.8248 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][54/94] Loss_D: 0.0000 Loss_G: 38.3239 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][55/94] Loss_D: 0.0000 Loss_G: 38.6518 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][56/94] Loss_D: 0.0000 Loss_G: 38.4056 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][57/94] Loss_D: 0.0000 Loss_G: 38.4531 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][58/94] Loss_D: 0.0000 Loss_G: 38.5672 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][59/94] Loss_D: 0.0000 Loss_G: 38.7642 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][60/94] Loss_D: 0.0000 Loss_G: 38.4268 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][61/94] Loss_D: 0.0000 Loss_G: 38.5485 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][62/94] Loss_D: 0.0000 Loss_G: 38.3392 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][63/94] Loss_D: 0.0000 Loss_G: 38.4275 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][64/94] Loss_D: 0.0000 Loss_G: 38.1698 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][65/94] Loss_D: 0.0000 Loss_G: 38.4272 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][66/94] Loss_D: 0.0000 Loss_G: 38.4629 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][67/94] Loss_D: 0.0000 Loss_G: 38.3930 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][68/94] Loss_D: 0.0000 Loss_G: 38.7664 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][69/94] Loss_D: 0.0000 Loss_G: 38.5277 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][70/94] Loss_D: 0.0000 Loss_G: 38.2806 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][71/94] Loss_D: 0.0020 Loss_G: 38.5155 D(x): 0.9981 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][72/94] Loss_D: 0.0000 Loss_G: 38.4277 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][73/94] Loss_D: 0.0000 Loss_G: 38.4076 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][74/94] Loss_D: 0.0001 Loss_G: 38.4114 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][75/94] Loss_D: 0.0000 Loss_G: 38.3911 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][76/94] Loss_D: 0.0000 Loss_G: 38.3683 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][77/94] Loss_D: 0.0000 Loss_G: 38.6404 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][78/94] Loss_D: 0.0000 Loss_G: 38.3854 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][79/94] Loss_D: 0.0000 Loss_G: 38.3413 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][80/94] Loss_D: 0.0000 Loss_G: 38.4262 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][81/94] Loss_D: 0.0000 Loss_G: 38.2905 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][82/94] Loss_D: 0.0000 Loss_G: 38.0735 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][83/94] Loss_D: 0.0000 Loss_G: 38.3377 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][84/94] Loss_D: 0.0000 Loss_G: 38.4087 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][85/94] Loss_D: 0.0000 Loss_G: 38.4795 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][86/94] Loss_D: 0.0000 Loss_G: 38.2295 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][87/94] Loss_D: 0.0000 Loss_G: 38.5550 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][88/94] Loss_D: 0.0000 Loss_G: 38.2012 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][89/94] Loss_D: 0.0000 Loss_G: 38.5056 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][90/94] Loss_D: 0.0000 Loss_G: 38.5866 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][91/94] Loss_D: 0.0000 Loss_G: 38.4572 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][92/94] Loss_D: 0.0000 Loss_G: 38.3402 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[2/25][93/94] Loss_D: 0.0000 Loss_G: 38.2917 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][0/94] Loss_D: 0.0000 Loss_G: 38.4489 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][1/94] Loss_D: 0.0000 Loss_G: 38.4317 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][2/94] Loss_D: 0.0000 Loss_G: 38.0893 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][3/94] Loss_D: 0.0000 Loss_G: 38.5853 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][4/94] Loss_D: 0.0000 Loss_G: 38.3817 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][5/94] Loss_D: 0.0000 Loss_G: 38.5838 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][6/94] Loss_D: 0.0000 Loss_G: 38.2691 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][7/94] Loss_D: 0.0000 Loss_G: 38.1049 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][8/94] Loss_D: 0.0000 Loss_G: 38.2663 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][9/94] Loss_D: 0.0000 Loss_G: 38.2862 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][10/94] Loss_D: 0.0000 Loss_G: 38.3646 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][11/94] Loss_D: 0.0000 Loss_G: 38.3955 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][12/94] Loss_D: 0.0000 Loss_G: 38.2702 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][13/94] Loss_D: 0.0000 Loss_G: 38.3750 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][14/94] Loss_D: 0.0000 Loss_G: 38.4346 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][15/94] Loss_D: 0.0000 Loss_G: 38.2413 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][16/94] Loss_D: 0.0000 Loss_G: 38.4059 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][17/94] Loss_D: 0.0000 Loss_G: 38.4703 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][18/94] Loss_D: 0.0000 Loss_G: 38.3236 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][19/94] Loss_D: 0.0000 Loss_G: 38.2541 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][20/94] Loss_D: 0.0000 Loss_G: 38.1335 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][21/94] Loss_D: 0.0000 Loss_G: 38.4852 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][22/94] Loss_D: 0.0000 Loss_G: 38.4912 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][23/94] Loss_D: 0.0000 Loss_G: 38.4252 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][24/94] Loss_D: 0.0001 Loss_G: 38.2200 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][25/94] Loss_D: 0.0000 Loss_G: 38.5390 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][26/94] Loss_D: 0.0006 Loss_G: 38.3335 D(x): 0.9994 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][27/94] Loss_D: 0.0000 Loss_G: 38.3233 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][28/94] Loss_D: 0.0000 Loss_G: 38.3469 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][29/94] Loss_D: 0.0001 Loss_G: 38.2582 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][30/94] Loss_D: 0.0000 Loss_G: 38.0805 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][31/94] Loss_D: 0.0000 Loss_G: 38.0975 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][32/94] Loss_D: 0.0000 Loss_G: 38.1478 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][33/94] Loss_D: 0.0000 Loss_G: 38.2223 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][34/94] Loss_D: 0.0000 Loss_G: 38.1703 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][35/94] Loss_D: 0.0000 Loss_G: 38.4366 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][36/94] Loss_D: 0.0000 Loss_G: 37.9731 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][37/94] Loss_D: 0.0000 Loss_G: 38.2847 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][38/94] Loss_D: 0.0000 Loss_G: 38.3235 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][39/94] Loss_D: 0.0000 Loss_G: 38.4112 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][40/94] Loss_D: 0.0000 Loss_G: 38.3759 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][41/94] Loss_D: 0.0000 Loss_G: 38.3439 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][42/94] Loss_D: 0.0000 Loss_G: 38.4151 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][43/94] Loss_D: 0.0000 Loss_G: 38.0965 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][44/94] Loss_D: 0.0000 Loss_G: 38.1641 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][45/94] Loss_D: 0.0000 Loss_G: 38.4178 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][46/94] Loss_D: 0.0000 Loss_G: 38.1995 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][47/94] Loss_D: 0.0000 Loss_G: 38.3207 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][48/94] Loss_D: 0.0000 Loss_G: 38.3469 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][49/94] Loss_D: 0.0000 Loss_G: 38.2949 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][50/94] Loss_D: 0.0000 Loss_G: 38.3810 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][51/94] Loss_D: 0.0000 Loss_G: 38.3175 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][52/94] Loss_D: 0.0000 Loss_G: 38.2139 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][53/94] Loss_D: 0.0000 Loss_G: 38.2867 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][54/94] Loss_D: 0.0000 Loss_G: 38.0503 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][55/94] Loss_D: 0.0000 Loss_G: 38.1068 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][56/94] Loss_D: 0.0000 Loss_G: 38.3289 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][57/94] Loss_D: 0.0000 Loss_G: 38.2199 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][58/94] Loss_D: 0.0000 Loss_G: 38.3804 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][59/94] Loss_D: 0.0000 Loss_G: 38.0475 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][60/94] Loss_D: 0.0000 Loss_G: 38.1873 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][61/94] Loss_D: 0.0000 Loss_G: 38.2485 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][62/94] Loss_D: 0.0000 Loss_G: 38.4605 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][63/94] Loss_D: 0.0000 Loss_G: 38.1238 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[3/25][64/94] Loss_D: 0.0000 Loss_G: 38.3562 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(batchSize, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "for epoch in range(niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label, device=device)\n",
    "\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, niter, i, len(dataloader),\n",
    "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu,\n",
    "                    '%s/real_samples.png' % outf,\n",
    "                    normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.detach(),\n",
    "                    '%s/fake_samples_epoch_%03d.png' % (outf, epoch),\n",
    "                    normalize=True)\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (outf, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
