{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  4482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd63c05a630>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing parameters:\n",
    "\n",
    "dataroot='gals/'\n",
    "device = torch.device(\"cpu\") # If GPU then use \"cuda:0\"\n",
    "ngpu = 0 #number of GPUs to use \n",
    "nz = 10 #size of the latent z vector\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "workers = 4*ngpu #number of data loading workers\n",
    "batchSize = 5 #input batch size\n",
    "imageSize = 64 #the height / width of the input image to network\n",
    "niter = 2 #number of epochs to train for\n",
    "lr = 0.0002 #learning rate, default=0.0002\n",
    "beta1 = 0.5 #beta1 for adam. default=0.5\n",
    "outf='outputs' #folder to output images and model checkpoints\n",
    "\n",
    "\n",
    "manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dset.MNIST(root=dataroot, download=True,\n",
    "                     transform=transforms.Compose([transforms.Resize(imageSize),transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,)),]))\n",
    "nc=1\n",
    "\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=int(workers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(10, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "#if netG != '':\n",
    "#    netG.load_state_dict(torch.load(netG))\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "#if netD != '':\n",
    "#    netD.load_state_dict(torch.load(netD))\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 167, 5)\n"
     ]
    }
   ],
   "source": [
    "from astropy.convolution import convolve\n",
    "import astropy.io.fits as pyfits\n",
    "\n",
    "a = torch.randn(64,64,batchSize)\n",
    "c = a.data.numpy()\n",
    "\n",
    "psf_h = pyfits.getdata('psf_h.fits')\n",
    "#psfh_1 = np.repeat(psf_h[:, :, np.newaxis], 1, axis=2)\n",
    "psfh_2 = np.repeat(psf_h[:, :, np.newaxis], batchSize, axis=2)\n",
    "print (np.shape(psfh_2))\n",
    "\n",
    "boz=convolve(c,psfh_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/2][0/1205] Loss_D: 1.1533 Loss_G: 11.5024 D(x): 0.7497 D(G(z)): 0.5578 / 0.0005\n",
      "[0/2][1/1205] Loss_D: 0.2104 Loss_G: 9.1487 D(x): 0.9999 D(G(z)): 0.1818 / 0.0003\n",
      "[0/2][2/1205] Loss_D: 0.2044 Loss_G: 6.2695 D(x): 0.8868 D(G(z)): 0.0365 / 0.0028\n",
      "[0/2][3/1205] Loss_D: 0.1094 Loss_G: 6.6647 D(x): 0.9997 D(G(z)): 0.1003 / 0.0016\n",
      "[0/2][4/1205] Loss_D: 0.1113 Loss_G: 7.5413 D(x): 1.0000 D(G(z)): 0.1046 / 0.0009\n",
      "[0/2][5/1205] Loss_D: 0.1839 Loss_G: 9.3767 D(x): 1.0000 D(G(z)): 0.1629 / 0.0002\n",
      "[0/2][6/1205] Loss_D: 0.1998 Loss_G: 10.2622 D(x): 1.0000 D(G(z)): 0.1410 / 0.0012\n",
      "[0/2][7/1205] Loss_D: 0.0481 Loss_G: 8.9634 D(x): 1.0000 D(G(z)): 0.0459 / 0.0005\n",
      "[0/2][8/1205] Loss_D: 0.0298 Loss_G: 7.4041 D(x): 0.9999 D(G(z)): 0.0289 / 0.0013\n",
      "[0/2][9/1205] Loss_D: 0.0407 Loss_G: 6.8458 D(x): 1.0000 D(G(z)): 0.0392 / 0.0021\n",
      "[0/2][10/1205] Loss_D: 0.0462 Loss_G: 7.2462 D(x): 1.0000 D(G(z)): 0.0443 / 0.0011\n",
      "[0/2][11/1205] Loss_D: 0.1795 Loss_G: 10.5535 D(x): 1.0000 D(G(z)): 0.1577 / 0.0002\n",
      "[0/2][12/1205] Loss_D: 0.0272 Loss_G: 8.6447 D(x): 1.0000 D(G(z)): 0.0264 / 0.0004\n",
      "[0/2][13/1205] Loss_D: 0.0015 Loss_G: 8.9537 D(x): 0.9999 D(G(z)): 0.0014 / 0.0003\n",
      "[0/2][14/1205] Loss_D: 0.0060 Loss_G: 7.9311 D(x): 0.9992 D(G(z)): 0.0052 / 0.0016\n",
      "[0/2][15/1205] Loss_D: 0.0871 Loss_G: 7.7215 D(x): 0.9988 D(G(z)): 0.0790 / 0.0006\n",
      "[0/2][16/1205] Loss_D: 0.0163 Loss_G: 7.0723 D(x): 0.9990 D(G(z)): 0.0151 / 0.0011\n",
      "[0/2][17/1205] Loss_D: 0.0430 Loss_G: 8.0557 D(x): 1.0000 D(G(z)): 0.0415 / 0.0004\n",
      "[0/2][18/1205] Loss_D: 0.0097 Loss_G: 8.6391 D(x): 0.9999 D(G(z)): 0.0095 / 0.0006\n",
      "[0/2][19/1205] Loss_D: 0.0390 Loss_G: 8.4966 D(x): 0.9990 D(G(z)): 0.0360 / 0.0002\n",
      "[0/2][20/1205] Loss_D: 0.0150 Loss_G: 7.2847 D(x): 0.9984 D(G(z)): 0.0132 / 0.0010\n",
      "[0/2][21/1205] Loss_D: 0.0130 Loss_G: 7.2737 D(x): 0.9993 D(G(z)): 0.0121 / 0.0012\n",
      "[0/2][22/1205] Loss_D: 0.0183 Loss_G: 7.0460 D(x): 0.9989 D(G(z)): 0.0169 / 0.0014\n",
      "[0/2][23/1205] Loss_D: 0.0431 Loss_G: 8.5254 D(x): 0.9986 D(G(z)): 0.0397 / 0.0003\n",
      "[0/2][24/1205] Loss_D: 0.0287 Loss_G: 7.0243 D(x): 0.9807 D(G(z)): 0.0085 / 0.0010\n",
      "[0/2][25/1205] Loss_D: 0.1141 Loss_G: 12.7958 D(x): 0.9984 D(G(z)): 0.0985 / 0.0004\n",
      "[0/2][26/1205] Loss_D: 0.0041 Loss_G: 12.3429 D(x): 0.9968 D(G(z)): 0.0009 / 0.0001\n",
      "[0/2][27/1205] Loss_D: 0.0052 Loss_G: 10.6608 D(x): 0.9956 D(G(z)): 0.0007 / 0.0004\n",
      "[0/2][28/1205] Loss_D: 0.0032 Loss_G: 9.1268 D(x): 0.9990 D(G(z)): 0.0022 / 0.0013\n",
      "[0/2][29/1205] Loss_D: 0.0094 Loss_G: 7.2314 D(x): 1.0000 D(G(z)): 0.0092 / 0.0016\n",
      "[0/2][30/1205] Loss_D: 0.0102 Loss_G: 7.6263 D(x): 0.9998 D(G(z)): 0.0098 / 0.0019\n",
      "[0/2][31/1205] Loss_D: 0.1198 Loss_G: 12.3903 D(x): 0.9985 D(G(z)): 0.0991 / 0.0000\n",
      "[0/2][32/1205] Loss_D: 0.0011 Loss_G: 10.3826 D(x): 0.9999 D(G(z)): 0.0009 / 0.0001\n",
      "[0/2][33/1205] Loss_D: 0.0097 Loss_G: 10.1687 D(x): 0.9921 D(G(z)): 0.0016 / 0.0004\n",
      "[0/2][34/1205] Loss_D: 0.0015 Loss_G: 8.7378 D(x): 1.0000 D(G(z)): 0.0014 / 0.0007\n",
      "[0/2][35/1205] Loss_D: 0.0364 Loss_G: 9.9472 D(x): 1.0000 D(G(z)): 0.0348 / 0.0002\n",
      "[0/2][36/1205] Loss_D: 0.0167 Loss_G: 9.3713 D(x): 0.9970 D(G(z)): 0.0134 / 0.0005\n",
      "[0/2][37/1205] Loss_D: 0.0133 Loss_G: 9.1010 D(x): 0.9883 D(G(z)): 0.0012 / 0.0005\n",
      "[0/2][38/1205] Loss_D: 0.0217 Loss_G: 6.7610 D(x): 0.9861 D(G(z)): 0.0074 / 0.0016\n",
      "[0/2][39/1205] Loss_D: 0.0872 Loss_G: 12.7671 D(x): 1.0000 D(G(z)): 0.0737 / 0.0000\n",
      "[0/2][40/1205] Loss_D: 0.0003 Loss_G: 13.2892 D(x): 1.0000 D(G(z)): 0.0003 / 0.0000\n",
      "[0/2][41/1205] Loss_D: 0.0032 Loss_G: 10.9039 D(x): 0.9999 D(G(z)): 0.0030 / 0.0003\n",
      "[0/2][42/1205] Loss_D: 0.0091 Loss_G: 8.7094 D(x): 0.9931 D(G(z)): 0.0021 / 0.0007\n",
      "[0/2][43/1205] Loss_D: 0.1557 Loss_G: 6.1286 D(x): 0.8980 D(G(z)): 0.0138 / 0.0046\n",
      "[0/2][44/1205] Loss_D: 0.3356 Loss_G: 23.8016 D(x): 1.0000 D(G(z)): 0.2386 / 0.0000\n",
      "[0/2][45/1205] Loss_D: 0.0000 Loss_G: 24.3253 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][46/1205] Loss_D: 0.0025 Loss_G: 25.9844 D(x): 0.9975 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][47/1205] Loss_D: 0.0002 Loss_G: 23.2942 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][48/1205] Loss_D: 0.0002 Loss_G: 19.4785 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][49/1205] Loss_D: 0.7889 Loss_G: 10.3790 D(x): 0.7240 D(G(z)): 0.0000 / 0.0005\n",
      "[0/2][50/1205] Loss_D: 0.0954 Loss_G: 8.0702 D(x): 1.0000 D(G(z)): 0.0798 / 0.0035\n",
      "[0/2][51/1205] Loss_D: 0.9248 Loss_G: 31.8799 D(x): 1.0000 D(G(z)): 0.4220 / 0.0000\n",
      "[0/2][52/1205] Loss_D: 0.0000 Loss_G: 29.0237 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][53/1205] Loss_D: 0.0000 Loss_G: 31.8377 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][54/1205] Loss_D: 0.0000 Loss_G: 30.1158 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][55/1205] Loss_D: 0.0008 Loss_G: 29.7575 D(x): 0.9992 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][56/1205] Loss_D: 0.9735 Loss_G: 27.0278 D(x): 0.8013 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][57/1205] Loss_D: 4.8127 Loss_G: 21.8526 D(x): 0.7998 D(G(z)): 0.0070 / 0.0091\n",
      "[0/2][58/1205] Loss_D: 0.0001 Loss_G: 15.2111 D(x): 1.0000 D(G(z)): 0.0001 / 0.0003\n",
      "[0/2][59/1205] Loss_D: 0.0111 Loss_G: 12.8382 D(x): 1.0000 D(G(z)): 0.0108 / 0.0030\n",
      "[0/2][60/1205] Loss_D: 1.0290 Loss_G: 19.8750 D(x): 1.0000 D(G(z)): 0.2888 / 0.0000\n",
      "[0/2][61/1205] Loss_D: 0.0032 Loss_G: 15.5215 D(x): 0.9994 D(G(z)): 0.0026 / 0.0000\n",
      "[0/2][62/1205] Loss_D: 0.0007 Loss_G: 12.9987 D(x): 0.9996 D(G(z)): 0.0003 / 0.0001\n",
      "[0/2][63/1205] Loss_D: 0.4317 Loss_G: 20.1853 D(x): 0.9839 D(G(z)): 0.2165 / 0.0000\n",
      "[0/2][64/1205] Loss_D: 0.4010 Loss_G: 12.9385 D(x): 0.7624 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][65/1205] Loss_D: 0.0107 Loss_G: 11.5462 D(x): 1.0000 D(G(z)): 0.0105 / 0.0393\n",
      "[0/2][66/1205] Loss_D: 4.3148 Loss_G: 26.0530 D(x): 1.0000 D(G(z)): 0.5971 / 0.0000\n",
      "[0/2][67/1205] Loss_D: 0.0000 Loss_G: 25.5460 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][68/1205] Loss_D: 0.0000 Loss_G: 24.0646 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][69/1205] Loss_D: 0.0164 Loss_G: 24.4629 D(x): 0.9840 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][70/1205] Loss_D: 0.0012 Loss_G: 16.9908 D(x): 0.9988 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][71/1205] Loss_D: 0.0016 Loss_G: 18.0331 D(x): 0.9985 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][72/1205] Loss_D: 0.0778 Loss_G: 11.6718 D(x): 0.9720 D(G(z)): 0.0448 / 0.0002\n",
      "[0/2][73/1205] Loss_D: 1.2231 Loss_G: 18.9774 D(x): 0.8293 D(G(z)): 0.1971 / 0.0000\n",
      "[0/2][74/1205] Loss_D: 0.0002 Loss_G: 15.4419 D(x): 1.0000 D(G(z)): 0.0002 / 0.0001\n",
      "[0/2][75/1205] Loss_D: 0.1885 Loss_G: 18.0204 D(x): 1.0000 D(G(z)): 0.1425 / 0.0003\n",
      "[0/2][76/1205] Loss_D: 0.0233 Loss_G: 17.3590 D(x): 0.9862 D(G(z)): 0.0088 / 0.0001\n",
      "[0/2][77/1205] Loss_D: 0.3172 Loss_G: 6.7851 D(x): 0.8425 D(G(z)): 0.0072 / 0.0413\n",
      "[0/2][78/1205] Loss_D: 2.7190 Loss_G: 29.1711 D(x): 1.0000 D(G(z)): 0.5903 / 0.0000\n",
      "[0/2][79/1205] Loss_D: 0.0054 Loss_G: 30.9552 D(x): 0.9946 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][80/1205] Loss_D: 5.0140 Loss_G: 18.6484 D(x): 0.0535 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][81/1205] Loss_D: 0.0007 Loss_G: 0.7323 D(x): 1.0000 D(G(z)): 0.0007 / 0.7735\n",
      "[0/2][82/1205] Loss_D: 5.7068 Loss_G: 28.2171 D(x): 1.0000 D(G(z)): 0.9722 / 0.0000\n",
      "[0/2][83/1205] Loss_D: 0.0003 Loss_G: 25.3512 D(x): 1.0000 D(G(z)): 0.0003 / 0.0000\n",
      "[0/2][84/1205] Loss_D: 0.0063 Loss_G: 29.6075 D(x): 0.9938 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][85/1205] Loss_D: 1.1529 Loss_G: 19.5584 D(x): 0.7816 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][86/1205] Loss_D: 0.9550 Loss_G: 13.1909 D(x): 0.7708 D(G(z)): 0.0005 / 0.0317\n",
      "[0/2][87/1205] Loss_D: 4.5856 Loss_G: 20.7744 D(x): 1.0000 D(G(z)): 0.6255 / 0.0000\n",
      "[0/2][88/1205] Loss_D: 0.1039 Loss_G: 21.1915 D(x): 0.9191 D(G(z)): 0.0003 / 0.0000\n",
      "[0/2][89/1205] Loss_D: 0.1747 Loss_G: 16.7739 D(x): 0.8815 D(G(z)): 0.0006 / 0.0001\n",
      "[0/2][90/1205] Loss_D: 0.1029 Loss_G: 12.2165 D(x): 0.9338 D(G(z)): 0.0266 / 0.0054\n",
      "[0/2][91/1205] Loss_D: 2.4364 Loss_G: 24.8649 D(x): 0.9888 D(G(z)): 0.6198 / 0.0000\n",
      "[0/2][92/1205] Loss_D: 4.7540 Loss_G: 9.4478 D(x): 0.0898 D(G(z)): 0.0000 / 0.0022\n",
      "[0/2][93/1205] Loss_D: 0.4607 Loss_G: 8.3993 D(x): 1.0000 D(G(z)): 0.2727 / 0.0028\n",
      "[0/2][94/1205] Loss_D: 4.0274 Loss_G: 22.8182 D(x): 0.9965 D(G(z)): 0.7032 / 0.0000\n",
      "[0/2][95/1205] Loss_D: 0.0019 Loss_G: 20.3770 D(x): 0.9985 D(G(z)): 0.0004 / 0.0000\n",
      "[0/2][96/1205] Loss_D: 1.6229 Loss_G: 13.1871 D(x): 0.6027 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][97/1205] Loss_D: 0.1085 Loss_G: 9.3801 D(x): 1.0000 D(G(z)): 0.0840 / 0.0005\n",
      "[0/2][98/1205] Loss_D: 0.3911 Loss_G: 7.8469 D(x): 0.8851 D(G(z)): 0.1651 / 0.0011\n",
      "[0/2][99/1205] Loss_D: 1.2659 Loss_G: 22.7202 D(x): 0.9894 D(G(z)): 0.5486 / 0.0000\n",
      "[0/2][100/1205] Loss_D: 7.8723 Loss_G: 3.2972 D(x): 0.0080 D(G(z)): 0.0000 / 0.0678\n",
      "[0/2][101/1205] Loss_D: 2.6477 Loss_G: 14.0924 D(x): 1.0000 D(G(z)): 0.6371 / 0.0000\n",
      "[0/2][102/1205] Loss_D: 1.3048 Loss_G: 10.2324 D(x): 0.7694 D(G(z)): 0.0646 / 0.0001\n",
      "[0/2][103/1205] Loss_D: 1.8304 Loss_G: 0.0227 D(x): 0.4212 D(G(z)): 0.0194 / 0.9777\n",
      "[0/2][104/1205] Loss_D: 6.8855 Loss_G: 17.8732 D(x): 0.9936 D(G(z)): 0.9945 / 0.0000\n",
      "[0/2][105/1205] Loss_D: 0.9515 Loss_G: 12.3609 D(x): 0.5199 D(G(z)): 0.0000 / 0.0000\n",
      "[0/2][106/1205] Loss_D: 0.0135 Loss_G: 8.6930 D(x): 0.9911 D(G(z)): 0.0044 / 0.0052\n",
      "[0/2][107/1205] Loss_D: 1.3297 Loss_G: 18.2804 D(x): 0.9990 D(G(z)): 0.6626 / 0.0000\n",
      "[0/2][108/1205] Loss_D: 6.2500 Loss_G: 1.7512 D(x): 0.0032 D(G(z)): 0.0000 / 0.3797\n",
      "[0/2][109/1205] Loss_D: 5.0917 Loss_G: 7.7737 D(x): 0.6014 D(G(z)): 0.7834 / 0.0021\n",
      "[0/2][110/1205] Loss_D: 0.1763 Loss_G: 9.7474 D(x): 0.9928 D(G(z)): 0.1542 / 0.0001\n",
      "[0/2][111/1205] Loss_D: 0.6408 Loss_G: 3.0712 D(x): 0.6393 D(G(z)): 0.0058 / 0.0761\n",
      "[0/2][112/1205] Loss_D: 2.9417 Loss_G: 15.1639 D(x): 0.7726 D(G(z)): 0.8787 / 0.0000\n",
      "[0/2][113/1205] Loss_D: 5.7282 Loss_G: 1.5327 D(x): 0.0124 D(G(z)): 0.0000 / 0.3389\n",
      "[0/2][114/1205] Loss_D: 3.6978 Loss_G: 9.6540 D(x): 0.7681 D(G(z)): 0.9097 / 0.0001\n",
      "[0/2][115/1205] Loss_D: 1.4072 Loss_G: 1.5840 D(x): 0.3175 D(G(z)): 0.0080 / 0.2908\n",
      "[0/2][116/1205] Loss_D: 1.4276 Loss_G: 7.8933 D(x): 0.9929 D(G(z)): 0.6527 / 0.0012\n",
      "[0/2][117/1205] Loss_D: 1.1748 Loss_G: 1.0479 D(x): 0.3944 D(G(z)): 0.0214 / 0.3911\n",
      "[0/2][118/1205] Loss_D: 2.1619 Loss_G: 10.5728 D(x): 0.9851 D(G(z)): 0.8639 / 0.0000\n",
      "[0/2][119/1205] Loss_D: 3.2424 Loss_G: 0.4413 D(x): 0.0447 D(G(z)): 0.0024 / 0.6486\n",
      "[0/2][120/1205] Loss_D: 1.9043 Loss_G: 6.2462 D(x): 0.8662 D(G(z)): 0.7786 / 0.0040\n",
      "[0/2][121/1205] Loss_D: 2.2174 Loss_G: 1.7717 D(x): 0.4096 D(G(z)): 0.0517 / 0.2123\n",
      "[0/2][122/1205] Loss_D: 1.0020 Loss_G: 7.4211 D(x): 0.9790 D(G(z)): 0.6117 / 0.0010\n",
      "[0/2][123/1205] Loss_D: 0.9638 Loss_G: 1.5728 D(x): 0.3919 D(G(z)): 0.0085 / 0.2323\n",
      "[0/2][124/1205] Loss_D: 1.1594 Loss_G: 5.9318 D(x): 0.9007 D(G(z)): 0.6273 / 0.0036\n",
      "[0/2][125/1205] Loss_D: 1.4420 Loss_G: 0.4362 D(x): 0.2997 D(G(z)): 0.0236 / 0.6620\n",
      "[0/2][126/1205] Loss_D: 2.6776 Loss_G: 7.6629 D(x): 0.9568 D(G(z)): 0.9160 / 0.0006\n",
      "[0/2][127/1205] Loss_D: 3.9173 Loss_G: 0.8823 D(x): 0.0278 D(G(z)): 0.0039 / 0.4442\n",
      "[0/2][128/1205] Loss_D: 0.8682 Loss_G: 3.0680 D(x): 0.9308 D(G(z)): 0.5141 / 0.0624\n",
      "[0/2][129/1205] Loss_D: 0.8053 Loss_G: 5.5056 D(x): 0.8363 D(G(z)): 0.4417 / 0.0057\n",
      "[0/2][130/1205] Loss_D: 2.5096 Loss_G: 0.1081 D(x): 0.0958 D(G(z)): 0.0268 / 0.9006\n",
      "[0/2][131/1205] Loss_D: 3.1560 Loss_G: 5.0275 D(x): 0.9953 D(G(z)): 0.9547 / 0.0089\n",
      "[0/2][132/1205] Loss_D: 1.2330 Loss_G: 1.5793 D(x): 0.3577 D(G(z)): 0.0416 / 0.2465\n",
      "[0/2][133/1205] Loss_D: 1.6013 Loss_G: 3.1199 D(x): 0.6627 D(G(z)): 0.6414 / 0.0539\n",
      "[0/2][134/1205] Loss_D: 2.3601 Loss_G: 0.2458 D(x): 0.1762 D(G(z)): 0.1493 / 0.7888\n",
      "[0/2][135/1205] Loss_D: 2.3130 Loss_G: 5.8737 D(x): 0.9817 D(G(z)): 0.8866 / 0.0032\n",
      "[0/2][136/1205] Loss_D: 2.0689 Loss_G: 0.9584 D(x): 0.1674 D(G(z)): 0.0204 / 0.3918\n",
      "[0/2][137/1205] Loss_D: 0.9116 Loss_G: 1.3228 D(x): 0.7322 D(G(z)): 0.4361 / 0.2849\n",
      "[0/2][138/1205] Loss_D: 1.0807 Loss_G: 5.2487 D(x): 0.8870 D(G(z)): 0.6041 / 0.0057\n",
      "[0/2][139/1205] Loss_D: 2.6131 Loss_G: 0.3776 D(x): 0.0890 D(G(z)): 0.0272 / 0.6909\n",
      "[0/2][140/1205] Loss_D: 1.4204 Loss_G: 3.4124 D(x): 0.9594 D(G(z)): 0.7287 / 0.0419\n",
      "[0/2][141/1205] Loss_D: 0.8792 Loss_G: 1.4461 D(x): 0.4648 D(G(z)): 0.0831 / 0.2710\n",
      "[0/2][142/1205] Loss_D: 0.8678 Loss_G: 3.4666 D(x): 0.9271 D(G(z)): 0.5107 / 0.0462\n",
      "[0/2][143/1205] Loss_D: 0.9127 Loss_G: 0.8274 D(x): 0.4779 D(G(z)): 0.1058 / 0.4487\n",
      "[0/2][144/1205] Loss_D: 1.1173 Loss_G: 5.6367 D(x): 0.9859 D(G(z)): 0.6640 / 0.0037\n",
      "[0/2][145/1205] Loss_D: 5.8774 Loss_G: 1.7729 D(x): 0.1304 D(G(z)): 0.0194 / 0.1802\n",
      "[0/2][146/1205] Loss_D: 1.2174 Loss_G: 0.4606 D(x): 0.4670 D(G(z)): 0.3229 / 0.6616\n",
      "[0/2][147/1205] Loss_D: 1.9245 Loss_G: 3.1765 D(x): 0.7615 D(G(z)): 0.7536 / 0.0495\n",
      "[0/2][148/1205] Loss_D: 1.0731 Loss_G: 1.6989 D(x): 0.5053 D(G(z)): 0.1368 / 0.2327\n",
      "[0/2][149/1205] Loss_D: 0.5993 Loss_G: 2.1975 D(x): 0.8358 D(G(z)): 0.3261 / 0.1261\n",
      "[0/2][150/1205] Loss_D: 0.5278 Loss_G: 1.9408 D(x): 0.7112 D(G(z)): 0.1483 / 0.1550\n",
      "[0/2][151/1205] Loss_D: 0.9048 Loss_G: 0.5121 D(x): 0.5599 D(G(z)): 0.2227 / 0.6048\n",
      "[0/2][152/1205] Loss_D: 1.5066 Loss_G: 4.4937 D(x): 0.9243 D(G(z)): 0.7519 / 0.0121\n",
      "[0/2][153/1205] Loss_D: 1.5796 Loss_G: 1.2706 D(x): 0.2388 D(G(z)): 0.0570 / 0.3201\n",
      "[0/2][154/1205] Loss_D: 0.5672 Loss_G: 2.5462 D(x): 0.9587 D(G(z)): 0.4031 / 0.0808\n",
      "[0/2][155/1205] Loss_D: 0.5130 Loss_G: 2.2776 D(x): 0.7145 D(G(z)): 0.1488 / 0.1079\n",
      "[0/2][156/1205] Loss_D: 0.4595 Loss_G: 2.3564 D(x): 0.8463 D(G(z)): 0.2502 / 0.0965\n",
      "[0/2][157/1205] Loss_D: 0.6565 Loss_G: 1.5242 D(x): 0.6717 D(G(z)): 0.2244 / 0.2199\n",
      "[0/2][158/1205] Loss_D: 0.9771 Loss_G: 2.4023 D(x): 0.7342 D(G(z)): 0.4640 / 0.0979\n",
      "[0/2][159/1205] Loss_D: 1.3979 Loss_G: 0.2257 D(x): 0.2893 D(G(z)): 0.1052 / 0.7992\n",
      "[0/2][160/1205] Loss_D: 2.7974 Loss_G: 3.4979 D(x): 0.9321 D(G(z)): 0.9295 / 0.0429\n",
      "[0/2][161/1205] Loss_D: 1.3077 Loss_G: 1.7495 D(x): 0.3645 D(G(z)): 0.0749 / 0.2058\n",
      "[0/2][162/1205] Loss_D: 0.6338 Loss_G: 1.2844 D(x): 0.7610 D(G(z)): 0.2707 / 0.3071\n",
      "[0/2][163/1205] Loss_D: 1.3190 Loss_G: 4.3271 D(x): 0.9182 D(G(z)): 0.7018 / 0.0165\n",
      "[0/2][164/1205] Loss_D: 3.6803 Loss_G: 0.5731 D(x): 0.0484 D(G(z)): 0.0621 / 0.5996\n",
      "[0/2][165/1205] Loss_D: 1.1192 Loss_G: 2.0230 D(x): 0.8727 D(G(z)): 0.6164 / 0.1342\n",
      "[0/2][166/1205] Loss_D: 0.7142 Loss_G: 3.5700 D(x): 0.8522 D(G(z)): 0.3820 / 0.0562\n",
      "[0/2][167/1205] Loss_D: 2.6928 Loss_G: 0.3468 D(x): 0.1944 D(G(z)): 0.0665 / 0.7114\n",
      "[0/2][168/1205] Loss_D: 2.2342 Loss_G: 2.7799 D(x): 0.9777 D(G(z)): 0.8744 / 0.0777\n",
      "[0/2][169/1205] Loss_D: 2.7717 Loss_G: 0.7984 D(x): 0.1505 D(G(z)): 0.4562 / 0.5004\n",
      "[0/2][170/1205] Loss_D: 1.0131 Loss_G: 3.1191 D(x): 0.8606 D(G(z)): 0.5638 / 0.0480\n",
      "[0/2][171/1205] Loss_D: 1.7666 Loss_G: 0.2374 D(x): 0.2103 D(G(z)): 0.0610 / 0.7891\n",
      "[0/2][172/1205] Loss_D: 2.3487 Loss_G: 2.9180 D(x): 0.9471 D(G(z)): 0.8941 / 0.0622\n",
      "[0/2][173/1205] Loss_D: 1.3885 Loss_G: 0.8553 D(x): 0.2940 D(G(z)): 0.0544 / 0.4394\n",
      "[0/2][174/1205] Loss_D: 1.9843 Loss_G: 1.3961 D(x): 0.6689 D(G(z)): 0.7587 / 0.3069\n",
      "[0/2][175/1205] Loss_D: 0.5948 Loss_G: 3.6150 D(x): 0.8356 D(G(z)): 0.3314 / 0.0306\n",
      "[0/2][176/1205] Loss_D: 1.2918 Loss_G: 0.5789 D(x): 0.3650 D(G(z)): 0.1329 / 0.5833\n",
      "[0/2][177/1205] Loss_D: 1.9885 Loss_G: 2.5667 D(x): 0.7468 D(G(z)): 0.7896 / 0.0812\n",
      "[0/2][178/1205] Loss_D: 0.7336 Loss_G: 1.8576 D(x): 0.5919 D(G(z)): 0.1500 / 0.1666\n",
      "[0/2][179/1205] Loss_D: 0.9884 Loss_G: 0.7352 D(x): 0.5594 D(G(z)): 0.3129 / 0.4913\n",
      "[0/2][180/1205] Loss_D: 1.4075 Loss_G: 2.5150 D(x): 0.6984 D(G(z)): 0.6362 / 0.0834\n",
      "[0/2][181/1205] Loss_D: 1.9882 Loss_G: 0.2619 D(x): 0.2075 D(G(z)): 0.1959 / 0.7824\n",
      "[0/2][182/1205] Loss_D: 2.1062 Loss_G: 3.9065 D(x): 0.9734 D(G(z)): 0.8408 / 0.0220\n",
      "[0/2][183/1205] Loss_D: 2.6247 Loss_G: 0.7222 D(x): 0.1156 D(G(z)): 0.0482 / 0.4939\n",
      "[0/2][184/1205] Loss_D: 1.1260 Loss_G: 2.0471 D(x): 0.8441 D(G(z)): 0.6044 / 0.1356\n",
      "[0/2][185/1205] Loss_D: 0.8036 Loss_G: 1.3913 D(x): 0.5656 D(G(z)): 0.2019 / 0.2511\n",
      "[0/2][186/1205] Loss_D: 1.1291 Loss_G: 1.0243 D(x): 0.5529 D(G(z)): 0.3685 / 0.3663\n",
      "[0/2][187/1205] Loss_D: 0.8726 Loss_G: 3.7699 D(x): 0.9460 D(G(z)): 0.5510 / 0.0237\n",
      "[0/2][188/1205] Loss_D: 1.4237 Loss_G: 0.6395 D(x): 0.2890 D(G(z)): 0.0958 / 0.5592\n",
      "[0/2][189/1205] Loss_D: 1.2012 Loss_G: 1.4996 D(x): 0.7267 D(G(z)): 0.5710 / 0.2255\n",
      "[0/2][190/1205] Loss_D: 0.8511 Loss_G: 2.7098 D(x): 0.7414 D(G(z)): 0.3997 / 0.0691\n",
      "[0/2][191/1205] Loss_D: 0.7794 Loss_G: 1.5477 D(x): 0.5698 D(G(z)): 0.0985 / 0.2246\n",
      "[0/2][192/1205] Loss_D: 1.6242 Loss_G: 0.5422 D(x): 0.4322 D(G(z)): 0.5205 / 0.5886\n",
      "[0/2][193/1205] Loss_D: 1.4338 Loss_G: 3.3745 D(x): 0.7772 D(G(z)): 0.6822 / 0.0375\n",
      "[0/2][194/1205] Loss_D: 1.2152 Loss_G: 0.6012 D(x): 0.3371 D(G(z)): 0.1088 / 0.5512\n",
      "[0/2][195/1205] Loss_D: 1.3948 Loss_G: 2.1987 D(x): 0.7762 D(G(z)): 0.6706 / 0.1207\n",
      "[0/2][196/1205] Loss_D: 1.1891 Loss_G: 0.6154 D(x): 0.3793 D(G(z)): 0.1898 / 0.5454\n",
      "[0/2][197/1205] Loss_D: 1.4497 Loss_G: 2.8452 D(x): 0.7925 D(G(z)): 0.6919 / 0.0608\n",
      "[0/2][198/1205] Loss_D: 9.3730 Loss_G: 2.1451 D(x): 0.0029 D(G(z)): 0.1051 / 0.1240\n",
      "[0/2][199/1205] Loss_D: 2.0947 Loss_G: 0.0584 D(x): 0.1623 D(G(z)): 0.1911 / 0.9433\n",
      "[0/2][200/1205] Loss_D: 2.7656 Loss_G: 2.2523 D(x): 0.9728 D(G(z)): 0.9347 / 0.1086\n",
      "[0/2][201/1205] Loss_D: 1.1433 Loss_G: 3.9268 D(x): 0.7393 D(G(z)): 0.5643 / 0.0219\n",
      "[0/2][202/1205] Loss_D: 3.1362 Loss_G: 0.8738 D(x): 0.0468 D(G(z)): 0.0540 / 0.4401\n",
      "[0/2][203/1205] Loss_D: 1.3025 Loss_G: 0.1328 D(x): 0.4265 D(G(z)): 0.2162 / 0.8763\n",
      "[0/2][204/1205] Loss_D: 3.5250 Loss_G: 0.8848 D(x): 0.9596 D(G(z)): 0.9659 / 0.4253\n",
      "[0/2][205/1205] Loss_D: 0.6065 Loss_G: 3.8812 D(x): 0.8800 D(G(z)): 0.3596 / 0.0290\n",
      "[0/2][206/1205] Loss_D: 2.6470 Loss_G: 0.7246 D(x): 0.1010 D(G(z)): 0.0781 / 0.4885\n",
      "[0/2][207/1205] Loss_D: 0.9571 Loss_G: 0.7766 D(x): 0.7513 D(G(z)): 0.4776 / 0.4633\n",
      "[0/2][208/1205] Loss_D: 1.1423 Loss_G: 1.3981 D(x): 0.6655 D(G(z)): 0.5064 / 0.2574\n",
      "[0/2][209/1205] Loss_D: 0.8915 Loss_G: 1.7419 D(x): 0.6745 D(G(z)): 0.3706 / 0.1768\n",
      "[0/2][210/1205] Loss_D: 1.1318 Loss_G: 0.7443 D(x): 0.4226 D(G(z)): 0.2240 / 0.4818\n",
      "[0/2][211/1205] Loss_D: 1.2412 Loss_G: 1.8795 D(x): 0.8426 D(G(z)): 0.6454 / 0.1707\n",
      "[0/2][212/1205] Loss_D: 1.2010 Loss_G: 0.8045 D(x): 0.3862 D(G(z)): 0.1921 / 0.4535\n",
      "[0/2][213/1205] Loss_D: 1.0374 Loss_G: 1.6932 D(x): 0.7564 D(G(z)): 0.5229 / 0.1910\n",
      "[0/2][214/1205] Loss_D: 1.1197 Loss_G: 1.1058 D(x): 0.4861 D(G(z)): 0.2954 / 0.3724\n",
      "[0/2][215/1205] Loss_D: 1.0097 Loss_G: 2.0997 D(x): 0.7803 D(G(z)): 0.5100 / 0.1399\n",
      "[0/2][216/1205] Loss_D: 0.8073 Loss_G: 1.3390 D(x): 0.5926 D(G(z)): 0.2408 / 0.2634\n",
      "[0/2][217/1205] Loss_D: 0.8571 Loss_G: 0.9501 D(x): 0.6273 D(G(z)): 0.3040 / 0.4001\n",
      "[0/2][218/1205] Loss_D: 1.2851 Loss_G: 2.0416 D(x): 0.6588 D(G(z)): 0.5178 / 0.1349\n",
      "[0/2][219/1205] Loss_D: 0.7458 Loss_G: 1.5842 D(x): 0.6120 D(G(z)): 0.1708 / 0.2187\n",
      "[0/2][220/1205] Loss_D: 1.0110 Loss_G: 2.1645 D(x): 0.7096 D(G(z)): 0.4618 / 0.1200\n",
      "[0/2][221/1205] Loss_D: 1.0735 Loss_G: 0.4273 D(x): 0.4023 D(G(z)): 0.1283 / 0.6636\n",
      "[0/2][222/1205] Loss_D: 1.4329 Loss_G: 3.0356 D(x): 0.9000 D(G(z)): 0.7270 / 0.0552\n",
      "[0/2][223/1205] Loss_D: 1.1087 Loss_G: 1.1025 D(x): 0.3980 D(G(z)): 0.1123 / 0.3423\n",
      "[0/2][224/1205] Loss_D: 0.8396 Loss_G: 1.9158 D(x): 0.8200 D(G(z)): 0.4619 / 0.1597\n",
      "[0/2][225/1205] Loss_D: 1.0179 Loss_G: 0.8641 D(x): 0.4747 D(G(z)): 0.2180 / 0.4353\n",
      "[0/2][226/1205] Loss_D: 1.4977 Loss_G: 2.2082 D(x): 0.7483 D(G(z)): 0.6289 / 0.1164\n",
      "[0/2][227/1205] Loss_D: 0.9553 Loss_G: 1.4209 D(x): 0.5414 D(G(z)): 0.2044 / 0.2560\n",
      "[0/2][228/1205] Loss_D: 0.6548 Loss_G: 2.2019 D(x): 0.8273 D(G(z)): 0.3561 / 0.1200\n",
      "[0/2][229/1205] Loss_D: 1.8876 Loss_G: 0.1301 D(x): 0.2040 D(G(z)): 0.2096 / 0.8786\n",
      "[0/2][230/1205] Loss_D: 1.9221 Loss_G: 3.0083 D(x): 0.9512 D(G(z)): 0.8218 / 0.0633\n",
      "[0/2][231/1205] Loss_D: 0.6758 Loss_G: 2.5006 D(x): 0.6113 D(G(z)): 0.1317 / 0.0952\n",
      "[0/2][232/1205] Loss_D: 1.1734 Loss_G: 0.3574 D(x): 0.4195 D(G(z)): 0.1141 / 0.7065\n",
      "[0/2][233/1205] Loss_D: 2.0268 Loss_G: 3.0495 D(x): 0.8982 D(G(z)): 0.8442 / 0.0610\n",
      "[0/2][234/1205] Loss_D: 1.6987 Loss_G: 0.7731 D(x): 0.2556 D(G(z)): 0.2123 / 0.4738\n",
      "[0/2][235/1205] Loss_D: 1.1359 Loss_G: 0.9262 D(x): 0.6234 D(G(z)): 0.4680 / 0.3971\n",
      "[0/2][236/1205] Loss_D: 0.8580 Loss_G: 2.5712 D(x): 0.8378 D(G(z)): 0.4843 / 0.0861\n",
      "[0/2][237/1205] Loss_D: 1.3493 Loss_G: 0.6784 D(x): 0.3708 D(G(z)): 0.2296 / 0.5166\n",
      "[0/2][238/1205] Loss_D: 1.3213 Loss_G: 1.5706 D(x): 0.6814 D(G(z)): 0.5842 / 0.2156\n",
      "[0/2][239/1205] Loss_D: 1.0722 Loss_G: 2.0678 D(x): 0.6165 D(G(z)): 0.4231 / 0.1298\n",
      "[0/2][240/1205] Loss_D: 1.7225 Loss_G: 0.1831 D(x): 0.2572 D(G(z)): 0.1513 / 0.8345\n",
      "[0/2][241/1205] Loss_D: 2.4307 Loss_G: 3.3635 D(x): 0.9605 D(G(z)): 0.9075 / 0.0359\n",
      "[0/2][242/1205] Loss_D: 1.0093 Loss_G: 2.1930 D(x): 0.4338 D(G(z)): 0.0961 / 0.1495\n",
      "[0/2][243/1205] Loss_D: 1.3343 Loss_G: 0.2548 D(x): 0.3741 D(G(z)): 0.1796 / 0.7794\n",
      "[0/2][244/1205] Loss_D: 2.5920 Loss_G: 2.1409 D(x): 0.9501 D(G(z)): 0.9122 / 0.1339\n",
      "[0/2][245/1205] Loss_D: 1.8441 Loss_G: 0.9392 D(x): 0.2250 D(G(z)): 0.2324 / 0.4022\n",
      "[0/2][246/1205] Loss_D: 1.0488 Loss_G: 1.0689 D(x): 0.6594 D(G(z)): 0.4631 / 0.3589\n",
      "[0/2][247/1205] Loss_D: 1.1581 Loss_G: 2.5431 D(x): 0.7752 D(G(z)): 0.5537 / 0.1059\n",
      "[0/2][248/1205] Loss_D: 1.8183 Loss_G: 0.2844 D(x): 0.2309 D(G(z)): 0.1646 / 0.7558\n",
      "[0/2][249/1205] Loss_D: 1.8146 Loss_G: 2.5256 D(x): 0.8893 D(G(z)): 0.8090 / 0.0964\n",
      "[0/2][250/1205] Loss_D: 1.3327 Loss_G: 0.8043 D(x): 0.3007 D(G(z)): 0.1077 / 0.4577\n",
      "[0/2][251/1205] Loss_D: 1.5173 Loss_G: 2.2504 D(x): 0.8074 D(G(z)): 0.7255 / 0.1079\n",
      "[0/2][252/1205] Loss_D: 1.2982 Loss_G: 0.8140 D(x): 0.3570 D(G(z)): 0.1788 / 0.4496\n",
      "[0/2][253/1205] Loss_D: 1.2188 Loss_G: 1.7497 D(x): 0.7514 D(G(z)): 0.5911 / 0.1768\n",
      "[0/2][254/1205] Loss_D: 1.1703 Loss_G: 1.4549 D(x): 0.5244 D(G(z)): 0.4012 / 0.2385\n",
      "[0/2][255/1205] Loss_D: 1.1215 Loss_G: 0.6708 D(x): 0.4484 D(G(z)): 0.2544 / 0.5183\n",
      "[0/2][256/1205] Loss_D: 1.3742 Loss_G: 2.4164 D(x): 0.8061 D(G(z)): 0.6632 / 0.0929\n",
      "[0/2][257/1205] Loss_D: 2.0051 Loss_G: 0.5374 D(x): 0.1680 D(G(z)): 0.1718 / 0.5891\n",
      "[0/2][258/1205] Loss_D: 1.3731 Loss_G: 1.3443 D(x): 0.7149 D(G(z)): 0.6361 / 0.2742\n",
      "[0/2][259/1205] Loss_D: 0.6421 Loss_G: 2.7593 D(x): 0.7919 D(G(z)): 0.3308 / 0.0634\n",
      "[0/2][260/1205] Loss_D: 1.7206 Loss_G: 0.2971 D(x): 0.2140 D(G(z)): 0.1053 / 0.7443\n",
      "[0/2][261/1205] Loss_D: 1.9589 Loss_G: 1.4490 D(x): 0.8850 D(G(z)): 0.8021 / 0.2911\n",
      "[0/2][262/1205] Loss_D: 0.9090 Loss_G: 1.8487 D(x): 0.6002 D(G(z)): 0.3157 / 0.1617\n",
      "[0/2][263/1205] Loss_D: 1.0930 Loss_G: 0.6896 D(x): 0.4483 D(G(z)): 0.2151 / 0.5400\n",
      "[0/2][264/1205] Loss_D: 1.1614 Loss_G: 1.7084 D(x): 0.8287 D(G(z)): 0.6190 / 0.1868\n",
      "[0/2][265/1205] Loss_D: 1.1100 Loss_G: 1.0626 D(x): 0.4643 D(G(z)): 0.2644 / 0.3598\n",
      "[0/2][266/1205] Loss_D: 1.2163 Loss_G: 1.3977 D(x): 0.6486 D(G(z)): 0.5333 / 0.2486\n",
      "[0/2][267/1205] Loss_D: 1.0440 Loss_G: 0.9716 D(x): 0.5442 D(G(z)): 0.2627 / 0.3890\n",
      "[0/2][268/1205] Loss_D: 1.0190 Loss_G: 1.9263 D(x): 0.7670 D(G(z)): 0.5200 / 0.1484\n",
      "[0/2][269/1205] Loss_D: 1.2244 Loss_G: 0.8043 D(x): 0.4479 D(G(z)): 0.2511 / 0.4542\n",
      "[0/2][270/1205] Loss_D: 0.8649 Loss_G: 1.4411 D(x): 0.7313 D(G(z)): 0.4148 / 0.2429\n",
      "[0/2][271/1205] Loss_D: 1.3500 Loss_G: 0.8177 D(x): 0.4636 D(G(z)): 0.3887 / 0.4509\n",
      "[0/2][272/1205] Loss_D: 1.3714 Loss_G: 2.3345 D(x): 0.7006 D(G(z)): 0.6344 / 0.0995\n",
      "[0/2][273/1205] Loss_D: 1.0492 Loss_G: 1.0501 D(x): 0.4427 D(G(z)): 0.1746 / 0.3521\n",
      "[0/2][274/1205] Loss_D: 0.8497 Loss_G: 0.9578 D(x): 0.6695 D(G(z)): 0.3418 / 0.3863\n",
      "[0/2][275/1205] Loss_D: 1.3067 Loss_G: 2.2063 D(x): 0.7248 D(G(z)): 0.6086 / 0.1136\n",
      "[0/2][276/1205] Loss_D: 1.1191 Loss_G: 0.8237 D(x): 0.4280 D(G(z)): 0.1372 / 0.4473\n",
      "[0/2][277/1205] Loss_D: 0.9475 Loss_G: 1.2030 D(x): 0.7099 D(G(z)): 0.4434 / 0.3030\n",
      "[0/2][278/1205] Loss_D: 1.0248 Loss_G: 2.9309 D(x): 0.7476 D(G(z)): 0.5026 / 0.0558\n",
      "[0/2][279/1205] Loss_D: 2.1225 Loss_G: 0.2313 D(x): 0.1564 D(G(z)): 0.1043 / 0.7956\n",
      "[0/2][280/1205] Loss_D: 1.6820 Loss_G: 2.2435 D(x): 0.8660 D(G(z)): 0.7798 / 0.1185\n",
      "[0/2][281/1205] Loss_D: 1.0105 Loss_G: 1.2108 D(x): 0.4694 D(G(z)): 0.1868 / 0.3216\n",
      "[0/2][282/1205] Loss_D: 0.8920 Loss_G: 2.1208 D(x): 0.7759 D(G(z)): 0.4588 / 0.1331\n",
      "[0/2][283/1205] Loss_D: 1.0181 Loss_G: 1.0928 D(x): 0.5505 D(G(z)): 0.2880 / 0.3510\n",
      "[0/2][284/1205] Loss_D: 1.2407 Loss_G: 1.3769 D(x): 0.6665 D(G(z)): 0.5530 / 0.2629\n",
      "[0/2][285/1205] Loss_D: 0.9568 Loss_G: 1.0121 D(x): 0.5663 D(G(z)): 0.2823 / 0.3750\n",
      "[0/2][286/1205] Loss_D: 0.9554 Loss_G: 2.2369 D(x): 0.7908 D(G(z)): 0.5105 / 0.1109\n",
      "[0/2][287/1205] Loss_D: 1.1183 Loss_G: 0.7070 D(x): 0.4136 D(G(z)): 0.1731 / 0.5077\n",
      "[0/2][288/1205] Loss_D: 0.9524 Loss_G: 2.2035 D(x): 0.8342 D(G(z)): 0.5176 / 0.1133\n",
      "[0/2][289/1205] Loss_D: 1.1255 Loss_G: 1.0647 D(x): 0.4802 D(G(z)): 0.2853 / 0.3558\n",
      "[0/2][290/1205] Loss_D: 1.0481 Loss_G: 1.3284 D(x): 0.6352 D(G(z)): 0.4213 / 0.2666\n",
      "[0/2][291/1205] Loss_D: 0.8733 Loss_G: 3.4810 D(x): 0.8620 D(G(z)): 0.5135 / 0.0346\n",
      "[0/2][292/1205] Loss_D: 1.1854 Loss_G: 1.0528 D(x): 0.3930 D(G(z)): 0.0537 / 0.3605\n",
      "[0/2][293/1205] Loss_D: 1.1716 Loss_G: 0.9143 D(x): 0.6155 D(G(z)): 0.4829 / 0.4199\n",
      "[0/2][294/1205] Loss_D: 1.2899 Loss_G: 2.6619 D(x): 0.7054 D(G(z)): 0.5977 / 0.0769\n",
      "[0/2][295/1205] Loss_D: 2.2244 Loss_G: 0.1037 D(x): 0.1316 D(G(z)): 0.1275 / 0.9025\n",
      "[0/2][296/1205] Loss_D: 2.8512 Loss_G: 2.4408 D(x): 0.9455 D(G(z)): 0.9379 / 0.0988\n",
      "[0/2][297/1205] Loss_D: 1.3960 Loss_G: 1.0601 D(x): 0.3769 D(G(z)): 0.2047 / 0.3586\n",
      "[0/2][298/1205] Loss_D: 1.0000 Loss_G: 0.8882 D(x): 0.6134 D(G(z)): 0.3975 / 0.4179\n",
      "[0/2][299/1205] Loss_D: 1.2605 Loss_G: 3.7188 D(x): 0.8289 D(G(z)): 0.6406 / 0.0256\n",
      "[0/2][300/1205] Loss_D: 2.1624 Loss_G: 0.2726 D(x): 0.1399 D(G(z)): 0.0455 / 0.7693\n",
      "[0/2][301/1205] Loss_D: 2.2935 Loss_G: 1.8181 D(x): 0.7909 D(G(z)): 0.8548 / 0.1816\n",
      "[0/2][302/1205] Loss_D: 1.0816 Loss_G: 1.6011 D(x): 0.5391 D(G(z)): 0.3522 / 0.2168\n",
      "[0/2][303/1205] Loss_D: 1.0346 Loss_G: 0.8733 D(x): 0.5167 D(G(z)): 0.2777 / 0.4227\n",
      "[0/2][304/1205] Loss_D: 1.0322 Loss_G: 1.6886 D(x): 0.7419 D(G(z)): 0.5127 / 0.1865\n",
      "[0/2][305/1205] Loss_D: 1.3030 Loss_G: 1.3460 D(x): 0.4294 D(G(z)): 0.3286 / 0.2617\n",
      "[0/2][306/1205] Loss_D: 1.0851 Loss_G: 0.5501 D(x): 0.4692 D(G(z)): 0.2656 / 0.5816\n",
      "[0/2][307/1205] Loss_D: 1.5282 Loss_G: 2.7663 D(x): 0.9042 D(G(z)): 0.7551 / 0.0646\n",
      "[0/2][308/1205] Loss_D: 2.0799 Loss_G: 0.7307 D(x): 0.1725 D(G(z)): 0.0663 / 0.4876\n",
      "[0/2][309/1205] Loss_D: 1.1120 Loss_G: 1.3979 D(x): 0.8087 D(G(z)): 0.5899 / 0.2527\n",
      "[0/2][310/1205] Loss_D: 1.1886 Loss_G: 0.8044 D(x): 0.4505 D(G(z)): 0.3038 / 0.4513\n",
      "[0/2][311/1205] Loss_D: 1.0557 Loss_G: 1.6947 D(x): 0.7591 D(G(z)): 0.5386 / 0.1859\n",
      "[0/2][312/1205] Loss_D: 1.3433 Loss_G: 0.7288 D(x): 0.3877 D(G(z)): 0.3090 / 0.4849\n",
      "[0/2][313/1205] Loss_D: 1.2374 Loss_G: 2.4578 D(x): 0.8317 D(G(z)): 0.6463 / 0.0999\n",
      "[0/2][314/1205] Loss_D: 2.1246 Loss_G: 0.6011 D(x): 0.2358 D(G(z)): 0.1260 / 0.5615\n",
      "[0/2][315/1205] Loss_D: 1.0490 Loss_G: 1.8008 D(x): 0.8329 D(G(z)): 0.5776 / 0.1680\n",
      "[0/2][316/1205] Loss_D: 1.0243 Loss_G: 0.9901 D(x): 0.4878 D(G(z)): 0.2447 / 0.3827\n",
      "[0/2][317/1205] Loss_D: 1.1810 Loss_G: 1.5348 D(x): 0.7165 D(G(z)): 0.5533 / 0.2341\n",
      "[0/2][318/1205] Loss_D: 1.1332 Loss_G: 0.8231 D(x): 0.4643 D(G(z)): 0.2949 / 0.4437\n",
      "[0/2][319/1205] Loss_D: 1.5228 Loss_G: 1.4157 D(x): 0.6777 D(G(z)): 0.6523 / 0.2485\n",
      "[0/2][320/1205] Loss_D: 0.5635 Loss_G: 2.4751 D(x): 0.7761 D(G(z)): 0.2556 / 0.0966\n",
      "[0/2][321/1205] Loss_D: 1.1879 Loss_G: 0.4624 D(x): 0.3796 D(G(z)): 0.1345 / 0.6375\n",
      "[0/2][322/1205] Loss_D: 1.5313 Loss_G: 2.0605 D(x): 0.8647 D(G(z)): 0.7025 / 0.1428\n",
      "[0/2][323/1205] Loss_D: 1.0683 Loss_G: 1.1182 D(x): 0.4796 D(G(z)): 0.2396 / 0.3288\n",
      "[0/2][324/1205] Loss_D: 1.1595 Loss_G: 0.4030 D(x): 0.4538 D(G(z)): 0.2935 / 0.6819\n",
      "[0/2][325/1205] Loss_D: 1.9483 Loss_G: 2.8844 D(x): 0.9323 D(G(z)): 0.8362 / 0.0578\n",
      "[0/2][326/1205] Loss_D: 1.6262 Loss_G: 1.0169 D(x): 0.2248 D(G(z)): 0.0856 / 0.3622\n",
      "[0/2][327/1205] Loss_D: 1.3290 Loss_G: 0.3790 D(x): 0.4496 D(G(z)): 0.3591 / 0.6888\n",
      "[0/2][328/1205] Loss_D: 2.0765 Loss_G: 2.3808 D(x): 0.8760 D(G(z)): 0.8509 / 0.0954\n",
      "[0/2][329/1205] Loss_D: 2.0535 Loss_G: 0.8345 D(x): 0.1934 D(G(z)): 0.1536 / 0.4372\n",
      "[0/2][330/1205] Loss_D: 0.8278 Loss_G: 1.3101 D(x): 0.7807 D(G(z)): 0.4291 / 0.2731\n",
      "[0/2][331/1205] Loss_D: 1.1721 Loss_G: 1.4870 D(x): 0.6211 D(G(z)): 0.4584 / 0.2286\n",
      "[0/2][332/1205] Loss_D: 1.2231 Loss_G: 0.7983 D(x): 0.4426 D(G(z)): 0.3239 / 0.4522\n",
      "[0/2][333/1205] Loss_D: 1.2236 Loss_G: 1.6944 D(x): 0.7158 D(G(z)): 0.5826 / 0.1981\n",
      "[0/2][334/1205] Loss_D: 1.4920 Loss_G: 0.8109 D(x): 0.3490 D(G(z)): 0.3158 / 0.4470\n",
      "[0/2][335/1205] Loss_D: 1.0215 Loss_G: 2.0548 D(x): 0.7942 D(G(z)): 0.5362 / 0.1360\n",
      "[0/2][336/1205] Loss_D: 1.1871 Loss_G: 1.2360 D(x): 0.4640 D(G(z)): 0.2843 / 0.2995\n",
      "[0/2][337/1205] Loss_D: 1.0720 Loss_G: 0.7220 D(x): 0.5282 D(G(z)): 0.3310 / 0.4905\n",
      "[0/2][338/1205] Loss_D: 1.0683 Loss_G: 2.7932 D(x): 0.8353 D(G(z)): 0.5832 / 0.0640\n",
      "[0/2][339/1205] Loss_D: 1.2500 Loss_G: 0.8799 D(x): 0.4017 D(G(z)): 0.1552 / 0.4288\n",
      "[0/2][340/1205] Loss_D: 1.0469 Loss_G: 1.7013 D(x): 0.7423 D(G(z)): 0.5179 / 0.2001\n",
      "[0/2][341/1205] Loss_D: 0.8554 Loss_G: 2.2824 D(x): 0.7326 D(G(z)): 0.3918 / 0.1297\n",
      "[0/2][342/1205] Loss_D: 1.6953 Loss_G: 0.3263 D(x): 0.2419 D(G(z)): 0.1535 / 0.7346\n",
      "[0/2][343/1205] Loss_D: 1.9479 Loss_G: 1.8915 D(x): 0.9079 D(G(z)): 0.8377 / 0.1711\n",
      "[0/2][344/1205] Loss_D: 0.9868 Loss_G: 1.4679 D(x): 0.4887 D(G(z)): 0.2218 / 0.2499\n",
      "[0/2][345/1205] Loss_D: 0.6489 Loss_G: 1.9945 D(x): 0.8123 D(G(z)): 0.3485 / 0.1502\n",
      "[0/2][346/1205] Loss_D: 1.1462 Loss_G: 0.4367 D(x): 0.4333 D(G(z)): 0.2515 / 0.6578\n",
      "[0/2][347/1205] Loss_D: 1.7431 Loss_G: 4.1286 D(x): 0.9200 D(G(z)): 0.8069 / 0.0186\n",
      "[0/2][348/1205] Loss_D: 2.1270 Loss_G: 0.5893 D(x): 0.1329 D(G(z)): 0.0586 / 0.5555\n",
      "[0/2][349/1205] Loss_D: 1.5039 Loss_G: 1.6162 D(x): 0.7941 D(G(z)): 0.7139 / 0.2007\n",
      "[0/2][350/1205] Loss_D: 1.1477 Loss_G: 1.1537 D(x): 0.4787 D(G(z)): 0.2822 / 0.3186\n",
      "[0/2][351/1205] Loss_D: 1.0791 Loss_G: 1.2941 D(x): 0.6302 D(G(z)): 0.4250 / 0.2855\n",
      "[0/2][352/1205] Loss_D: 1.1618 Loss_G: 1.7911 D(x): 0.6289 D(G(z)): 0.4820 / 0.1689\n",
      "[0/2][353/1205] Loss_D: 0.9887 Loss_G: 1.0910 D(x): 0.4957 D(G(z)): 0.2288 / 0.3384\n",
      "[0/2][354/1205] Loss_D: 1.0609 Loss_G: 1.1262 D(x): 0.6138 D(G(z)): 0.4185 / 0.3266\n",
      "[0/2][355/1205] Loss_D: 1.0631 Loss_G: 2.4517 D(x): 0.7377 D(G(z)): 0.5248 / 0.0882\n",
      "[0/2][356/1205] Loss_D: 0.8294 Loss_G: 1.4811 D(x): 0.5202 D(G(z)): 0.1242 / 0.2339\n",
      "[0/2][357/1205] Loss_D: 0.7130 Loss_G: 1.7774 D(x): 0.7950 D(G(z)): 0.3687 / 0.1760\n",
      "[0/2][358/1205] Loss_D: 1.8428 Loss_G: 0.3520 D(x): 0.2856 D(G(z)): 0.3687 / 0.7062\n",
      "[0/2][359/1205] Loss_D: 1.6524 Loss_G: 2.5828 D(x): 0.7956 D(G(z)): 0.7543 / 0.0776\n",
      "[0/2][360/1205] Loss_D: 1.4469 Loss_G: 0.8163 D(x): 0.2934 D(G(z)): 0.1146 / 0.4458\n",
      "[0/2][361/1205] Loss_D: 1.0881 Loss_G: 2.2056 D(x): 0.9072 D(G(z)): 0.6262 / 0.1133\n",
      "[0/2][362/1205] Loss_D: 1.4367 Loss_G: 0.6270 D(x): 0.3230 D(G(z)): 0.2282 / 0.5394\n",
      "[0/2][363/1205] Loss_D: 1.2320 Loss_G: 1.8115 D(x): 0.7920 D(G(z)): 0.6209 / 0.1767\n",
      "[0/2][364/1205] Loss_D: 1.1572 Loss_G: 1.2226 D(x): 0.4823 D(G(z)): 0.2818 / 0.3043\n",
      "[0/2][365/1205] Loss_D: 1.1972 Loss_G: 0.9722 D(x): 0.5462 D(G(z)): 0.4291 / 0.3973\n",
      "[0/2][366/1205] Loss_D: 1.2522 Loss_G: 1.5087 D(x): 0.6060 D(G(z)): 0.5114 / 0.2380\n",
      "[0/2][367/1205] Loss_D: 1.1739 Loss_G: 1.6447 D(x): 0.6137 D(G(z)): 0.4729 / 0.2114\n",
      "[0/2][368/1205] Loss_D: 1.3157 Loss_G: 0.5294 D(x): 0.3923 D(G(z)): 0.3028 / 0.6049\n",
      "[0/2][369/1205] Loss_D: 1.0980 Loss_G: 2.9784 D(x): 0.8246 D(G(z)): 0.5786 / 0.0613\n",
      "[0/2][370/1205] Loss_D: 1.0490 Loss_G: 0.8692 D(x): 0.4964 D(G(z)): 0.1586 / 0.4268\n",
      "[0/2][371/1205] Loss_D: 1.3927 Loss_G: 1.6353 D(x): 0.6703 D(G(z)): 0.5865 / 0.1996\n",
      "[0/2][372/1205] Loss_D: 1.0421 Loss_G: 0.5755 D(x): 0.4708 D(G(z)): 0.2299 / 0.5695\n",
      "[0/2][373/1205] Loss_D: 1.3978 Loss_G: 3.2125 D(x): 0.8663 D(G(z)): 0.7044 / 0.0505\n",
      "[0/2][374/1205] Loss_D: 2.3677 Loss_G: 0.3015 D(x): 0.1259 D(G(z)): 0.1495 / 0.7421\n",
      "[0/2][375/1205] Loss_D: 1.6317 Loss_G: 1.8362 D(x): 0.8214 D(G(z)): 0.7599 / 0.1649\n",
      "[0/2][376/1205] Loss_D: 1.3422 Loss_G: 1.5087 D(x): 0.4901 D(G(z)): 0.4394 / 0.2336\n",
      "[0/2][377/1205] Loss_D: 1.1179 Loss_G: 0.7205 D(x): 0.4179 D(G(z)): 0.2100 / 0.5152\n",
      "[0/2][378/1205] Loss_D: 1.2443 Loss_G: 1.8342 D(x): 0.7772 D(G(z)): 0.6230 / 0.1634\n",
      "[0/2][379/1205] Loss_D: 1.1212 Loss_G: 1.1692 D(x): 0.5003 D(G(z)): 0.3180 / 0.3282\n",
      "[0/2][380/1205] Loss_D: 1.2445 Loss_G: 0.4339 D(x): 0.4399 D(G(z)): 0.3056 / 0.6493\n",
      "[0/2][381/1205] Loss_D: 1.9410 Loss_G: 2.8539 D(x): 0.9122 D(G(z)): 0.8392 / 0.0585\n",
      "[0/2][382/1205] Loss_D: 1.1512 Loss_G: 1.7598 D(x): 0.3747 D(G(z)): 0.0713 / 0.1747\n",
      "[0/2][383/1205] Loss_D: 0.9047 Loss_G: 0.4884 D(x): 0.5103 D(G(z)): 0.2041 / 0.6192\n",
      "[0/2][384/1205] Loss_D: 1.5621 Loss_G: 2.7555 D(x): 0.8955 D(G(z)): 0.7532 / 0.0645\n",
      "[0/2][385/1205] Loss_D: 1.1499 Loss_G: 1.1502 D(x): 0.3905 D(G(z)): 0.1805 / 0.3458\n",
      "[0/2][386/1205] Loss_D: 1.4499 Loss_G: 0.6796 D(x): 0.4717 D(G(z)): 0.4174 / 0.5241\n",
      "[0/2][387/1205] Loss_D: 1.1698 Loss_G: 2.5090 D(x): 0.8556 D(G(z)): 0.6246 / 0.0832\n",
      "[0/2][388/1205] Loss_D: 0.7976 Loss_G: 2.5187 D(x): 0.6455 D(G(z)): 0.2251 / 0.0864\n",
      "[0/2][389/1205] Loss_D: 1.2600 Loss_G: 0.8932 D(x): 0.3848 D(G(z)): 0.0480 / 0.4259\n",
      "[0/2][390/1205] Loss_D: 1.4278 Loss_G: 1.5360 D(x): 0.8137 D(G(z)): 0.6571 / 0.2427\n",
      "[0/2][391/1205] Loss_D: 0.8311 Loss_G: 2.1797 D(x): 0.6678 D(G(z)): 0.3365 / 0.1171\n",
      "[0/2][392/1205] Loss_D: 1.2601 Loss_G: 0.7671 D(x): 0.4246 D(G(z)): 0.2523 / 0.4716\n",
      "[0/2][393/1205] Loss_D: 1.6562 Loss_G: 1.5131 D(x): 0.6370 D(G(z)): 0.6772 / 0.2316\n",
      "[0/2][394/1205] Loss_D: 0.5726 Loss_G: 3.0683 D(x): 0.8047 D(G(z)): 0.2931 / 0.0529\n",
      "[0/2][395/1205] Loss_D: 2.1046 Loss_G: 0.3380 D(x): 0.1723 D(G(z)): 0.0658 / 0.7193\n",
      "[0/2][396/1205] Loss_D: 2.0581 Loss_G: 1.7118 D(x): 0.8743 D(G(z)): 0.8323 / 0.1897\n",
      "[0/2][397/1205] Loss_D: 0.8391 Loss_G: 2.4309 D(x): 0.6768 D(G(z)): 0.3424 / 0.1038\n",
      "[0/2][398/1205] Loss_D: 2.1778 Loss_G: 0.3735 D(x): 0.1432 D(G(z)): 0.1376 / 0.6950\n",
      "[0/2][399/1205] Loss_D: 1.4616 Loss_G: 1.7160 D(x): 0.9061 D(G(z)): 0.7217 / 0.1949\n",
      "[0/2][400/1205] Loss_D: 0.8402 Loss_G: 3.0331 D(x): 0.8167 D(G(z)): 0.4611 / 0.0517\n",
      "[0/2][401/1205] Loss_D: 1.9908 Loss_G: 0.6132 D(x): 0.2582 D(G(z)): 0.1700 / 0.5545\n",
      "[0/2][402/1205] Loss_D: 1.4061 Loss_G: 0.5296 D(x): 0.5117 D(G(z)): 0.4684 / 0.5938\n",
      "[0/2][403/1205] Loss_D: 1.5702 Loss_G: 2.3037 D(x): 0.8181 D(G(z)): 0.7350 / 0.1072\n",
      "[0/2][404/1205] Loss_D: 1.4568 Loss_G: 0.8932 D(x): 0.3167 D(G(z)): 0.2084 / 0.4203\n",
      "[0/2][405/1205] Loss_D: 0.9276 Loss_G: 1.3586 D(x): 0.7215 D(G(z)): 0.4327 / 0.2655\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(batchSize, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "for epoch in range(niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label, device=device)\n",
    "\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, niter, i, len(dataloader),\n",
    "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu,\n",
    "                    '%s/real_samples.png' % outf,\n",
    "                    normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.detach(),\n",
    "                    '%s/fake_samples_epoch_%03d.png' % (outf, epoch),\n",
    "                    normalize=True)\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (outf, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
