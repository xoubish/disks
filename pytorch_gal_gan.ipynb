{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  1237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa424104710>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing parameters:\n",
    "\n",
    "dataroot='gals/'\n",
    "device = torch.device(\"cpu\") # If GPU then use \"cuda:0\"\n",
    "ngpu = 0 #number of GPUs to use \n",
    "nz = 10 #size of the latent z vector\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "workers = 4*ngpu #number of data loading workers\n",
    "batchSize = 64 #input batch size\n",
    "imageSize = 64 #the height / width of the input image to network\n",
    "niter = 25 #number of epochs to train for\n",
    "lr = 0.0002 #learning rate, default=0.0002\n",
    "beta1 = 0.5 #beta1 for adam. default=0.5\n",
    "outf='outputs' #folder to output images and model checkpoints\n",
    "\n",
    "\n",
    "manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dset.MNIST(root=dataroot, download=True,\n",
    "                     transform=transforms.Compose([transforms.Resize(imageSize),transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,)),]))\n",
    "nc=1\n",
    "\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=int(workers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(10, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "#if netG != '':\n",
    "#    netG.load_state_dict(torch.load(netG))\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "#if netD != '':\n",
    "#    netD.load_state_dict(torch.load(netD))\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][0/95] Loss_D: 1.6713 Loss_G: 4.4100 D(x): 0.5699 D(G(z)): 0.5740 / 0.0280\n",
      "[0/25][1/95] Loss_D: 1.4845 Loss_G: 5.5299 D(x): 1.0000 D(G(z)): 0.6927 / 0.0129\n",
      "[0/25][2/95] Loss_D: 0.8212 Loss_G: 6.4648 D(x): 0.9958 D(G(z)): 0.5138 / 0.0054\n",
      "[0/25][3/95] Loss_D: 0.4146 Loss_G: 6.5944 D(x): 0.9884 D(G(z)): 0.3065 / 0.0036\n",
      "[0/25][4/95] Loss_D: 0.4834 Loss_G: 6.8803 D(x): 0.9957 D(G(z)): 0.3390 / 0.0023\n",
      "[0/25][5/95] Loss_D: 0.2466 Loss_G: 6.9902 D(x): 1.0000 D(G(z)): 0.2029 / 0.0023\n",
      "[0/25][6/95] Loss_D: 0.2125 Loss_G: 7.1663 D(x): 0.9949 D(G(z)): 0.1730 / 0.0019\n",
      "[0/25][7/95] Loss_D: 0.7387 Loss_G: 6.9970 D(x): 0.9516 D(G(z)): 0.1261 / 0.0018\n",
      "[0/25][8/95] Loss_D: 0.3597 Loss_G: 6.9353 D(x): 0.9462 D(G(z)): 0.1303 / 0.0016\n",
      "[0/25][9/95] Loss_D: 0.1257 Loss_G: 6.8731 D(x): 0.9741 D(G(z)): 0.0806 / 0.0020\n",
      "[0/25][10/95] Loss_D: 0.1771 Loss_G: 7.1061 D(x): 0.9815 D(G(z)): 0.0991 / 0.0014\n",
      "[0/25][11/95] Loss_D: 0.0704 Loss_G: 7.0090 D(x): 0.9945 D(G(z)): 0.0607 / 0.0016\n",
      "[0/25][12/95] Loss_D: 0.3464 Loss_G: 6.5976 D(x): 0.9833 D(G(z)): 0.0704 / 0.0020\n",
      "[0/25][13/95] Loss_D: 0.1050 Loss_G: 7.1995 D(x): 0.9871 D(G(z)): 0.0815 / 0.0010\n",
      "[0/25][14/95] Loss_D: 0.0595 Loss_G: 7.1290 D(x): 0.9902 D(G(z)): 0.0470 / 0.0012\n",
      "[0/25][15/95] Loss_D: 0.0712 Loss_G: 7.3611 D(x): 0.9946 D(G(z)): 0.0604 / 0.0010\n",
      "[0/25][16/95] Loss_D: 0.0918 Loss_G: 6.9670 D(x): 0.9749 D(G(z)): 0.0427 / 0.0014\n",
      "[0/25][17/95] Loss_D: 0.1069 Loss_G: 8.4397 D(x): 0.9974 D(G(z)): 0.0912 / 0.0003\n",
      "[0/25][18/95] Loss_D: 0.0177 Loss_G: 8.0981 D(x): 0.9978 D(G(z)): 0.0152 / 0.0005\n",
      "[0/25][19/95] Loss_D: 0.0245 Loss_G: 6.9491 D(x): 0.9955 D(G(z)): 0.0191 / 0.0014\n",
      "[0/25][20/95] Loss_D: 0.0881 Loss_G: 7.6188 D(x): 0.9831 D(G(z)): 0.0642 / 0.0007\n",
      "[0/25][21/95] Loss_D: 0.1748 Loss_G: 8.3601 D(x): 0.9771 D(G(z)): 0.0462 / 0.0005\n",
      "[0/25][22/95] Loss_D: 0.0868 Loss_G: 7.3755 D(x): 0.9630 D(G(z)): 0.0213 / 0.0011\n",
      "[0/25][23/95] Loss_D: 0.0767 Loss_G: 7.2657 D(x): 0.9757 D(G(z)): 0.0401 / 0.0011\n",
      "[0/25][24/95] Loss_D: 0.0715 Loss_G: 9.3313 D(x): 0.9998 D(G(z)): 0.0657 / 0.0001\n",
      "[0/25][25/95] Loss_D: 0.0059 Loss_G: 8.9518 D(x): 0.9998 D(G(z)): 0.0056 / 0.0002\n",
      "[0/25][26/95] Loss_D: 0.0091 Loss_G: 7.2763 D(x): 0.9992 D(G(z)): 0.0083 / 0.0010\n",
      "[0/25][27/95] Loss_D: 0.0414 Loss_G: 7.7184 D(x): 0.9997 D(G(z)): 0.0394 / 0.0007\n",
      "[0/25][28/95] Loss_D: 0.0323 Loss_G: 7.9160 D(x): 0.9933 D(G(z)): 0.0238 / 0.0006\n",
      "[0/25][29/95] Loss_D: 0.0218 Loss_G: 7.7227 D(x): 0.9976 D(G(z)): 0.0191 / 0.0006\n",
      "[0/25][30/95] Loss_D: 0.0364 Loss_G: 7.2322 D(x): 0.9822 D(G(z)): 0.0139 / 0.0010\n",
      "[0/25][31/95] Loss_D: 0.0318 Loss_G: 7.7586 D(x): 0.9977 D(G(z)): 0.0287 / 0.0006\n",
      "[0/25][32/95] Loss_D: 0.0192 Loss_G: 7.8964 D(x): 0.9979 D(G(z)): 0.0168 / 0.0005\n",
      "[0/25][33/95] Loss_D: 0.0134 Loss_G: 7.7736 D(x): 0.9988 D(G(z)): 0.0120 / 0.0006\n",
      "[0/25][34/95] Loss_D: 0.0217 Loss_G: 7.6654 D(x): 0.9982 D(G(z)): 0.0195 / 0.0006\n",
      "[0/25][35/95] Loss_D: 0.0225 Loss_G: 7.9561 D(x): 0.9965 D(G(z)): 0.0186 / 0.0005\n",
      "[0/25][36/95] Loss_D: 0.0179 Loss_G: 7.9797 D(x): 0.9947 D(G(z)): 0.0122 / 0.0006\n",
      "[0/25][37/95] Loss_D: 0.0168 Loss_G: 7.6883 D(x): 0.9964 D(G(z)): 0.0130 / 0.0008\n",
      "[0/25][38/95] Loss_D: 0.0588 Loss_G: 7.0976 D(x): 0.9796 D(G(z)): 0.0160 / 0.0014\n",
      "[0/25][39/95] Loss_D: 0.0351 Loss_G: 8.9324 D(x): 0.9989 D(G(z)): 0.0329 / 0.0002\n",
      "[0/25][40/95] Loss_D: 0.0079 Loss_G: 8.7270 D(x): 0.9963 D(G(z)): 0.0041 / 0.0002\n",
      "[0/25][41/95] Loss_D: 0.0061 Loss_G: 7.6523 D(x): 0.9986 D(G(z)): 0.0047 / 0.0007\n",
      "[0/25][42/95] Loss_D: 0.0231 Loss_G: 8.0670 D(x): 0.9988 D(G(z)): 0.0215 / 0.0005\n",
      "[0/25][43/95] Loss_D: 0.0171 Loss_G: 8.1588 D(x): 0.9939 D(G(z)): 0.0103 / 0.0004\n",
      "[0/25][44/95] Loss_D: 0.0180 Loss_G: 7.3571 D(x): 0.9902 D(G(z)): 0.0076 / 0.0010\n",
      "[0/25][45/95] Loss_D: 0.0274 Loss_G: 7.9896 D(x): 0.9925 D(G(z)): 0.0190 / 0.0005\n",
      "[0/25][46/95] Loss_D: 0.0107 Loss_G: 8.1837 D(x): 0.9970 D(G(z)): 0.0076 / 0.0005\n",
      "[0/25][47/95] Loss_D: 0.0102 Loss_G: 7.8284 D(x): 0.9981 D(G(z)): 0.0083 / 0.0006\n",
      "[0/25][48/95] Loss_D: 0.0112 Loss_G: 7.7961 D(x): 0.9985 D(G(z)): 0.0097 / 0.0006\n",
      "[0/25][49/95] Loss_D: 0.0256 Loss_G: 7.0939 D(x): 0.9876 D(G(z)): 0.0113 / 0.0013\n",
      "[0/25][50/95] Loss_D: 0.0379 Loss_G: 10.8601 D(x): 0.9983 D(G(z)): 0.0352 / 0.0000\n",
      "[0/25][51/95] Loss_D: 0.1159 Loss_G: 3.4124 D(x): 0.9255 D(G(z)): 0.0005 / 0.0492\n",
      "[0/25][52/95] Loss_D: 1.8058 Loss_G: 39.9084 D(x): 0.9972 D(G(z)): 0.8030 / 0.0000\n",
      "[0/25][53/95] Loss_D: 17.9991 Loss_G: 37.5779 D(x): 0.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][54/95] Loss_D: 0.9380 Loss_G: 34.8374 D(x): 0.9472 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][55/95] Loss_D: 0.0016 Loss_G: 31.6650 D(x): 0.9985 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][56/95] Loss_D: 0.0737 Loss_G: 26.3286 D(x): 0.9793 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][57/95] Loss_D: 0.0468 Loss_G: 9.3907 D(x): 0.9997 D(G(z)): 0.0227 / 0.0166\n",
      "[0/25][58/95] Loss_D: 8.7248 Loss_G: 26.5668 D(x): 0.9979 D(G(z)): 0.9977 / 0.0000\n",
      "[0/25][59/95] Loss_D: 3.4713 Loss_G: 19.6243 D(x): 0.1130 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][60/95] Loss_D: 0.0515 Loss_G: 7.6512 D(x): 0.9847 D(G(z)): 0.0023 / 0.0104\n",
      "[0/25][61/95] Loss_D: 5.1738 Loss_G: 19.7420 D(x): 0.9943 D(G(z)): 0.9623 / 0.0000\n",
      "[0/25][62/95] Loss_D: 0.8669 Loss_G: 19.0168 D(x): 0.7139 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][63/95] Loss_D: 1.2987 Loss_G: 6.5948 D(x): 0.5649 D(G(z)): 0.0001 / 0.0083\n",
      "[0/25][64/95] Loss_D: 2.7784 Loss_G: 16.7581 D(x): 0.9290 D(G(z)): 0.7810 / 0.0000\n",
      "[0/25][65/95] Loss_D: 2.3428 Loss_G: 8.1384 D(x): 0.3112 D(G(z)): 0.0001 / 0.0008\n",
      "[0/25][66/95] Loss_D: 1.7809 Loss_G: 13.0170 D(x): 0.8408 D(G(z)): 0.6336 / 0.0000\n",
      "[0/25][67/95] Loss_D: 1.0765 Loss_G: 5.8194 D(x): 0.5455 D(G(z)): 0.0028 / 0.0058\n",
      "[0/25][68/95] Loss_D: 1.5192 Loss_G: 15.9944 D(x): 0.8474 D(G(z)): 0.6525 / 0.0000\n",
      "[0/25][69/95] Loss_D: 5.3319 Loss_G: 0.7449 D(x): 0.0184 D(G(z)): 0.0001 / 0.5484\n",
      "[0/25][70/95] Loss_D: 4.1967 Loss_G: 11.4964 D(x): 0.9428 D(G(z)): 0.9735 / 0.0002\n",
      "[0/25][71/95] Loss_D: 0.8713 Loss_G: 7.4592 D(x): 0.5568 D(G(z)): 0.0052 / 0.0051\n",
      "[0/25][72/95] Loss_D: 0.4748 Loss_G: 4.6865 D(x): 0.9020 D(G(z)): 0.2228 / 0.0187\n",
      "[0/25][73/95] Loss_D: 1.6997 Loss_G: 13.5414 D(x): 0.9487 D(G(z)): 0.6106 / 0.0000\n",
      "[0/25][74/95] Loss_D: 4.5636 Loss_G: 3.7744 D(x): 0.0504 D(G(z)): 0.0001 / 0.0344\n",
      "[0/25][75/95] Loss_D: 0.8868 Loss_G: 4.1664 D(x): 0.8630 D(G(z)): 0.4583 / 0.0272\n",
      "[0/25][76/95] Loss_D: 0.9899 Loss_G: 3.3362 D(x): 0.6403 D(G(z)): 0.2916 / 0.0473\n",
      "[0/25][77/95] Loss_D: 1.1770 Loss_G: 6.5247 D(x): 0.7427 D(G(z)): 0.5198 / 0.0026\n",
      "[0/25][78/95] Loss_D: 2.1557 Loss_G: 0.0524 D(x): 0.1781 D(G(z)): 0.0292 / 0.9551\n",
      "[0/25][79/95] Loss_D: 4.4211 Loss_G: 7.0763 D(x): 0.9828 D(G(z)): 0.9807 / 0.0014\n",
      "[0/25][80/95] Loss_D: 0.9134 Loss_G: 3.3030 D(x): 0.4730 D(G(z)): 0.0120 / 0.0467\n",
      "[0/25][81/95] Loss_D: 0.5878 Loss_G: 4.1935 D(x): 0.8954 D(G(z)): 0.3433 / 0.0228\n",
      "[0/25][82/95] Loss_D: 0.6763 Loss_G: 2.7067 D(x): 0.7027 D(G(z)): 0.1853 / 0.0764\n",
      "[0/25][83/95] Loss_D: 0.6096 Loss_G: 5.4209 D(x): 0.8654 D(G(z)): 0.3459 / 0.0065\n",
      "[0/25][84/95] Loss_D: 1.6876 Loss_G: 0.0624 D(x): 0.2775 D(G(z)): 0.0731 / 0.9454\n",
      "[0/25][85/95] Loss_D: 4.2495 Loss_G: 5.5337 D(x): 0.9902 D(G(z)): 0.9687 / 0.0084\n",
      "[0/25][86/95] Loss_D: 1.9394 Loss_G: 0.5125 D(x): 0.2074 D(G(z)): 0.0299 / 0.6238\n",
      "[0/25][87/95] Loss_D: 2.4147 Loss_G: 5.2460 D(x): 0.9796 D(G(z)): 0.8902 / 0.0089\n",
      "[0/25][88/95] Loss_D: 2.1738 Loss_G: 0.5552 D(x): 0.1718 D(G(z)): 0.0233 / 0.6114\n",
      "[0/25][89/95] Loss_D: 1.8874 Loss_G: 4.1975 D(x): 0.9724 D(G(z)): 0.8037 / 0.0254\n",
      "[0/25][90/95] Loss_D: 1.1640 Loss_G: 0.9536 D(x): 0.3766 D(G(z)): 0.0776 / 0.4238\n",
      "[0/25][91/95] Loss_D: 1.5049 Loss_G: 5.8431 D(x): 0.9474 D(G(z)): 0.7205 / 0.0038\n",
      "[0/25][92/95] Loss_D: 2.0520 Loss_G: 0.2850 D(x): 0.1615 D(G(z)): 0.0217 / 0.7703\n",
      "[0/25][93/95] Loss_D: 2.5179 Loss_G: 3.8253 D(x): 0.9677 D(G(z)): 0.8716 / 0.0261\n",
      "[0/25][94/95] Loss_D: 0.7799 Loss_G: 2.6667 D(x): 0.5433 D(G(z)): 0.0596 / 0.0835\n",
      "[1/25][0/95] Loss_D: 0.7966 Loss_G: 1.8104 D(x): 0.7010 D(G(z)): 0.2895 / 0.1886\n",
      "[1/25][1/95] Loss_D: 0.8636 Loss_G: 3.1163 D(x): 0.7224 D(G(z)): 0.3476 / 0.0554\n",
      "[1/25][2/95] Loss_D: 1.0382 Loss_G: 0.9845 D(x): 0.5214 D(G(z)): 0.1899 / 0.3932\n",
      "[1/25][3/95] Loss_D: 1.0388 Loss_G: 4.6155 D(x): 0.8387 D(G(z)): 0.5439 / 0.0127\n",
      "[1/25][4/95] Loss_D: 1.9943 Loss_G: 0.0568 D(x): 0.2270 D(G(z)): 0.0435 / 0.9453\n",
      "[1/25][5/95] Loss_D: 3.4064 Loss_G: 3.4697 D(x): 0.9712 D(G(z)): 0.9538 / 0.0610\n",
      "[1/25][6/95] Loss_D: 1.5257 Loss_G: 1.2300 D(x): 0.3212 D(G(z)): 0.1130 / 0.3525\n",
      "[1/25][7/95] Loss_D: 0.7902 Loss_G: 2.0310 D(x): 0.8718 D(G(z)): 0.4502 / 0.1469\n",
      "[1/25][8/95] Loss_D: 0.7933 Loss_G: 1.8935 D(x): 0.6703 D(G(z)): 0.2707 / 0.1651\n",
      "[1/25][9/95] Loss_D: 0.9305 Loss_G: 1.4889 D(x): 0.6366 D(G(z)): 0.3037 / 0.2390\n",
      "[1/25][10/95] Loss_D: 1.0085 Loss_G: 3.8548 D(x): 0.7759 D(G(z)): 0.4412 / 0.0285\n",
      "[1/25][11/95] Loss_D: 1.5610 Loss_G: 0.1675 D(x): 0.2710 D(G(z)): 0.0830 / 0.8545\n",
      "[1/25][12/95] Loss_D: 2.5609 Loss_G: 4.2180 D(x): 0.9294 D(G(z)): 0.8836 / 0.0195\n",
      "[1/25][13/95] Loss_D: 1.3696 Loss_G: 1.4666 D(x): 0.3043 D(G(z)): 0.0362 / 0.2423\n",
      "[1/25][14/95] Loss_D: 0.8277 Loss_G: 1.9641 D(x): 0.8296 D(G(z)): 0.4255 / 0.1477\n",
      "[1/25][15/95] Loss_D: 0.8319 Loss_G: 1.8838 D(x): 0.6654 D(G(z)): 0.2626 / 0.1640\n",
      "[1/25][16/95] Loss_D: 0.9252 Loss_G: 2.0291 D(x): 0.6493 D(G(z)): 0.3330 / 0.1556\n",
      "[1/25][17/95] Loss_D: 0.7830 Loss_G: 1.6086 D(x): 0.6549 D(G(z)): 0.2684 / 0.2334\n",
      "[1/25][18/95] Loss_D: 0.9009 Loss_G: 1.2610 D(x): 0.6550 D(G(z)): 0.3486 / 0.2961\n",
      "[1/25][19/95] Loss_D: 1.1054 Loss_G: 2.6820 D(x): 0.6952 D(G(z)): 0.4874 / 0.0885\n",
      "[1/25][20/95] Loss_D: 2.4676 Loss_G: 0.8200 D(x): 0.2491 D(G(z)): 0.1449 / 0.5136\n",
      "[1/25][21/95] Loss_D: 1.2831 Loss_G: 3.8731 D(x): 0.8740 D(G(z)): 0.6215 / 0.0413\n",
      "[1/25][22/95] Loss_D: 1.7787 Loss_G: 0.1116 D(x): 0.2350 D(G(z)): 0.0670 / 0.8960\n",
      "[1/25][23/95] Loss_D: 2.8997 Loss_G: 3.5143 D(x): 0.9593 D(G(z)): 0.9297 / 0.0515\n",
      "[1/25][24/95] Loss_D: 1.9170 Loss_G: 0.9124 D(x): 0.2545 D(G(z)): 0.1187 / 0.4438\n",
      "[1/25][25/95] Loss_D: 1.2241 Loss_G: 1.5017 D(x): 0.7354 D(G(z)): 0.5563 / 0.2428\n",
      "[1/25][26/95] Loss_D: 1.0732 Loss_G: 2.3004 D(x): 0.6211 D(G(z)): 0.3978 / 0.1183\n",
      "[1/25][27/95] Loss_D: 1.2290 Loss_G: 0.5407 D(x): 0.4183 D(G(z)): 0.2325 / 0.6066\n",
      "[1/25][28/95] Loss_D: 1.4055 Loss_G: 3.7772 D(x): 0.8315 D(G(z)): 0.6715 / 0.0335\n",
      "[1/25][29/95] Loss_D: 1.8155 Loss_G: 0.3146 D(x): 0.2136 D(G(z)): 0.0669 / 0.7380\n",
      "[1/25][30/95] Loss_D: 2.1402 Loss_G: 2.9554 D(x): 0.9070 D(G(z)): 0.8091 / 0.0688\n",
      "[1/25][31/95] Loss_D: 1.7076 Loss_G: 0.5346 D(x): 0.2376 D(G(z)): 0.1119 / 0.6050\n",
      "[1/25][32/95] Loss_D: 1.5068 Loss_G: 2.5586 D(x): 0.8648 D(G(z)): 0.7044 / 0.0917\n",
      "[1/25][33/95] Loss_D: 1.1836 Loss_G: 0.8800 D(x): 0.4027 D(G(z)): 0.1748 / 0.4278\n",
      "[1/25][34/95] Loss_D: 0.9958 Loss_G: 2.6848 D(x): 0.8360 D(G(z)): 0.5456 / 0.0859\n",
      "[1/25][35/95] Loss_D: 1.5748 Loss_G: 0.2628 D(x): 0.2817 D(G(z)): 0.1217 / 0.7762\n",
      "[1/25][36/95] Loss_D: 1.9610 Loss_G: 2.6583 D(x): 0.9270 D(G(z)): 0.8287 / 0.0897\n",
      "[1/25][37/95] Loss_D: 1.3119 Loss_G: 0.9745 D(x): 0.3321 D(G(z)): 0.1282 / 0.3912\n",
      "[1/25][38/95] Loss_D: 1.1580 Loss_G: 1.7733 D(x): 0.7113 D(G(z)): 0.5342 / 0.1896\n",
      "[1/25][39/95] Loss_D: 1.0384 Loss_G: 1.0337 D(x): 0.5031 D(G(z)): 0.2651 / 0.3776\n",
      "[1/25][40/95] Loss_D: 1.0294 Loss_G: 1.4995 D(x): 0.6934 D(G(z)): 0.4542 / 0.2468\n",
      "[1/25][41/95] Loss_D: 1.0999 Loss_G: 1.3306 D(x): 0.5577 D(G(z)): 0.3482 / 0.2855\n",
      "[1/25][42/95] Loss_D: 1.0672 Loss_G: 1.4478 D(x): 0.6022 D(G(z)): 0.3912 / 0.2605\n",
      "[1/25][43/95] Loss_D: 0.8339 Loss_G: 2.6795 D(x): 0.7429 D(G(z)): 0.3666 / 0.0810\n",
      "[1/25][44/95] Loss_D: 1.3338 Loss_G: 0.2207 D(x): 0.3241 D(G(z)): 0.1093 / 0.8054\n",
      "[1/25][45/95] Loss_D: 2.0414 Loss_G: 3.5498 D(x): 0.9307 D(G(z)): 0.8432 / 0.0381\n",
      "[1/25][46/95] Loss_D: 2.3123 Loss_G: 0.2897 D(x): 0.1308 D(G(z)): 0.0610 / 0.7527\n",
      "[1/25][47/95] Loss_D: 1.8647 Loss_G: 2.0210 D(x): 0.9240 D(G(z)): 0.8141 / 0.1581\n",
      "[1/25][48/95] Loss_D: 0.6602 Loss_G: 3.1728 D(x): 0.6775 D(G(z)): 0.2029 / 0.0609\n",
      "[1/25][49/95] Loss_D: 1.8656 Loss_G: 0.2727 D(x): 0.2062 D(G(z)): 0.0854 / 0.7685\n",
      "[1/25][50/95] Loss_D: 1.8408 Loss_G: 1.8955 D(x): 0.9385 D(G(z)): 0.8163 / 0.1860\n",
      "[1/25][51/95] Loss_D: 1.2015 Loss_G: 1.1905 D(x): 0.4416 D(G(z)): 0.2297 / 0.3248\n",
      "[1/25][52/95] Loss_D: 0.6517 Loss_G: 2.2032 D(x): 0.8687 D(G(z)): 0.3889 / 0.1257\n",
      "[1/25][53/95] Loss_D: 0.7584 Loss_G: 1.5195 D(x): 0.5841 D(G(z)): 0.1792 / 0.2378\n",
      "[1/25][54/95] Loss_D: 0.9559 Loss_G: 0.6911 D(x): 0.5779 D(G(z)): 0.3054 / 0.5104\n",
      "[1/25][55/95] Loss_D: 1.3898 Loss_G: 1.8338 D(x): 0.7494 D(G(z)): 0.6523 / 0.1739\n",
      "[1/25][56/95] Loss_D: 1.0581 Loss_G: 1.0678 D(x): 0.4526 D(G(z)): 0.1938 / 0.3621\n",
      "[1/25][57/95] Loss_D: 1.0170 Loss_G: 0.8783 D(x): 0.6083 D(G(z)): 0.3764 / 0.4310\n",
      "[1/25][58/95] Loss_D: 1.0450 Loss_G: 2.1698 D(x): 0.7632 D(G(z)): 0.5192 / 0.1308\n",
      "[1/25][59/95] Loss_D: 1.3649 Loss_G: 0.4913 D(x): 0.3369 D(G(z)): 0.1463 / 0.6308\n",
      "[1/25][60/95] Loss_D: 1.4813 Loss_G: 1.9774 D(x): 0.8742 D(G(z)): 0.7193 / 0.1654\n",
      "[1/25][61/95] Loss_D: 1.1008 Loss_G: 1.0089 D(x): 0.4581 D(G(z)): 0.2003 / 0.3791\n",
      "[1/25][62/95] Loss_D: 0.9147 Loss_G: 1.5725 D(x): 0.7621 D(G(z)): 0.4526 / 0.2225\n",
      "[1/25][63/95] Loss_D: 0.8439 Loss_G: 1.0576 D(x): 0.6102 D(G(z)): 0.2380 / 0.3670\n",
      "[1/25][64/95] Loss_D: 1.0209 Loss_G: 2.0185 D(x): 0.7333 D(G(z)): 0.4833 / 0.1489\n",
      "[1/25][65/95] Loss_D: 1.1008 Loss_G: 0.6415 D(x): 0.4577 D(G(z)): 0.2023 / 0.5367\n",
      "[1/25][66/95] Loss_D: 1.2163 Loss_G: 2.9154 D(x): 0.8126 D(G(z)): 0.6170 / 0.0689\n",
      "[1/25][67/95] Loss_D: 1.5552 Loss_G: 0.2593 D(x): 0.2639 D(G(z)): 0.1112 / 0.7762\n",
      "[1/25][68/95] Loss_D: 1.7392 Loss_G: 2.8582 D(x): 0.8706 D(G(z)): 0.7786 / 0.0705\n",
      "[1/25][69/95] Loss_D: 1.4516 Loss_G: 0.6435 D(x): 0.2946 D(G(z)): 0.1241 / 0.5396\n",
      "[1/25][70/95] Loss_D: 1.2090 Loss_G: 1.9255 D(x): 0.7719 D(G(z)): 0.5860 / 0.1605\n",
      "[1/25][71/95] Loss_D: 1.0517 Loss_G: 1.0757 D(x): 0.5127 D(G(z)): 0.2431 / 0.3625\n",
      "[1/25][72/95] Loss_D: 0.8863 Loss_G: 1.9301 D(x): 0.7686 D(G(z)): 0.4430 / 0.1699\n",
      "[1/25][73/95] Loss_D: 1.0666 Loss_G: 0.4867 D(x): 0.4690 D(G(z)): 0.2166 / 0.6243\n",
      "[1/25][74/95] Loss_D: 1.6739 Loss_G: 3.1381 D(x): 0.8956 D(G(z)): 0.7719 / 0.0598\n",
      "[1/25][75/95] Loss_D: 2.0519 Loss_G: 0.3016 D(x): 0.1663 D(G(z)): 0.0693 / 0.7477\n",
      "[1/25][76/95] Loss_D: 1.8795 Loss_G: 1.7425 D(x): 0.8884 D(G(z)): 0.8012 / 0.1992\n",
      "[1/25][77/95] Loss_D: 1.1534 Loss_G: 1.0672 D(x): 0.4265 D(G(z)): 0.1975 / 0.3658\n",
      "[1/25][78/95] Loss_D: 1.1145 Loss_G: 1.5777 D(x): 0.7073 D(G(z)): 0.5016 / 0.2332\n",
      "[1/25][79/95] Loss_D: 1.0395 Loss_G: 1.1063 D(x): 0.5624 D(G(z)): 0.3317 / 0.3517\n",
      "[1/25][80/95] Loss_D: 1.2093 Loss_G: 1.5012 D(x): 0.6179 D(G(z)): 0.4825 / 0.2421\n",
      "[1/25][81/95] Loss_D: 1.1654 Loss_G: 1.0548 D(x): 0.5225 D(G(z)): 0.3582 / 0.3618\n",
      "[1/25][82/95] Loss_D: 1.2432 Loss_G: 2.2012 D(x): 0.6210 D(G(z)): 0.5082 / 0.1259\n",
      "[1/25][83/95] Loss_D: 1.5144 Loss_G: 0.3637 D(x): 0.2991 D(G(z)): 0.1933 / 0.7065\n",
      "[1/25][84/95] Loss_D: 1.7676 Loss_G: 3.7066 D(x): 0.8929 D(G(z)): 0.7922 / 0.0355\n",
      "[1/25][85/95] Loss_D: 1.5887 Loss_G: 1.6289 D(x): 0.2450 D(G(z)): 0.0553 / 0.2254\n",
      "[1/25][86/95] Loss_D: 1.2880 Loss_G: 0.2570 D(x): 0.5015 D(G(z)): 0.3533 / 0.7776\n",
      "[1/25][87/95] Loss_D: 2.0837 Loss_G: 2.7718 D(x): 0.8412 D(G(z)): 0.8250 / 0.0921\n",
      "[1/25][88/95] Loss_D: 2.3145 Loss_G: 0.4375 D(x): 0.1640 D(G(z)): 0.1424 / 0.6525\n",
      "[1/25][89/95] Loss_D: 1.3500 Loss_G: 1.4218 D(x): 0.7895 D(G(z)): 0.6384 / 0.2804\n",
      "[1/25][90/95] Loss_D: 1.2484 Loss_G: 1.0692 D(x): 0.5179 D(G(z)): 0.3969 / 0.3702\n",
      "[1/25][91/95] Loss_D: 1.1381 Loss_G: 1.2471 D(x): 0.6387 D(G(z)): 0.4684 / 0.2975\n",
      "[1/25][92/95] Loss_D: 0.7199 Loss_G: 2.4389 D(x): 0.7984 D(G(z)): 0.3820 / 0.0979\n",
      "[1/25][93/95] Loss_D: 1.7322 Loss_G: 0.3151 D(x): 0.2396 D(G(z)): 0.1633 / 0.7363\n",
      "[1/25][94/95] Loss_D: 1.7543 Loss_G: 2.3924 D(x): 0.8531 D(G(z)): 0.7869 / 0.1437\n",
      "[2/25][0/95] Loss_D: 1.1055 Loss_G: 1.3936 D(x): 0.5031 D(G(z)): 0.2780 / 0.2716\n",
      "[2/25][1/95] Loss_D: 0.8769 Loss_G: 1.0668 D(x): 0.6359 D(G(z)): 0.3182 / 0.3651\n",
      "[2/25][2/95] Loss_D: 1.0527 Loss_G: 2.9940 D(x): 0.8194 D(G(z)): 0.5386 / 0.0628\n",
      "[2/25][3/95] Loss_D: 1.8166 Loss_G: 0.4033 D(x): 0.2285 D(G(z)): 0.1338 / 0.6871\n",
      "[2/25][4/95] Loss_D: 1.6293 Loss_G: 2.4730 D(x): 0.8507 D(G(z)): 0.7328 / 0.1015\n",
      "[2/25][5/95] Loss_D: 1.1359 Loss_G: 1.2851 D(x): 0.4399 D(G(z)): 0.1864 / 0.2952\n",
      "[2/25][6/95] Loss_D: 1.1498 Loss_G: 1.0804 D(x): 0.6039 D(G(z)): 0.4186 / 0.3559\n",
      "[2/25][7/95] Loss_D: 1.2090 Loss_G: 1.6610 D(x): 0.6205 D(G(z)): 0.4817 / 0.2073\n",
      "[2/25][8/95] Loss_D: 0.6582 Loss_G: 2.8767 D(x): 0.7654 D(G(z)): 0.3054 / 0.0828\n",
      "[2/25][9/95] Loss_D: 2.2093 Loss_G: 0.1198 D(x): 0.1598 D(G(z)): 0.1685 / 0.8880\n",
      "[2/25][10/95] Loss_D: 2.3347 Loss_G: 2.2818 D(x): 0.9391 D(G(z)): 0.8907 / 0.1339\n",
      "[2/25][11/95] Loss_D: 1.4562 Loss_G: 1.1016 D(x): 0.3348 D(G(z)): 0.2006 / 0.3530\n",
      "[2/25][12/95] Loss_D: 1.0752 Loss_G: 1.2137 D(x): 0.6899 D(G(z)): 0.4732 / 0.3055\n",
      "[2/25][13/95] Loss_D: 1.0129 Loss_G: 1.5522 D(x): 0.6244 D(G(z)): 0.3897 / 0.2246\n",
      "[2/25][14/95] Loss_D: 1.3034 Loss_G: 1.0785 D(x): 0.4938 D(G(z)): 0.3880 / 0.3536\n",
      "[2/25][15/95] Loss_D: 1.0626 Loss_G: 2.1181 D(x): 0.6641 D(G(z)): 0.4475 / 0.1283\n",
      "[2/25][16/95] Loss_D: 1.0677 Loss_G: 0.8127 D(x): 0.4697 D(G(z)): 0.2405 / 0.4507\n",
      "[2/25][17/95] Loss_D: 1.2202 Loss_G: 1.4008 D(x): 0.6878 D(G(z)): 0.5576 / 0.2609\n",
      "[2/25][18/95] Loss_D: 1.6988 Loss_G: 0.3771 D(x): 0.3222 D(G(z)): 0.3854 / 0.6921\n",
      "[2/25][19/95] Loss_D: 1.5718 Loss_G: 2.3759 D(x): 0.8409 D(G(z)): 0.7344 / 0.1201\n",
      "[2/25][20/95] Loss_D: 1.3932 Loss_G: 0.5181 D(x): 0.3127 D(G(z)): 0.1261 / 0.6167\n",
      "[2/25][21/95] Loss_D: 1.5513 Loss_G: 1.6840 D(x): 0.8407 D(G(z)): 0.7190 / 0.2077\n",
      "[2/25][22/95] Loss_D: 0.8758 Loss_G: 2.0334 D(x): 0.6018 D(G(z)): 0.2880 / 0.1429\n",
      "[2/25][23/95] Loss_D: 1.2264 Loss_G: 0.7306 D(x): 0.4362 D(G(z)): 0.2670 / 0.4962\n",
      "[2/25][24/95] Loss_D: 1.1391 Loss_G: 1.5126 D(x): 0.7299 D(G(z)): 0.5393 / 0.2412\n",
      "[2/25][25/95] Loss_D: 1.3312 Loss_G: 1.0250 D(x): 0.4823 D(G(z)): 0.3973 / 0.3707\n",
      "[2/25][26/95] Loss_D: 1.5080 Loss_G: 1.9499 D(x): 0.5677 D(G(z)): 0.5578 / 0.1506\n",
      "[2/25][27/95] Loss_D: 1.5127 Loss_G: 0.3082 D(x): 0.3104 D(G(z)): 0.1950 / 0.7395\n",
      "[2/25][28/95] Loss_D: 2.1149 Loss_G: 3.3327 D(x): 0.8362 D(G(z)): 0.8351 / 0.0421\n",
      "[2/25][29/95] Loss_D: 1.6970 Loss_G: 1.0961 D(x): 0.2175 D(G(z)): 0.0682 / 0.3456\n",
      "[2/25][30/95] Loss_D: 1.0712 Loss_G: 0.6837 D(x): 0.6198 D(G(z)): 0.4220 / 0.5103\n",
      "[2/25][31/95] Loss_D: 1.2945 Loss_G: 2.6421 D(x): 0.7884 D(G(z)): 0.6414 / 0.0914\n",
      "[2/25][32/95] Loss_D: 1.5569 Loss_G: 0.7341 D(x): 0.2775 D(G(z)): 0.1033 / 0.4913\n",
      "[2/25][33/95] Loss_D: 1.2081 Loss_G: 2.2981 D(x): 0.9427 D(G(z)): 0.6740 / 0.1268\n",
      "[2/25][34/95] Loss_D: 1.5848 Loss_G: 0.7118 D(x): 0.2967 D(G(z)): 0.1842 / 0.4979\n",
      "[2/25][35/95] Loss_D: 1.2260 Loss_G: 1.3992 D(x): 0.7527 D(G(z)): 0.6010 / 0.2598\n",
      "[2/25][36/95] Loss_D: 1.5313 Loss_G: 0.5814 D(x): 0.3612 D(G(z)): 0.3594 / 0.5642\n",
      "[2/25][37/95] Loss_D: 1.5709 Loss_G: 1.8531 D(x): 0.7864 D(G(z)): 0.7215 / 0.1839\n",
      "[2/25][38/95] Loss_D: 1.4291 Loss_G: 0.7224 D(x): 0.3060 D(G(z)): 0.1456 / 0.5084\n",
      "[2/25][39/95] Loss_D: 1.3673 Loss_G: 1.7802 D(x): 0.8630 D(G(z)): 0.6896 / 0.1917\n",
      "[2/25][40/95] Loss_D: 0.6558 Loss_G: 2.3393 D(x): 0.6862 D(G(z)): 0.2265 / 0.1105\n",
      "[2/25][41/95] Loss_D: 1.0556 Loss_G: 0.8774 D(x): 0.4519 D(G(z)): 0.1816 / 0.4433\n",
      "[2/25][42/95] Loss_D: 1.2228 Loss_G: 0.7028 D(x): 0.6085 D(G(z)): 0.4866 / 0.5153\n",
      "[2/25][43/95] Loss_D: 1.2836 Loss_G: 2.0509 D(x): 0.7605 D(G(z)): 0.6155 / 0.1484\n",
      "[2/25][44/95] Loss_D: 0.5172 Loss_G: 2.7041 D(x): 0.7426 D(G(z)): 0.1860 / 0.0802\n",
      "[2/25][45/95] Loss_D: 1.7314 Loss_G: 0.2846 D(x): 0.2679 D(G(z)): 0.1850 / 0.7632\n",
      "[2/25][46/95] Loss_D: 2.1007 Loss_G: 2.3271 D(x): 0.8509 D(G(z)): 0.8367 / 0.1126\n",
      "[2/25][47/95] Loss_D: 1.5759 Loss_G: 0.8180 D(x): 0.2972 D(G(z)): 0.2158 / 0.4580\n",
      "[2/25][48/95] Loss_D: 1.1983 Loss_G: 1.3259 D(x): 0.6854 D(G(z)): 0.5145 / 0.2893\n",
      "[2/25][49/95] Loss_D: 1.0373 Loss_G: 1.7513 D(x): 0.5936 D(G(z)): 0.3644 / 0.1929\n",
      "[2/25][50/95] Loss_D: 1.2282 Loss_G: 1.2733 D(x): 0.5540 D(G(z)): 0.4127 / 0.2958\n",
      "[2/25][51/95] Loss_D: 1.0251 Loss_G: 1.2240 D(x): 0.5768 D(G(z)): 0.3395 / 0.3149\n",
      "[2/25][52/95] Loss_D: 1.3290 Loss_G: 1.2045 D(x): 0.5548 D(G(z)): 0.4807 / 0.3145\n",
      "[2/25][53/95] Loss_D: 0.8309 Loss_G: 3.2283 D(x): 0.8756 D(G(z)): 0.4583 / 0.0532\n",
      "[2/25][54/95] Loss_D: 3.5945 Loss_G: 0.1785 D(x): 0.0525 D(G(z)): 0.0718 / 0.8383\n",
      "[2/25][55/95] Loss_D: 2.1760 Loss_G: 1.2908 D(x): 0.8951 D(G(z)): 0.8618 / 0.3139\n",
      "[2/25][56/95] Loss_D: 0.9097 Loss_G: 3.0127 D(x): 0.7895 D(G(z)): 0.3287 / 0.0710\n",
      "[2/25][57/95] Loss_D: 1.4305 Loss_G: 1.0246 D(x): 0.2942 D(G(z)): 0.1137 / 0.3782\n",
      "[2/25][58/95] Loss_D: 1.2396 Loss_G: 0.4458 D(x): 0.5661 D(G(z)): 0.4659 / 0.6522\n",
      "[2/25][59/95] Loss_D: 1.4678 Loss_G: 1.4021 D(x): 0.7549 D(G(z)): 0.6736 / 0.2589\n",
      "[2/25][60/95] Loss_D: 0.9665 Loss_G: 1.5395 D(x): 0.5870 D(G(z)): 0.3342 / 0.2275\n",
      "[2/25][61/95] Loss_D: 1.3346 Loss_G: 0.6983 D(x): 0.4241 D(G(z)): 0.3471 / 0.5086\n",
      "[2/25][62/95] Loss_D: 1.2868 Loss_G: 1.5226 D(x): 0.7098 D(G(z)): 0.5909 / 0.2319\n",
      "[2/25][63/95] Loss_D: 1.3889 Loss_G: 0.8735 D(x): 0.4225 D(G(z)): 0.3678 / 0.4250\n",
      "[2/25][64/95] Loss_D: 1.5240 Loss_G: 0.7297 D(x): 0.4342 D(G(z)): 0.4695 / 0.4884\n",
      "[2/25][65/95] Loss_D: 1.3865 Loss_G: 2.1537 D(x): 0.6546 D(G(z)): 0.6072 / 0.1254\n",
      "[2/25][66/95] Loss_D: 1.5027 Loss_G: 0.6448 D(x): 0.3081 D(G(z)): 0.1961 / 0.5329\n",
      "[2/25][67/95] Loss_D: 1.1631 Loss_G: 1.8236 D(x): 0.7764 D(G(z)): 0.5847 / 0.1754\n",
      "[2/25][68/95] Loss_D: 1.1500 Loss_G: 0.8159 D(x): 0.4497 D(G(z)): 0.2543 / 0.4498\n",
      "[2/25][69/95] Loss_D: 1.0733 Loss_G: 2.0550 D(x): 0.7703 D(G(z)): 0.5409 / 0.1429\n",
      "[2/25][70/95] Loss_D: 1.1555 Loss_G: 0.7955 D(x): 0.4554 D(G(z)): 0.2619 / 0.4578\n",
      "[2/25][71/95] Loss_D: 1.2826 Loss_G: 1.4875 D(x): 0.6805 D(G(z)): 0.5749 / 0.2341\n",
      "[2/25][72/95] Loss_D: 1.2275 Loss_G: 0.7354 D(x): 0.4621 D(G(z)): 0.3228 / 0.4838\n",
      "[2/25][73/95] Loss_D: 1.1506 Loss_G: 2.2596 D(x): 0.8005 D(G(z)): 0.5967 / 0.1201\n",
      "[2/25][74/95] Loss_D: 1.8702 Loss_G: 0.3760 D(x): 0.2180 D(G(z)): 0.1593 / 0.6965\n",
      "[2/25][75/95] Loss_D: 1.6625 Loss_G: 1.6921 D(x): 0.8487 D(G(z)): 0.7650 / 0.1984\n",
      "[2/25][76/95] Loss_D: 1.1936 Loss_G: 0.9926 D(x): 0.4254 D(G(z)): 0.2221 / 0.3828\n",
      "[2/25][77/95] Loss_D: 1.2640 Loss_G: 1.3679 D(x): 0.6749 D(G(z)): 0.5662 / 0.2703\n",
      "[2/25][78/95] Loss_D: 1.2996 Loss_G: 1.1376 D(x): 0.5290 D(G(z)): 0.4559 / 0.3315\n",
      "[2/25][79/95] Loss_D: 1.0658 Loss_G: 0.7585 D(x): 0.5372 D(G(z)): 0.3208 / 0.4786\n",
      "[2/25][80/95] Loss_D: 1.7035 Loss_G: 2.5025 D(x): 0.7417 D(G(z)): 0.7281 / 0.0958\n",
      "[2/25][81/95] Loss_D: 1.9728 Loss_G: 0.5759 D(x): 0.1886 D(G(z)): 0.1213 / 0.5761\n",
      "[2/25][82/95] Loss_D: 1.4794 Loss_G: 1.5984 D(x): 0.7439 D(G(z)): 0.6583 / 0.2148\n",
      "[2/25][83/95] Loss_D: 1.2276 Loss_G: 1.1773 D(x): 0.4436 D(G(z)): 0.2994 / 0.3166\n",
      "[2/25][84/95] Loss_D: 1.1610 Loss_G: 1.6344 D(x): 0.6120 D(G(z)): 0.4616 / 0.2113\n",
      "[2/25][85/95] Loss_D: 1.2720 Loss_G: 0.5926 D(x): 0.4179 D(G(z)): 0.2843 / 0.5615\n",
      "[2/25][86/95] Loss_D: 1.4144 Loss_G: 3.1174 D(x): 0.8978 D(G(z)): 0.6997 / 0.0568\n",
      "[2/25][87/95] Loss_D: 1.9227 Loss_G: 0.5670 D(x): 0.2094 D(G(z)): 0.1129 / 0.5835\n",
      "[2/25][88/95] Loss_D: 1.3153 Loss_G: 1.0207 D(x): 0.7467 D(G(z)): 0.6274 / 0.3838\n",
      "[2/25][89/95] Loss_D: 1.3688 Loss_G: 1.2823 D(x): 0.5670 D(G(z)): 0.4989 / 0.2965\n",
      "[2/25][90/95] Loss_D: 1.3614 Loss_G: 0.8713 D(x): 0.4936 D(G(z)): 0.4484 / 0.4324\n",
      "[2/25][91/95] Loss_D: 1.4656 Loss_G: 1.2292 D(x): 0.5499 D(G(z)): 0.5534 / 0.3109\n",
      "[2/25][92/95] Loss_D: 1.1553 Loss_G: 0.7936 D(x): 0.5046 D(G(z)): 0.3499 / 0.4670\n",
      "[2/25][93/95] Loss_D: 1.2854 Loss_G: 2.1885 D(x): 0.7139 D(G(z)): 0.5903 / 0.1225\n",
      "[2/25][94/95] Loss_D: 1.9268 Loss_G: 0.4368 D(x): 0.2114 D(G(z)): 0.2314 / 0.6840\n",
      "[3/25][0/95] Loss_D: 1.9219 Loss_G: 2.8025 D(x): 0.9128 D(G(z)): 0.8185 / 0.0719\n",
      "[3/25][1/95] Loss_D: 1.7724 Loss_G: 0.6701 D(x): 0.2250 D(G(z)): 0.1515 / 0.5405\n",
      "[3/25][2/95] Loss_D: 1.2328 Loss_G: 1.3114 D(x): 0.7597 D(G(z)): 0.5808 / 0.2827\n",
      "[3/25][3/95] Loss_D: 1.3495 Loss_G: 1.2395 D(x): 0.4885 D(G(z)): 0.4114 / 0.3039\n",
      "[3/25][4/95] Loss_D: 1.4112 Loss_G: 1.1674 D(x): 0.5127 D(G(z)): 0.4811 / 0.3260\n",
      "[3/25][5/95] Loss_D: 1.5454 Loss_G: 0.9429 D(x): 0.4462 D(G(z)): 0.4803 / 0.3993\n",
      "[3/25][6/95] Loss_D: 1.5483 Loss_G: 1.0079 D(x): 0.4595 D(G(z)): 0.4912 / 0.3781\n",
      "[3/25][7/95] Loss_D: 1.5172 Loss_G: 0.6928 D(x): 0.4367 D(G(z)): 0.4698 / 0.5058\n",
      "[3/25][8/95] Loss_D: 1.2343 Loss_G: 2.1889 D(x): 0.7098 D(G(z)): 0.5775 / 0.1358\n",
      "[3/25][9/95] Loss_D: 0.5882 Loss_G: 2.7555 D(x): 0.6883 D(G(z)): 0.1707 / 0.0877\n",
      "[3/25][10/95] Loss_D: 1.4860 Loss_G: 0.4486 D(x): 0.2998 D(G(z)): 0.1312 / 0.6425\n",
      "[3/25][11/95] Loss_D: 1.4946 Loss_G: 1.1270 D(x): 0.7145 D(G(z)): 0.6780 / 0.3397\n",
      "[3/25][12/95] Loss_D: 0.6613 Loss_G: 3.4823 D(x): 0.9223 D(G(z)): 0.4333 / 0.0424\n",
      "[3/25][13/95] Loss_D: 2.1500 Loss_G: 0.6850 D(x): 0.1665 D(G(z)): 0.1001 / 0.5078\n",
      "[3/25][14/95] Loss_D: 1.6282 Loss_G: 0.3175 D(x): 0.5005 D(G(z)): 0.5866 / 0.7325\n",
      "[3/25][15/95] Loss_D: 1.6028 Loss_G: 1.7262 D(x): 0.8046 D(G(z)): 0.7331 / 0.2163\n",
      "[3/25][16/95] Loss_D: 1.4971 Loss_G: 0.8792 D(x): 0.3356 D(G(z)): 0.2402 / 0.4383\n",
      "[3/25][17/95] Loss_D: 1.3924 Loss_G: 0.6534 D(x): 0.6030 D(G(z)): 0.5585 / 0.5287\n",
      "[3/25][18/95] Loss_D: 1.1302 Loss_G: 2.0241 D(x): 0.8076 D(G(z)): 0.5882 / 0.1551\n",
      "[3/25][19/95] Loss_D: 1.5756 Loss_G: 0.6279 D(x): 0.3165 D(G(z)): 0.2452 / 0.5418\n",
      "[3/25][20/95] Loss_D: 1.1855 Loss_G: 0.8700 D(x): 0.6878 D(G(z)): 0.5431 / 0.4254\n",
      "[3/25][21/95] Loss_D: 1.4045 Loss_G: 1.6793 D(x): 0.6546 D(G(z)): 0.6009 / 0.2075\n",
      "[3/25][22/95] Loss_D: 1.5083 Loss_G: 0.7506 D(x): 0.3275 D(G(z)): 0.2618 / 0.4833\n",
      "[3/25][23/95] Loss_D: 1.4333 Loss_G: 0.7614 D(x): 0.5703 D(G(z)): 0.5544 / 0.4761\n",
      "[3/25][24/95] Loss_D: 1.2424 Loss_G: 1.5650 D(x): 0.6626 D(G(z)): 0.5302 / 0.2191\n",
      "[3/25][25/95] Loss_D: 1.5571 Loss_G: 0.7177 D(x): 0.3093 D(G(z)): 0.2723 / 0.5026\n",
      "[3/25][26/95] Loss_D: 1.4816 Loss_G: 1.7011 D(x): 0.7523 D(G(z)): 0.6615 / 0.1935\n",
      "[3/25][27/95] Loss_D: 1.4595 Loss_G: 0.7070 D(x): 0.3284 D(G(z)): 0.2431 / 0.5026\n",
      "[3/25][28/95] Loss_D: 1.4649 Loss_G: 1.2233 D(x): 0.6366 D(G(z)): 0.6102 / 0.3144\n",
      "[3/25][29/95] Loss_D: 1.2838 Loss_G: 0.8834 D(x): 0.4444 D(G(z)): 0.3405 / 0.4355\n",
      "[3/25][30/95] Loss_D: 1.2702 Loss_G: 1.6349 D(x): 0.6812 D(G(z)): 0.5507 / 0.2270\n",
      "[3/25][31/95] Loss_D: 1.3696 Loss_G: 0.7526 D(x): 0.4339 D(G(z)): 0.3496 / 0.4923\n",
      "[3/25][32/95] Loss_D: 0.9797 Loss_G: 1.5679 D(x): 0.7690 D(G(z)): 0.4960 / 0.2298\n",
      "[3/25][33/95] Loss_D: 1.2277 Loss_G: 0.7942 D(x): 0.4603 D(G(z)): 0.3002 / 0.4787\n",
      "[3/25][34/95] Loss_D: 1.3555 Loss_G: 1.2581 D(x): 0.7099 D(G(z)): 0.6045 / 0.3139\n",
      "[3/25][35/95] Loss_D: 1.3681 Loss_G: 0.9382 D(x): 0.5025 D(G(z)): 0.4518 / 0.4096\n",
      "[3/25][36/95] Loss_D: 1.2983 Loss_G: 0.9888 D(x): 0.5502 D(G(z)): 0.4670 / 0.3841\n",
      "[3/25][37/95] Loss_D: 1.1253 Loss_G: 1.7396 D(x): 0.6329 D(G(z)): 0.4642 / 0.1945\n",
      "[3/25][38/95] Loss_D: 1.0633 Loss_G: 2.0794 D(x): 0.6015 D(G(z)): 0.3683 / 0.1463\n",
      "[3/25][39/95] Loss_D: 2.2855 Loss_G: 0.0961 D(x): 0.1465 D(G(z)): 0.2022 / 0.9142\n",
      "[3/25][40/95] Loss_D: 2.9913 Loss_G: 1.5860 D(x): 0.9207 D(G(z)): 0.9113 / 0.2265\n",
      "[3/25][41/95] Loss_D: 1.1874 Loss_G: 1.6788 D(x): 0.4582 D(G(z)): 0.3068 / 0.2028\n",
      "[3/25][42/95] Loss_D: 1.2472 Loss_G: 0.6775 D(x): 0.4313 D(G(z)): 0.3025 / 0.5232\n",
      "[3/25][43/95] Loss_D: 1.3428 Loss_G: 1.2844 D(x): 0.6257 D(G(z)): 0.5501 / 0.2880\n",
      "[3/25][44/95] Loss_D: 1.2040 Loss_G: 1.3857 D(x): 0.5368 D(G(z)): 0.4211 / 0.2657\n",
      "[3/25][45/95] Loss_D: 1.4121 Loss_G: 0.6959 D(x): 0.4075 D(G(z)): 0.3668 / 0.5055\n",
      "[3/25][46/95] Loss_D: 1.3140 Loss_G: 0.7504 D(x): 0.5458 D(G(z)): 0.4860 / 0.4800\n",
      "[3/25][47/95] Loss_D: 1.3687 Loss_G: 1.0520 D(x): 0.5614 D(G(z)): 0.5299 / 0.3614\n",
      "[3/25][48/95] Loss_D: 1.2171 Loss_G: 0.7437 D(x): 0.4989 D(G(z)): 0.3822 / 0.4852\n",
      "[3/25][49/95] Loss_D: 1.3500 Loss_G: 1.2163 D(x): 0.6232 D(G(z)): 0.5762 / 0.3054\n",
      "[3/25][50/95] Loss_D: 1.1998 Loss_G: 0.8931 D(x): 0.4873 D(G(z)): 0.3540 / 0.4166\n",
      "[3/25][51/95] Loss_D: 1.3183 Loss_G: 0.7300 D(x): 0.5259 D(G(z)): 0.4736 / 0.4854\n",
      "[3/25][52/95] Loss_D: 1.0703 Loss_G: 2.0401 D(x): 0.8221 D(G(z)): 0.5766 / 0.1487\n",
      "[3/25][53/95] Loss_D: 2.0729 Loss_G: 0.3451 D(x): 0.1730 D(G(z)): 0.1652 / 0.7120\n",
      "[3/25][54/95] Loss_D: 1.7694 Loss_G: 0.9441 D(x): 0.7682 D(G(z)): 0.7700 / 0.3966\n",
      "[3/25][55/95] Loss_D: 1.2321 Loss_G: 1.3364 D(x): 0.5091 D(G(z)): 0.4085 / 0.2701\n",
      "[3/25][56/95] Loss_D: 1.4841 Loss_G: 0.4622 D(x): 0.3839 D(G(z)): 0.3729 / 0.6370\n",
      "[3/25][57/95] Loss_D: 1.5478 Loss_G: 1.4841 D(x): 0.7463 D(G(z)): 0.6873 / 0.2340\n",
      "[3/25][58/95] Loss_D: 1.2314 Loss_G: 0.9945 D(x): 0.4215 D(G(z)): 0.2785 / 0.3751\n",
      "[3/25][59/95] Loss_D: 1.3939 Loss_G: 0.8416 D(x): 0.5200 D(G(z)): 0.4926 / 0.4365\n",
      "[3/25][60/95] Loss_D: 1.1287 Loss_G: 1.3063 D(x): 0.6346 D(G(z)): 0.4763 / 0.2836\n",
      "[3/25][61/95] Loss_D: 1.1334 Loss_G: 0.8777 D(x): 0.4852 D(G(z)): 0.3173 / 0.4215\n",
      "[3/25][62/95] Loss_D: 1.2553 Loss_G: 1.2803 D(x): 0.6479 D(G(z)): 0.5495 / 0.2861\n",
      "[3/25][63/95] Loss_D: 1.0434 Loss_G: 1.0788 D(x): 0.5240 D(G(z)): 0.3080 / 0.3484\n",
      "[3/25][64/95] Loss_D: 1.2937 Loss_G: 0.4454 D(x): 0.4304 D(G(z)): 0.3460 / 0.6442\n",
      "[3/25][65/95] Loss_D: 1.9176 Loss_G: 1.1544 D(x): 0.7114 D(G(z)): 0.7809 / 0.3377\n",
      "[3/25][66/95] Loss_D: 1.6077 Loss_G: 0.4684 D(x): 0.3019 D(G(z)): 0.2946 / 0.6320\n",
      "[3/25][67/95] Loss_D: 1.5311 Loss_G: 1.4187 D(x): 0.7668 D(G(z)): 0.7064 / 0.2516\n",
      "[3/25][68/95] Loss_D: 1.4019 Loss_G: 0.7930 D(x): 0.3600 D(G(z)): 0.2716 / 0.4572\n",
      "[3/25][69/95] Loss_D: 1.2084 Loss_G: 0.7846 D(x): 0.6012 D(G(z)): 0.4900 / 0.4641\n",
      "[3/25][70/95] Loss_D: 1.2388 Loss_G: 1.3008 D(x): 0.6329 D(G(z)): 0.5283 / 0.2969\n",
      "[3/25][71/95] Loss_D: 1.2461 Loss_G: 0.7851 D(x): 0.4814 D(G(z)): 0.3687 / 0.4721\n",
      "[3/25][72/95] Loss_D: 1.2053 Loss_G: 0.9659 D(x): 0.6191 D(G(z)): 0.5055 / 0.3910\n",
      "[3/25][73/95] Loss_D: 1.7941 Loss_G: 1.4548 D(x): 0.4884 D(G(z)): 0.6413 / 0.2448\n",
      "[3/25][74/95] Loss_D: 1.2415 Loss_G: 0.7308 D(x): 0.3925 D(G(z)): 0.2383 / 0.4985\n",
      "[3/25][75/95] Loss_D: 1.7389 Loss_G: 1.2111 D(x): 0.5924 D(G(z)): 0.6858 / 0.3073\n",
      "[3/25][76/95] Loss_D: 1.3969 Loss_G: 1.0037 D(x): 0.3983 D(G(z)): 0.3566 / 0.3734\n",
      "[3/25][77/95] Loss_D: 1.4347 Loss_G: 1.0548 D(x): 0.4811 D(G(z)): 0.4795 / 0.3604\n",
      "[3/25][78/95] Loss_D: 0.9278 Loss_G: 2.8435 D(x): 0.7514 D(G(z)): 0.4483 / 0.0674\n",
      "[3/25][79/95] Loss_D: 2.2990 Loss_G: 0.2062 D(x): 0.1303 D(G(z)): 0.1237 / 0.8212\n",
      "[3/25][80/95] Loss_D: 2.0241 Loss_G: 1.5126 D(x): 0.8626 D(G(z)): 0.8206 / 0.2450\n",
      "[3/25][81/95] Loss_D: 1.4818 Loss_G: 1.3109 D(x): 0.4432 D(G(z)): 0.4297 / 0.2931\n",
      "[3/25][82/95] Loss_D: 0.6016 Loss_G: 2.1822 D(x): 0.7858 D(G(z)): 0.2857 / 0.1443\n",
      "[3/25][83/95] Loss_D: 2.0237 Loss_G: 0.3062 D(x): 0.2397 D(G(z)): 0.2792 / 0.7412\n",
      "[3/25][84/95] Loss_D: 1.7341 Loss_G: 0.8461 D(x): 0.7458 D(G(z)): 0.7539 / 0.4401\n",
      "[3/25][85/95] Loss_D: 1.3477 Loss_G: 1.3450 D(x): 0.5537 D(G(z)): 0.5077 / 0.2849\n",
      "[3/25][86/95] Loss_D: 1.4710 Loss_G: 0.5363 D(x): 0.3647 D(G(z)): 0.3250 / 0.5927\n",
      "[3/25][87/95] Loss_D: 1.5188 Loss_G: 1.3356 D(x): 0.7815 D(G(z)): 0.7085 / 0.2801\n",
      "[3/25][88/95] Loss_D: 1.6023 Loss_G: 0.7982 D(x): 0.3350 D(G(z)): 0.3641 / 0.4559\n",
      "[3/25][89/95] Loss_D: 1.1721 Loss_G: 1.9416 D(x): 0.7562 D(G(z)): 0.5746 / 0.1570\n",
      "[3/25][90/95] Loss_D: 1.9474 Loss_G: 0.5332 D(x): 0.1904 D(G(z)): 0.1921 / 0.5979\n",
      "[3/25][91/95] Loss_D: 1.5177 Loss_G: 1.0190 D(x): 0.6739 D(G(z)): 0.6519 / 0.3696\n",
      "[3/25][92/95] Loss_D: 1.3633 Loss_G: 1.1359 D(x): 0.4851 D(G(z)): 0.4250 / 0.3351\n",
      "[3/25][93/95] Loss_D: 1.2558 Loss_G: 1.0899 D(x): 0.5230 D(G(z)): 0.4227 / 0.3475\n",
      "[3/25][94/95] Loss_D: 1.4076 Loss_G: 0.6727 D(x): 0.4102 D(G(z)): 0.3742 / 0.5233\n",
      "[4/25][0/95] Loss_D: 1.4272 Loss_G: 1.7625 D(x): 0.8804 D(G(z)): 0.7066 / 0.1869\n",
      "[4/25][1/95] Loss_D: 1.6801 Loss_G: 0.7372 D(x): 0.2548 D(G(z)): 0.2201 / 0.4923\n",
      "[4/25][2/95] Loss_D: 1.3946 Loss_G: 0.7065 D(x): 0.5955 D(G(z)): 0.5610 / 0.5009\n",
      "[4/25][3/95] Loss_D: 1.3920 Loss_G: 0.9558 D(x): 0.5768 D(G(z)): 0.5549 / 0.4038\n",
      "[4/25][4/95] Loss_D: 1.2554 Loss_G: 0.9458 D(x): 0.5227 D(G(z)): 0.4268 / 0.4075\n",
      "[4/25][5/95] Loss_D: 1.1509 Loss_G: 0.9196 D(x): 0.5976 D(G(z)): 0.4484 / 0.4124\n",
      "[4/25][6/95] Loss_D: 1.3315 Loss_G: 1.0286 D(x): 0.5571 D(G(z)): 0.5050 / 0.3689\n",
      "[4/25][7/95] Loss_D: 1.2197 Loss_G: 0.7676 D(x): 0.4794 D(G(z)): 0.3619 / 0.4725\n",
      "[4/25][8/95] Loss_D: 0.9785 Loss_G: 1.9353 D(x): 0.8585 D(G(z)): 0.5431 / 0.1548\n",
      "[4/25][9/95] Loss_D: 1.2700 Loss_G: 1.3960 D(x): 0.3850 D(G(z)): 0.2041 / 0.2596\n",
      "[4/25][10/95] Loss_D: 1.3014 Loss_G: 0.5434 D(x): 0.4066 D(G(z)): 0.2995 / 0.5915\n",
      "[4/25][11/95] Loss_D: 1.4577 Loss_G: 1.1263 D(x): 0.6959 D(G(z)): 0.6528 / 0.3304\n",
      "[4/25][12/95] Loss_D: 1.2894 Loss_G: 0.9306 D(x): 0.4345 D(G(z)): 0.3450 / 0.4045\n",
      "[4/25][13/95] Loss_D: 0.9674 Loss_G: 1.4259 D(x): 0.6744 D(G(z)): 0.4234 / 0.2520\n",
      "[4/25][14/95] Loss_D: 1.4263 Loss_G: 0.6410 D(x): 0.4190 D(G(z)): 0.4060 / 0.5320\n",
      "[4/25][15/95] Loss_D: 1.6326 Loss_G: 0.6684 D(x): 0.5328 D(G(z)): 0.6195 / 0.5242\n",
      "[4/25][16/95] Loss_D: 1.6791 Loss_G: 0.9950 D(x): 0.5735 D(G(z)): 0.6615 / 0.3882\n",
      "[4/25][17/95] Loss_D: 1.0979 Loss_G: 1.1466 D(x): 0.5552 D(G(z)): 0.3804 / 0.3242\n",
      "[4/25][18/95] Loss_D: 1.3819 Loss_G: 0.4572 D(x): 0.3481 D(G(z)): 0.2420 / 0.6429\n",
      "[4/25][19/95] Loss_D: 1.5087 Loss_G: 1.3322 D(x): 0.9040 D(G(z)): 0.7381 / 0.3020\n",
      "[4/25][20/95] Loss_D: 1.0021 Loss_G: 1.4863 D(x): 0.5327 D(G(z)): 0.2904 / 0.2602\n",
      "[4/25][21/95] Loss_D: 1.2682 Loss_G: 0.6372 D(x): 0.4805 D(G(z)): 0.3888 / 0.5526\n",
      "[4/25][22/95] Loss_D: 1.5737 Loss_G: 1.0834 D(x): 0.7251 D(G(z)): 0.7020 / 0.3516\n",
      "[4/25][23/95] Loss_D: 1.4131 Loss_G: 0.9161 D(x): 0.4153 D(G(z)): 0.3839 / 0.4129\n",
      "[4/25][24/95] Loss_D: 1.6448 Loss_G: 1.2390 D(x): 0.4926 D(G(z)): 0.5777 / 0.3060\n",
      "[4/25][25/95] Loss_D: 1.4626 Loss_G: 0.7710 D(x): 0.3914 D(G(z)): 0.3724 / 0.4771\n",
      "[4/25][26/95] Loss_D: 1.3306 Loss_G: 1.7518 D(x): 0.6084 D(G(z)): 0.5393 / 0.1842\n",
      "[4/25][27/95] Loss_D: 1.5659 Loss_G: 0.5572 D(x): 0.3104 D(G(z)): 0.2847 / 0.5807\n",
      "[4/25][28/95] Loss_D: 1.5011 Loss_G: 1.3149 D(x): 0.6723 D(G(z)): 0.6559 / 0.2789\n",
      "[4/25][29/95] Loss_D: 1.3406 Loss_G: 0.9065 D(x): 0.4116 D(G(z)): 0.3436 / 0.4132\n",
      "[4/25][30/95] Loss_D: 1.4216 Loss_G: 0.6894 D(x): 0.5099 D(G(z)): 0.5103 / 0.5095\n",
      "[4/25][31/95] Loss_D: 1.5296 Loss_G: 0.8191 D(x): 0.5240 D(G(z)): 0.5738 / 0.4510\n",
      "[4/25][32/95] Loss_D: 1.4440 Loss_G: 1.0181 D(x): 0.5398 D(G(z)): 0.5461 / 0.3747\n",
      "[4/25][33/95] Loss_D: 1.3348 Loss_G: 0.9729 D(x): 0.5381 D(G(z)): 0.4854 / 0.3868\n",
      "[4/25][34/95] Loss_D: 0.9673 Loss_G: 1.9494 D(x): 0.7055 D(G(z)): 0.4449 / 0.1572\n",
      "[4/25][35/95] Loss_D: 1.9172 Loss_G: 0.2875 D(x): 0.2214 D(G(z)): 0.2574 / 0.7581\n",
      "[4/25][36/95] Loss_D: 2.0131 Loss_G: 1.6926 D(x): 0.8181 D(G(z)): 0.8277 / 0.1932\n",
      "[4/25][37/95] Loss_D: 1.7379 Loss_G: 0.8673 D(x): 0.2358 D(G(z)): 0.2131 / 0.4293\n",
      "[4/25][38/95] Loss_D: 1.3673 Loss_G: 0.8892 D(x): 0.5668 D(G(z)): 0.5304 / 0.4172\n",
      "[4/25][39/95] Loss_D: 1.1782 Loss_G: 1.5122 D(x): 0.5676 D(G(z)): 0.4434 / 0.2290\n",
      "[4/25][40/95] Loss_D: 1.2610 Loss_G: 0.9859 D(x): 0.4218 D(G(z)): 0.3132 / 0.3767\n",
      "[4/25][41/95] Loss_D: 1.4668 Loss_G: 0.6667 D(x): 0.5187 D(G(z)): 0.5406 / 0.5235\n",
      "[4/25][42/95] Loss_D: 1.4043 Loss_G: 1.1941 D(x): 0.5987 D(G(z)): 0.5801 / 0.3189\n",
      "[4/25][43/95] Loss_D: 1.5445 Loss_G: 0.5069 D(x): 0.3554 D(G(z)): 0.3474 / 0.6073\n",
      "[4/25][44/95] Loss_D: 1.5329 Loss_G: 0.7882 D(x): 0.6196 D(G(z)): 0.6311 / 0.4679\n",
      "[4/25][45/95] Loss_D: 1.1565 Loss_G: 1.5643 D(x): 0.6483 D(G(z)): 0.4943 / 0.2279\n",
      "[4/25][46/95] Loss_D: 1.5619 Loss_G: 0.3784 D(x): 0.3189 D(G(z)): 0.2704 / 0.6905\n",
      "[4/25][47/95] Loss_D: 1.7129 Loss_G: 1.0170 D(x): 0.7833 D(G(z)): 0.7604 / 0.3797\n",
      "[4/25][48/95] Loss_D: 1.3100 Loss_G: 1.3203 D(x): 0.5437 D(G(z)): 0.4697 / 0.2801\n",
      "[4/25][49/95] Loss_D: 1.4354 Loss_G: 0.7417 D(x): 0.3892 D(G(z)): 0.3464 / 0.4845\n",
      "[4/25][50/95] Loss_D: 1.4867 Loss_G: 0.7213 D(x): 0.5223 D(G(z)): 0.5501 / 0.4939\n",
      "[4/25][51/95] Loss_D: 1.3237 Loss_G: 1.0166 D(x): 0.5532 D(G(z)): 0.5008 / 0.3734\n",
      "[4/25][52/95] Loss_D: 1.2924 Loss_G: 0.9947 D(x): 0.5211 D(G(z)): 0.4449 / 0.3819\n",
      "[4/25][53/95] Loss_D: 1.3957 Loss_G: 1.0356 D(x): 0.4926 D(G(z)): 0.4788 / 0.3658\n",
      "[4/25][54/95] Loss_D: 1.1908 Loss_G: 1.3151 D(x): 0.5080 D(G(z)): 0.3804 / 0.2762\n",
      "[4/25][55/95] Loss_D: 1.2240 Loss_G: 1.4345 D(x): 0.5903 D(G(z)): 0.4818 / 0.2457\n",
      "[4/25][56/95] Loss_D: 1.4905 Loss_G: 0.4049 D(x): 0.3320 D(G(z)): 0.3007 / 0.6716\n",
      "[4/25][57/95] Loss_D: 1.6929 Loss_G: 1.1746 D(x): 0.7683 D(G(z)): 0.7526 / 0.3244\n",
      "[4/25][58/95] Loss_D: 1.5435 Loss_G: 0.6854 D(x): 0.3333 D(G(z)): 0.3334 / 0.5099\n",
      "[4/25][59/95] Loss_D: 1.4544 Loss_G: 0.9342 D(x): 0.5949 D(G(z)): 0.5940 / 0.4001\n",
      "[4/25][60/95] Loss_D: 1.3714 Loss_G: 0.7543 D(x): 0.4453 D(G(z)): 0.3969 / 0.4756\n",
      "[4/25][61/95] Loss_D: 1.3234 Loss_G: 0.9954 D(x): 0.6031 D(G(z)): 0.5443 / 0.3815\n",
      "[4/25][62/95] Loss_D: 1.4249 Loss_G: 0.6491 D(x): 0.4879 D(G(z)): 0.4694 / 0.5270\n",
      "[4/25][63/95] Loss_D: 1.3314 Loss_G: 1.0020 D(x): 0.6554 D(G(z)): 0.5854 / 0.3709\n",
      "[4/25][64/95] Loss_D: 1.0161 Loss_G: 1.3999 D(x): 0.5996 D(G(z)): 0.3883 / 0.2504\n",
      "[4/25][65/95] Loss_D: 0.8527 Loss_G: 1.7898 D(x): 0.6513 D(G(z)): 0.3371 / 0.1789\n",
      "[4/25][66/95] Loss_D: 1.5696 Loss_G: 0.5419 D(x): 0.2866 D(G(z)): 0.2355 / 0.5863\n",
      "[4/25][67/95] Loss_D: 1.4557 Loss_G: 0.7250 D(x): 0.6263 D(G(z)): 0.6150 / 0.4902\n",
      "[4/25][68/95] Loss_D: 1.4143 Loss_G: 0.8787 D(x): 0.4801 D(G(z)): 0.4685 / 0.4271\n",
      "[4/25][69/95] Loss_D: 1.7143 Loss_G: 0.6606 D(x): 0.5157 D(G(z)): 0.6291 / 0.5248\n",
      "[4/25][70/95] Loss_D: 1.4237 Loss_G: 0.9725 D(x): 0.5237 D(G(z)): 0.5150 / 0.3873\n",
      "[4/25][71/95] Loss_D: 1.1974 Loss_G: 0.8456 D(x): 0.5233 D(G(z)): 0.3985 / 0.4364\n",
      "[4/25][72/95] Loss_D: 1.3705 Loss_G: 1.4540 D(x): 0.6353 D(G(z)): 0.5605 / 0.2389\n",
      "[4/25][73/95] Loss_D: 1.0137 Loss_G: 1.1272 D(x): 0.5363 D(G(z)): 0.3045 / 0.3361\n",
      "[4/25][74/95] Loss_D: 1.2897 Loss_G: 0.6238 D(x): 0.4910 D(G(z)): 0.4043 / 0.5416\n",
      "[4/25][75/95] Loss_D: 1.1066 Loss_G: 1.6340 D(x): 0.8507 D(G(z)): 0.6047 / 0.2017\n",
      "[4/25][76/95] Loss_D: 1.7896 Loss_G: 0.4980 D(x): 0.2309 D(G(z)): 0.1998 / 0.6126\n",
      "[4/25][77/95] Loss_D: 1.4316 Loss_G: 0.8178 D(x): 0.7654 D(G(z)): 0.6693 / 0.4470\n",
      "[4/25][78/95] Loss_D: 1.2863 Loss_G: 1.8497 D(x): 0.6879 D(G(z)): 0.5828 / 0.1753\n",
      "[4/25][79/95] Loss_D: 1.8380 Loss_G: 0.5233 D(x): 0.2352 D(G(z)): 0.1937 / 0.6028\n",
      "[4/25][80/95] Loss_D: 1.4061 Loss_G: 1.0157 D(x): 0.7614 D(G(z)): 0.6696 / 0.3686\n",
      "[4/25][81/95] Loss_D: 1.7672 Loss_G: 1.1459 D(x): 0.4004 D(G(z)): 0.5370 / 0.3297\n",
      "[4/25][82/95] Loss_D: 1.5805 Loss_G: 0.6614 D(x): 0.3695 D(G(z)): 0.4131 / 0.5258\n",
      "[4/25][83/95] Loss_D: 1.3523 Loss_G: 1.2930 D(x): 0.6260 D(G(z)): 0.5686 / 0.2795\n",
      "[4/25][84/95] Loss_D: 1.5434 Loss_G: 0.7900 D(x): 0.3120 D(G(z)): 0.2915 / 0.4668\n",
      "[4/25][85/95] Loss_D: 1.4001 Loss_G: 1.1568 D(x): 0.5959 D(G(z)): 0.5704 / 0.3212\n",
      "[4/25][86/95] Loss_D: 0.9409 Loss_G: 2.2304 D(x): 0.6616 D(G(z)): 0.3964 / 0.1184\n",
      "[4/25][87/95] Loss_D: 2.0620 Loss_G: 0.3416 D(x): 0.1902 D(G(z)): 0.1878 / 0.7126\n",
      "[4/25][88/95] Loss_D: 1.6298 Loss_G: 1.1173 D(x): 0.7451 D(G(z)): 0.7304 / 0.3431\n",
      "[4/25][89/95] Loss_D: 0.9949 Loss_G: 1.7623 D(x): 0.6237 D(G(z)): 0.3903 / 0.2000\n",
      "[4/25][90/95] Loss_D: 1.8493 Loss_G: 0.4133 D(x): 0.2505 D(G(z)): 0.2643 / 0.6638\n",
      "[4/25][91/95] Loss_D: 1.5207 Loss_G: 0.9807 D(x): 0.7814 D(G(z)): 0.7124 / 0.3894\n",
      "[4/25][92/95] Loss_D: 1.4772 Loss_G: 1.0066 D(x): 0.5060 D(G(z)): 0.5213 / 0.3795\n",
      "[4/25][93/95] Loss_D: 1.0400 Loss_G: 1.8139 D(x): 0.9263 D(G(z)): 0.6056 / 0.1823\n",
      "[4/25][94/95] Loss_D: 3.6001 Loss_G: 0.6330 D(x): 0.0408 D(G(z)): 0.0469 / 0.5365\n",
      "[5/25][0/95] Loss_D: 1.6612 Loss_G: 0.4419 D(x): 0.6909 D(G(z)): 0.7024 / 0.6640\n",
      "[5/25][1/95] Loss_D: 1.4567 Loss_G: 1.4359 D(x): 0.7965 D(G(z)): 0.6875 / 0.2567\n",
      "[5/25][2/95] Loss_D: 1.3455 Loss_G: 1.2643 D(x): 0.3668 D(G(z)): 0.2480 / 0.3094\n",
      "[5/25][3/95] Loss_D: 1.3112 Loss_G: 1.0906 D(x): 0.5832 D(G(z)): 0.4751 / 0.3665\n",
      "[5/25][4/95] Loss_D: 1.1927 Loss_G: 1.7390 D(x): 0.6588 D(G(z)): 0.4708 / 0.1935\n",
      "[5/25][5/95] Loss_D: 1.2976 Loss_G: 1.0880 D(x): 0.3658 D(G(z)): 0.2161 / 0.3847\n",
      "[5/25][6/95] Loss_D: 1.4156 Loss_G: 0.7967 D(x): 0.5610 D(G(z)): 0.5125 / 0.4748\n",
      "[5/25][7/95] Loss_D: 1.2902 Loss_G: 1.1864 D(x): 0.6149 D(G(z)): 0.5160 / 0.3279\n",
      "[5/25][8/95] Loss_D: 1.4376 Loss_G: 1.1927 D(x): 0.4940 D(G(z)): 0.4883 / 0.3124\n",
      "[5/25][9/95] Loss_D: 1.6190 Loss_G: 0.7015 D(x): 0.3342 D(G(z)): 0.3390 / 0.5163\n",
      "[5/25][10/95] Loss_D: 1.3172 Loss_G: 1.1189 D(x): 0.6385 D(G(z)): 0.5547 / 0.3518\n",
      "[5/25][11/95] Loss_D: 1.1844 Loss_G: 1.3245 D(x): 0.5728 D(G(z)): 0.4096 / 0.2879\n",
      "[5/25][12/95] Loss_D: 0.6569 Loss_G: 2.1793 D(x): 0.7887 D(G(z)): 0.3267 / 0.1334\n",
      "[5/25][13/95] Loss_D: 1.4991 Loss_G: 0.7125 D(x): 0.3397 D(G(z)): 0.2641 / 0.5090\n",
      "[5/25][14/95] Loss_D: 1.1118 Loss_G: 0.9427 D(x): 0.6683 D(G(z)): 0.4845 / 0.4004\n",
      "[5/25][15/95] Loss_D: 1.7242 Loss_G: 0.5441 D(x): 0.3582 D(G(z)): 0.4546 / 0.5860\n",
      "[5/25][16/95] Loss_D: 1.6343 Loss_G: 0.8989 D(x): 0.6238 D(G(z)): 0.6758 / 0.4308\n",
      "[5/25][17/95] Loss_D: 1.2846 Loss_G: 1.0604 D(x): 0.5091 D(G(z)): 0.4269 / 0.3548\n",
      "[5/25][18/95] Loss_D: 1.3109 Loss_G: 0.8629 D(x): 0.4896 D(G(z)): 0.4325 / 0.4295\n",
      "[5/25][19/95] Loss_D: 1.2123 Loss_G: 1.0481 D(x): 0.6129 D(G(z)): 0.5033 / 0.3570\n",
      "[5/25][20/95] Loss_D: 0.9094 Loss_G: 1.7242 D(x): 0.7145 D(G(z)): 0.4229 / 0.1943\n",
      "[5/25][21/95] Loss_D: 1.7926 Loss_G: 0.5373 D(x): 0.2645 D(G(z)): 0.3185 / 0.5924\n",
      "[5/25][22/95] Loss_D: 1.3291 Loss_G: 0.8912 D(x): 0.6584 D(G(z)): 0.5686 / 0.4207\n",
      "[5/25][23/95] Loss_D: 1.6198 Loss_G: 0.7843 D(x): 0.5147 D(G(z)): 0.5948 / 0.4631\n",
      "[5/25][24/95] Loss_D: 1.0775 Loss_G: 1.3131 D(x): 0.6200 D(G(z)): 0.4356 / 0.2805\n",
      "[5/25][25/95] Loss_D: 1.4409 Loss_G: 0.7233 D(x): 0.3624 D(G(z)): 0.3144 / 0.4927\n",
      "[5/25][26/95] Loss_D: 1.0307 Loss_G: 1.5087 D(x): 0.8060 D(G(z)): 0.5433 / 0.2340\n",
      "[5/25][27/95] Loss_D: 1.6402 Loss_G: 0.6394 D(x): 0.2962 D(G(z)): 0.3004 / 0.5338\n",
      "[5/25][28/95] Loss_D: 1.3119 Loss_G: 0.7712 D(x): 0.6630 D(G(z)): 0.5849 / 0.4688\n",
      "[5/25][29/95] Loss_D: 1.6112 Loss_G: 0.6377 D(x): 0.4434 D(G(z)): 0.5344 / 0.5330\n",
      "[5/25][30/95] Loss_D: 1.3700 Loss_G: 0.9815 D(x): 0.6283 D(G(z)): 0.5856 / 0.3859\n",
      "[5/25][31/95] Loss_D: 1.4286 Loss_G: 1.0256 D(x): 0.5052 D(G(z)): 0.5145 / 0.3644\n",
      "[5/25][32/95] Loss_D: 1.4967 Loss_G: 0.5563 D(x): 0.3383 D(G(z)): 0.3054 / 0.5782\n",
      "[5/25][33/95] Loss_D: 1.5039 Loss_G: 0.8760 D(x): 0.7837 D(G(z)): 0.6963 / 0.4223\n",
      "[5/25][34/95] Loss_D: 0.9393 Loss_G: 2.5576 D(x): 0.8469 D(G(z)): 0.5249 / 0.0924\n",
      "[5/25][35/95] Loss_D: 2.5836 Loss_G: 1.0966 D(x): 0.1036 D(G(z)): 0.1062 / 0.3409\n",
      "[5/25][36/95] Loss_D: 1.4663 Loss_G: 0.3030 D(x): 0.3880 D(G(z)): 0.3892 / 0.7471\n",
      "[5/25][37/95] Loss_D: 1.6998 Loss_G: 0.8282 D(x): 0.7835 D(G(z)): 0.7463 / 0.4475\n",
      "[5/25][38/95] Loss_D: 1.2881 Loss_G: 1.3082 D(x): 0.5394 D(G(z)): 0.4722 / 0.2772\n",
      "[5/25][39/95] Loss_D: 1.4550 Loss_G: 0.8123 D(x): 0.3489 D(G(z)): 0.3002 / 0.4504\n",
      "[5/25][40/95] Loss_D: 1.3280 Loss_G: 0.8219 D(x): 0.5598 D(G(z)): 0.5115 / 0.4473\n",
      "[5/25][41/95] Loss_D: 1.2787 Loss_G: 1.0455 D(x): 0.5721 D(G(z)): 0.5001 / 0.3617\n",
      "[5/25][42/95] Loss_D: 1.2521 Loss_G: 0.9340 D(x): 0.5201 D(G(z)): 0.4285 / 0.3978\n",
      "[5/25][43/95] Loss_D: 1.3121 Loss_G: 0.8563 D(x): 0.4964 D(G(z)): 0.4459 / 0.4298\n",
      "[5/25][44/95] Loss_D: 0.7781 Loss_G: 1.8540 D(x): 0.7989 D(G(z)): 0.4112 / 0.1694\n",
      "[5/25][45/95] Loss_D: 1.7973 Loss_G: 0.5675 D(x): 0.2758 D(G(z)): 0.3113 / 0.5707\n",
      "[5/25][46/95] Loss_D: 1.7314 Loss_G: 0.3873 D(x): 0.4521 D(G(z)): 0.5912 / 0.6845\n",
      "[5/25][47/95] Loss_D: 1.3911 Loss_G: 0.9706 D(x): 0.7251 D(G(z)): 0.6469 / 0.3976\n",
      "[5/25][48/95] Loss_D: 1.2415 Loss_G: 1.0746 D(x): 0.5500 D(G(z)): 0.4637 / 0.3510\n",
      "[5/25][49/95] Loss_D: 1.3147 Loss_G: 0.8883 D(x): 0.5259 D(G(z)): 0.4704 / 0.4154\n",
      "[5/25][50/95] Loss_D: 1.1377 Loss_G: 0.9643 D(x): 0.6008 D(G(z)): 0.4555 / 0.3883\n",
      "[5/25][51/95] Loss_D: 1.2501 Loss_G: 1.0187 D(x): 0.5735 D(G(z)): 0.4874 / 0.3687\n",
      "[5/25][52/95] Loss_D: 1.3582 Loss_G: 0.9882 D(x): 0.5026 D(G(z)): 0.4774 / 0.3780\n",
      "[5/25][53/95] Loss_D: 1.2950 Loss_G: 0.7657 D(x): 0.5069 D(G(z)): 0.4377 / 0.4801\n",
      "[5/25][54/95] Loss_D: 1.8630 Loss_G: 1.3840 D(x): 0.5447 D(G(z)): 0.6966 / 0.2575\n",
      "[5/25][55/95] Loss_D: 1.7556 Loss_G: 0.8437 D(x): 0.2720 D(G(z)): 0.3216 / 0.4354\n",
      "[5/25][56/95] Loss_D: 1.2323 Loss_G: 0.6292 D(x): 0.4972 D(G(z)): 0.3975 / 0.5393\n",
      "[5/25][57/95] Loss_D: 1.4438 Loss_G: 1.7360 D(x): 0.7125 D(G(z)): 0.6512 / 0.1802\n",
      "[5/25][58/95] Loss_D: 1.4378 Loss_G: 1.0194 D(x): 0.3759 D(G(z)): 0.3518 / 0.3642\n",
      "[5/25][59/95] Loss_D: 1.4253 Loss_G: 0.5669 D(x): 0.4199 D(G(z)): 0.4138 / 0.5703\n",
      "[5/25][60/95] Loss_D: 1.2717 Loss_G: 1.2028 D(x): 0.6812 D(G(z)): 0.5809 / 0.3218\n",
      "[5/25][61/95] Loss_D: 1.1275 Loss_G: 1.1166 D(x): 0.5251 D(G(z)): 0.3466 / 0.3439\n",
      "[5/25][62/95] Loss_D: 1.3460 Loss_G: 0.7050 D(x): 0.5310 D(G(z)): 0.4677 / 0.5038\n",
      "[5/25][63/95] Loss_D: 1.3495 Loss_G: 1.1807 D(x): 0.6129 D(G(z)): 0.5578 / 0.3167\n",
      "[5/25][64/95] Loss_D: 1.3830 Loss_G: 0.6954 D(x): 0.4159 D(G(z)): 0.3585 / 0.5068\n",
      "[5/25][65/95] Loss_D: 1.3331 Loss_G: 1.4438 D(x): 0.6738 D(G(z)): 0.5904 / 0.2521\n",
      "[5/25][66/95] Loss_D: 1.4127 Loss_G: 0.8435 D(x): 0.4061 D(G(z)): 0.3292 / 0.4407\n",
      "[5/25][67/95] Loss_D: 1.4320 Loss_G: 1.0370 D(x): 0.6018 D(G(z)): 0.5682 / 0.3667\n",
      "[5/25][68/95] Loss_D: 1.3162 Loss_G: 1.0767 D(x): 0.5008 D(G(z)): 0.4320 / 0.3570\n",
      "[5/25][69/95] Loss_D: 1.3128 Loss_G: 0.7549 D(x): 0.4666 D(G(z)): 0.3978 / 0.4837\n",
      "[5/25][70/95] Loss_D: 1.2842 Loss_G: 1.2921 D(x): 0.6574 D(G(z)): 0.5498 / 0.2916\n",
      "[5/25][71/95] Loss_D: 0.9360 Loss_G: 2.0220 D(x): 0.6702 D(G(z)): 0.3952 / 0.1509\n",
      "[5/25][72/95] Loss_D: 1.6760 Loss_G: 0.6250 D(x): 0.2978 D(G(z)): 0.2511 / 0.5471\n",
      "[5/25][73/95] Loss_D: 1.5922 Loss_G: 0.6752 D(x): 0.6105 D(G(z)): 0.6378 / 0.5205\n",
      "[5/25][74/95] Loss_D: 1.5733 Loss_G: 0.9096 D(x): 0.4941 D(G(z)): 0.5540 / 0.4137\n",
      "[5/25][75/95] Loss_D: 1.5168 Loss_G: 0.7456 D(x): 0.4390 D(G(z)): 0.4740 / 0.4826\n",
      "[5/25][76/95] Loss_D: 1.4766 Loss_G: 0.8749 D(x): 0.5976 D(G(z)): 0.6051 / 0.4259\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(batchSize, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "for epoch in range(niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label, device=device)\n",
    "\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, niter, i, len(dataloader),\n",
    "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu,\n",
    "                    '%s/real_samples.png' % outf,\n",
    "                    normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.detach(),\n",
    "                    '%s/fake_samples_epoch_%03d.png' % (outf, epoch),\n",
    "                    normalize=True)\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (outf, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
