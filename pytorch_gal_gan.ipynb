{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  7849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa424104710>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing parameters:\n",
    "\n",
    "dataroot='gals/'\n",
    "device = torch.device(\"cpu\") # If GPU then use \"cuda:0\"\n",
    "ngpu = 0 #number of GPUs to use \n",
    "nz = 10 #size of the latent z vector\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "workers = 4*ngpu #number of data loading workers\n",
    "batchSize = 64 #input batch size\n",
    "imageSize = 64 #the height / width of the input image to network\n",
    "niter = 5 #number of epochs to train for\n",
    "lr = 0.0002 #learning rate, default=0.0002\n",
    "beta1 = 0.5 #beta1 for adam. default=0.5\n",
    "outf='outputs' #folder to output images and model checkpoints\n",
    "\n",
    "\n",
    "manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dset.MNIST(root=dataroot, download=True,\n",
    "                     transform=transforms.Compose([transforms.Resize(imageSize),transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,)),]))\n",
    "nc=1\n",
    "\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=int(workers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(10, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "#if netG != '':\n",
    "#    netG.load_state_dict(torch.load(netG))\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "#if netD != '':\n",
    "#    netD.load_state_dict(torch.load(netD))\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5][0/95] Loss_D: 1.9996 Loss_G: 3.3831 D(x): 0.4079 D(G(z)): 0.5822 / 0.0561\n",
      "[0/5][1/95] Loss_D: 2.8406 Loss_G: 3.8503 D(x): 0.9999 D(G(z)): 0.8927 / 0.0643\n",
      "[0/5][2/95] Loss_D: 1.9712 Loss_G: 5.1548 D(x): 0.9998 D(G(z)): 0.8036 / 0.0213\n",
      "[0/5][3/95] Loss_D: 1.2470 Loss_G: 6.1126 D(x): 0.9841 D(G(z)): 0.6376 / 0.0080\n",
      "[0/5][4/95] Loss_D: 0.7188 Loss_G: 6.5935 D(x): 0.9836 D(G(z)): 0.3939 / 0.0029\n",
      "[0/5][5/95] Loss_D: 0.7281 Loss_G: 6.6545 D(x): 0.9834 D(G(z)): 0.3189 / 0.0020\n",
      "[0/5][6/95] Loss_D: 0.2974 Loss_G: 6.6936 D(x): 0.9718 D(G(z)): 0.1882 / 0.0021\n",
      "[0/5][7/95] Loss_D: 0.3518 Loss_G: 7.0840 D(x): 0.9779 D(G(z)): 0.2309 / 0.0014\n",
      "[0/5][8/95] Loss_D: 0.1419 Loss_G: 6.8598 D(x): 0.9871 D(G(z)): 0.1110 / 0.0019\n",
      "[0/5][9/95] Loss_D: 0.3171 Loss_G: 6.5104 D(x): 0.9615 D(G(z)): 0.1370 / 0.0027\n",
      "[0/5][10/95] Loss_D: 0.4059 Loss_G: 6.2410 D(x): 0.9339 D(G(z)): 0.1027 / 0.0032\n",
      "[0/5][11/95] Loss_D: 0.2347 Loss_G: 7.1606 D(x): 0.9851 D(G(z)): 0.1809 / 0.0012\n",
      "[0/5][12/95] Loss_D: 0.0889 Loss_G: 6.8134 D(x): 0.9880 D(G(z)): 0.0655 / 0.0022\n",
      "[0/5][13/95] Loss_D: 0.1671 Loss_G: 6.4000 D(x): 0.9685 D(G(z)): 0.0799 / 0.0038\n",
      "[0/5][14/95] Loss_D: 0.2360 Loss_G: 8.0589 D(x): 0.9941 D(G(z)): 0.1870 / 0.0008\n",
      "[0/5][15/95] Loss_D: 0.0530 Loss_G: 7.6587 D(x): 0.9812 D(G(z)): 0.0300 / 0.0013\n",
      "[0/5][16/95] Loss_D: 0.0894 Loss_G: 6.3315 D(x): 0.9743 D(G(z)): 0.0384 / 0.0034\n",
      "[0/5][17/95] Loss_D: 0.1425 Loss_G: 7.3647 D(x): 0.9940 D(G(z)): 0.1181 / 0.0015\n",
      "[0/5][18/95] Loss_D: 0.0490 Loss_G: 7.4657 D(x): 0.9975 D(G(z)): 0.0448 / 0.0012\n",
      "[0/5][19/95] Loss_D: 0.0945 Loss_G: 6.9472 D(x): 0.9695 D(G(z)): 0.0341 / 0.0019\n",
      "[0/5][20/95] Loss_D: 0.0485 Loss_G: 6.7321 D(x): 0.9967 D(G(z)): 0.0434 / 0.0023\n",
      "[0/5][21/95] Loss_D: 0.0635 Loss_G: 7.3462 D(x): 0.9995 D(G(z)): 0.0593 / 0.0014\n",
      "[0/5][22/95] Loss_D: 0.0330 Loss_G: 7.1188 D(x): 0.9916 D(G(z)): 0.0230 / 0.0014\n",
      "[0/5][23/95] Loss_D: 0.0318 Loss_G: 6.8346 D(x): 0.9963 D(G(z)): 0.0272 / 0.0018\n",
      "[0/5][24/95] Loss_D: 0.0792 Loss_G: 6.6891 D(x): 0.9810 D(G(z)): 0.0326 / 0.0019\n",
      "[0/5][25/95] Loss_D: 0.0634 Loss_G: 7.6940 D(x): 0.9974 D(G(z)): 0.0568 / 0.0008\n",
      "[0/5][26/95] Loss_D: 0.0206 Loss_G: 7.5812 D(x): 0.9953 D(G(z)): 0.0155 / 0.0009\n",
      "[0/5][27/95] Loss_D: 0.0252 Loss_G: 6.8041 D(x): 0.9942 D(G(z)): 0.0186 / 0.0018\n",
      "[0/5][28/95] Loss_D: 0.0524 Loss_G: 7.0096 D(x): 0.9922 D(G(z)): 0.0428 / 0.0015\n",
      "[0/5][29/95] Loss_D: 0.0454 Loss_G: 7.5411 D(x): 0.9906 D(G(z)): 0.0339 / 0.0008\n",
      "[0/5][30/95] Loss_D: 0.1006 Loss_G: 6.8560 D(x): 0.9609 D(G(z)): 0.0161 / 0.0019\n",
      "[0/5][31/95] Loss_D: 0.0338 Loss_G: 6.8602 D(x): 0.9992 D(G(z)): 0.0322 / 0.0016\n",
      "[0/5][32/95] Loss_D: 0.0481 Loss_G: 7.9154 D(x): 0.9977 D(G(z)): 0.0441 / 0.0006\n",
      "[0/5][33/95] Loss_D: 0.0225 Loss_G: 7.6446 D(x): 0.9931 D(G(z)): 0.0148 / 0.0008\n",
      "[0/5][34/95] Loss_D: 0.0275 Loss_G: 6.8950 D(x): 0.9880 D(G(z)): 0.0132 / 0.0016\n",
      "[0/5][35/95] Loss_D: 0.0313 Loss_G: 6.9655 D(x): 0.9972 D(G(z)): 0.0277 / 0.0014\n",
      "[0/5][36/95] Loss_D: 0.0242 Loss_G: 7.4922 D(x): 0.9970 D(G(z)): 0.0206 / 0.0010\n",
      "[0/5][37/95] Loss_D: 0.0196 Loss_G: 6.8863 D(x): 0.9914 D(G(z)): 0.0101 / 0.0013\n",
      "[0/5][38/95] Loss_D: 0.0196 Loss_G: 6.9250 D(x): 0.9994 D(G(z)): 0.0187 / 0.0019\n",
      "[0/5][39/95] Loss_D: 0.0348 Loss_G: 6.9283 D(x): 0.9907 D(G(z)): 0.0218 / 0.0015\n",
      "[0/5][40/95] Loss_D: 0.0244 Loss_G: 7.0580 D(x): 0.9946 D(G(z)): 0.0179 / 0.0012\n",
      "[0/5][41/95] Loss_D: 0.0191 Loss_G: 7.1971 D(x): 0.9976 D(G(z)): 0.0163 / 0.0012\n",
      "[0/5][42/95] Loss_D: 0.0157 Loss_G: 7.1302 D(x): 0.9998 D(G(z)): 0.0153 / 0.0012\n",
      "[0/5][43/95] Loss_D: 0.0192 Loss_G: 7.0895 D(x): 0.9973 D(G(z)): 0.0163 / 0.0012\n",
      "[0/5][44/95] Loss_D: 0.0311 Loss_G: 6.8445 D(x): 0.9876 D(G(z)): 0.0167 / 0.0015\n",
      "[0/5][45/95] Loss_D: 0.0255 Loss_G: 7.4460 D(x): 0.9992 D(G(z)): 0.0243 / 0.0008\n",
      "[0/5][46/95] Loss_D: 0.0142 Loss_G: 7.6016 D(x): 0.9987 D(G(z)): 0.0128 / 0.0007\n",
      "[0/5][47/95] Loss_D: 0.0360 Loss_G: 6.5223 D(x): 0.9832 D(G(z)): 0.0098 / 0.0020\n",
      "[0/5][48/95] Loss_D: 0.0521 Loss_G: 9.5779 D(x): 0.9995 D(G(z)): 0.0495 / 0.0001\n",
      "[0/5][49/95] Loss_D: 0.0097 Loss_G: 9.3962 D(x): 0.9928 D(G(z)): 0.0018 / 0.0001\n",
      "[0/5][50/95] Loss_D: 0.0146 Loss_G: 7.2925 D(x): 0.9890 D(G(z)): 0.0030 / 0.0009\n",
      "[0/5][51/95] Loss_D: 0.0214 Loss_G: 7.2535 D(x): 0.9995 D(G(z)): 0.0206 / 0.0010\n",
      "[0/5][52/95] Loss_D: 0.0758 Loss_G: 6.5000 D(x): 0.9690 D(G(z)): 0.0200 / 0.0020\n",
      "[0/5][53/95] Loss_D: 0.2538 Loss_G: 10.4969 D(x): 0.9685 D(G(z)): 0.0483 / 0.0000\n",
      "[0/5][54/95] Loss_D: 0.0646 Loss_G: 6.6804 D(x): 0.9506 D(G(z)): 0.0011 / 0.0017\n",
      "[0/5][55/95] Loss_D: 0.0794 Loss_G: 12.9994 D(x): 0.9999 D(G(z)): 0.0748 / 0.0000\n",
      "[0/5][56/95] Loss_D: 0.0039 Loss_G: 13.3415 D(x): 0.9964 D(G(z)): 0.0002 / 0.0000\n",
      "[0/5][57/95] Loss_D: 0.0022 Loss_G: 10.9692 D(x): 0.9980 D(G(z)): 0.0002 / 0.0000\n",
      "[0/5][58/95] Loss_D: 0.0063 Loss_G: 7.3686 D(x): 0.9967 D(G(z)): 0.0028 / 0.0009\n",
      "[0/5][59/95] Loss_D: 0.0863 Loss_G: 15.2414 D(x): 0.9972 D(G(z)): 0.0776 / 0.0000\n",
      "[0/5][60/95] Loss_D: 0.5239 Loss_G: 0.0022 D(x): 0.7075 D(G(z)): 0.0000 / 0.9978\n",
      "[0/5][61/95] Loss_D: 8.0699 Loss_G: 32.9663 D(x): 1.0000 D(G(z)): 0.9987 / 0.0000\n",
      "[0/5][62/95] Loss_D: 7.1973 Loss_G: 30.2277 D(x): 0.0764 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][63/95] Loss_D: 0.0289 Loss_G: 27.5272 D(x): 0.9774 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][64/95] Loss_D: 0.0008 Loss_G: 20.7570 D(x): 0.9993 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][65/95] Loss_D: 0.1077 Loss_G: 9.3567 D(x): 0.9869 D(G(z)): 0.0526 / 0.0089\n",
      "[0/5][66/95] Loss_D: 6.6824 Loss_G: 28.7266 D(x): 0.9976 D(G(z)): 0.9427 / 0.0000\n",
      "[0/5][67/95] Loss_D: 2.5865 Loss_G: 22.5554 D(x): 0.3437 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][68/95] Loss_D: 0.1178 Loss_G: 10.6601 D(x): 0.9681 D(G(z)): 0.0005 / 0.0039\n",
      "[0/5][69/95] Loss_D: 2.5063 Loss_G: 21.2848 D(x): 0.9966 D(G(z)): 0.6018 / 0.0000\n",
      "[0/5][70/95] Loss_D: 0.9348 Loss_G: 17.6161 D(x): 0.7601 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][71/95] Loss_D: 0.9730 Loss_G: 2.7445 D(x): 0.7001 D(G(z)): 0.0016 / 0.1775\n",
      "[0/5][72/95] Loss_D: 5.5599 Loss_G: 23.6009 D(x): 0.9035 D(G(z)): 0.9787 / 0.0000\n",
      "[0/5][73/95] Loss_D: 9.5709 Loss_G: 7.6476 D(x): 0.0002 D(G(z)): 0.0000 / 0.0012\n",
      "[0/5][74/95] Loss_D: 0.8297 Loss_G: 6.9741 D(x): 0.9314 D(G(z)): 0.4481 / 0.0016\n",
      "[0/5][75/95] Loss_D: 0.4291 Loss_G: 10.2323 D(x): 0.9233 D(G(z)): 0.2579 / 0.0001\n",
      "[0/5][76/95] Loss_D: 1.6084 Loss_G: 0.3693 D(x): 0.3783 D(G(z)): 0.0080 / 0.7224\n",
      "[0/5][77/95] Loss_D: 4.0359 Loss_G: 14.8060 D(x): 0.9819 D(G(z)): 0.9707 / 0.0000\n",
      "[0/5][78/95] Loss_D: 2.9658 Loss_G: 5.8417 D(x): 0.1131 D(G(z)): 0.0001 / 0.0037\n",
      "[0/5][79/95] Loss_D: 0.5568 Loss_G: 5.1814 D(x): 0.9069 D(G(z)): 0.3018 / 0.0071\n",
      "[0/5][80/95] Loss_D: 1.2161 Loss_G: 15.2264 D(x): 0.9452 D(G(z)): 0.6581 / 0.0000\n",
      "[0/5][81/95] Loss_D: 6.5552 Loss_G: 2.8517 D(x): 0.0024 D(G(z)): 0.0000 / 0.0683\n",
      "[0/5][82/95] Loss_D: 1.8761 Loss_G: 9.9302 D(x): 0.9126 D(G(z)): 0.8084 / 0.0001\n",
      "[0/5][83/95] Loss_D: 2.7544 Loss_G: 1.2437 D(x): 0.1252 D(G(z)): 0.0013 / 0.3114\n",
      "[0/5][84/95] Loss_D: 1.9909 Loss_G: 9.9083 D(x): 0.9642 D(G(z)): 0.8497 / 0.0001\n",
      "[0/5][85/95] Loss_D: 1.1632 Loss_G: 6.0680 D(x): 0.4776 D(G(z)): 0.0014 / 0.0032\n",
      "[0/5][86/95] Loss_D: 0.3615 Loss_G: 1.4252 D(x): 0.7956 D(G(z)): 0.0593 / 0.2647\n",
      "[0/5][87/95] Loss_D: 1.8302 Loss_G: 10.7281 D(x): 0.9634 D(G(z)): 0.8193 / 0.0000\n",
      "[0/5][88/95] Loss_D: 2.6464 Loss_G: 4.7460 D(x): 0.1375 D(G(z)): 0.0005 / 0.0146\n",
      "[0/5][89/95] Loss_D: 0.7315 Loss_G: 0.1350 D(x): 0.6118 D(G(z)): 0.1094 / 0.8765\n",
      "[0/5][90/95] Loss_D: 3.2472 Loss_G: 7.5939 D(x): 0.9962 D(G(z)): 0.9544 / 0.0011\n",
      "[0/5][91/95] Loss_D: 2.0170 Loss_G: 2.9155 D(x): 0.2191 D(G(z)): 0.0059 / 0.0800\n",
      "[0/5][92/95] Loss_D: 0.4740 Loss_G: 2.8021 D(x): 0.9391 D(G(z)): 0.3142 / 0.0839\n",
      "[0/5][93/95] Loss_D: 0.6089 Loss_G: 5.2342 D(x): 0.9104 D(G(z)): 0.3759 / 0.0092\n",
      "[0/5][94/95] Loss_D: 1.9045 Loss_G: 0.1359 D(x): 0.1955 D(G(z)): 0.0671 / 0.8739\n",
      "[1/5][0/95] Loss_D: 2.9762 Loss_G: 6.0710 D(x): 0.9921 D(G(z)): 0.9448 / 0.0059\n",
      "[1/5][1/95] Loss_D: 1.8539 Loss_G: 1.6605 D(x): 0.2393 D(G(z)): 0.0201 / 0.2461\n",
      "[1/5][2/95] Loss_D: 1.1801 Loss_G: 3.0217 D(x): 0.8364 D(G(z)): 0.5936 / 0.0730\n",
      "[1/5][3/95] Loss_D: 0.7444 Loss_G: 2.0935 D(x): 0.6448 D(G(z)): 0.2088 / 0.1449\n",
      "[1/5][4/95] Loss_D: 1.0668 Loss_G: 3.4422 D(x): 0.7113 D(G(z)): 0.4796 / 0.0391\n",
      "[1/5][5/95] Loss_D: 0.3097 Loss_G: 4.9934 D(x): 0.9038 D(G(z)): 0.1780 / 0.0100\n",
      "[1/5][6/95] Loss_D: 2.4971 Loss_G: 0.0152 D(x): 0.1280 D(G(z)): 0.0391 / 0.9852\n",
      "[1/5][7/95] Loss_D: 5.3798 Loss_G: 1.5826 D(x): 0.9911 D(G(z)): 0.9906 / 0.2636\n",
      "[1/5][8/95] Loss_D: 1.0401 Loss_G: 4.6126 D(x): 0.6872 D(G(z)): 0.4319 / 0.0131\n",
      "[1/5][9/95] Loss_D: 2.4977 Loss_G: 0.2861 D(x): 0.1151 D(G(z)): 0.0333 / 0.7865\n",
      "[1/5][10/95] Loss_D: 2.3564 Loss_G: 2.5207 D(x): 0.9624 D(G(z)): 0.8308 / 0.1060\n",
      "[1/5][11/95] Loss_D: 0.6896 Loss_G: 3.9417 D(x): 0.7523 D(G(z)): 0.2974 / 0.0321\n",
      "[1/5][12/95] Loss_D: 1.3715 Loss_G: 0.6892 D(x): 0.2993 D(G(z)): 0.0368 / 0.5560\n",
      "[1/5][13/95] Loss_D: 1.5333 Loss_G: 2.8004 D(x): 0.9514 D(G(z)): 0.7210 / 0.1033\n",
      "[1/5][14/95] Loss_D: 1.1892 Loss_G: 1.5728 D(x): 0.4641 D(G(z)): 0.2062 / 0.2487\n",
      "[1/5][15/95] Loss_D: 0.7012 Loss_G: 2.7876 D(x): 0.8545 D(G(z)): 0.3857 / 0.0792\n",
      "[1/5][16/95] Loss_D: 0.8415 Loss_G: 1.1457 D(x): 0.5549 D(G(z)): 0.1519 / 0.3530\n",
      "[1/5][17/95] Loss_D: 1.0054 Loss_G: 4.3037 D(x): 0.9171 D(G(z)): 0.5703 / 0.0223\n",
      "[1/5][18/95] Loss_D: 2.0368 Loss_G: 0.3298 D(x): 0.1744 D(G(z)): 0.0442 / 0.7333\n",
      "[1/5][19/95] Loss_D: 2.0720 Loss_G: 3.7322 D(x): 0.9727 D(G(z)): 0.8486 / 0.0323\n",
      "[1/5][20/95] Loss_D: 1.5668 Loss_G: 1.0057 D(x): 0.2882 D(G(z)): 0.0985 / 0.3842\n",
      "[1/5][21/95] Loss_D: 0.9394 Loss_G: 1.0178 D(x): 0.6752 D(G(z)): 0.3803 / 0.3796\n",
      "[1/5][22/95] Loss_D: 1.1973 Loss_G: 4.3222 D(x): 0.9517 D(G(z)): 0.6395 / 0.0166\n",
      "[1/5][23/95] Loss_D: 2.5704 Loss_G: 0.6285 D(x): 0.1056 D(G(z)): 0.0344 / 0.5610\n",
      "[1/5][24/95] Loss_D: 1.4202 Loss_G: 1.6336 D(x): 0.8439 D(G(z)): 0.6672 / 0.2246\n",
      "[1/5][25/95] Loss_D: 0.8540 Loss_G: 2.4932 D(x): 0.6899 D(G(z)): 0.3405 / 0.0978\n",
      "[1/5][26/95] Loss_D: 1.2391 Loss_G: 0.6914 D(x): 0.3981 D(G(z)): 0.1776 / 0.5276\n",
      "[1/5][27/95] Loss_D: 1.1972 Loss_G: 2.5959 D(x): 0.8386 D(G(z)): 0.5689 / 0.1037\n",
      "[1/5][28/95] Loss_D: 0.7620 Loss_G: 1.8099 D(x): 0.5926 D(G(z)): 0.1481 / 0.2099\n",
      "[1/5][29/95] Loss_D: 0.7800 Loss_G: 1.7501 D(x): 0.7732 D(G(z)): 0.3663 / 0.1967\n",
      "[1/5][30/95] Loss_D: 0.9901 Loss_G: 2.0865 D(x): 0.6652 D(G(z)): 0.3705 / 0.1404\n",
      "[1/5][31/95] Loss_D: 1.0644 Loss_G: 0.8875 D(x): 0.5182 D(G(z)): 0.2612 / 0.4225\n",
      "[1/5][32/95] Loss_D: 1.0350 Loss_G: 3.2068 D(x): 0.7894 D(G(z)): 0.5221 / 0.0531\n",
      "[1/5][33/95] Loss_D: 1.3557 Loss_G: 0.2193 D(x): 0.3387 D(G(z)): 0.1056 / 0.8052\n",
      "[1/5][34/95] Loss_D: 2.2117 Loss_G: 4.3564 D(x): 0.9360 D(G(z)): 0.8698 / 0.0246\n",
      "[1/5][35/95] Loss_D: 1.9797 Loss_G: 1.0255 D(x): 0.1833 D(G(z)): 0.0387 / 0.3828\n",
      "[1/5][36/95] Loss_D: 1.0871 Loss_G: 1.7417 D(x): 0.8401 D(G(z)): 0.5638 / 0.2084\n",
      "[1/5][37/95] Loss_D: 0.9497 Loss_G: 1.6998 D(x): 0.6020 D(G(z)): 0.3076 / 0.2088\n",
      "[1/5][38/95] Loss_D: 0.9367 Loss_G: 0.9898 D(x): 0.5900 D(G(z)): 0.2883 / 0.3862\n",
      "[1/5][39/95] Loss_D: 1.1419 Loss_G: 2.4112 D(x): 0.7276 D(G(z)): 0.5338 / 0.1096\n",
      "[1/5][40/95] Loss_D: 1.1998 Loss_G: 0.5000 D(x): 0.4141 D(G(z)): 0.1901 / 0.6144\n",
      "[1/5][41/95] Loss_D: 1.6150 Loss_G: 3.7057 D(x): 0.8899 D(G(z)): 0.7562 / 0.0377\n",
      "[1/5][42/95] Loss_D: 2.1142 Loss_G: 0.2635 D(x): 0.1555 D(G(z)): 0.0596 / 0.7768\n",
      "[1/5][43/95] Loss_D: 2.2974 Loss_G: 2.7891 D(x): 0.9129 D(G(z)): 0.8742 / 0.0815\n",
      "[1/5][44/95] Loss_D: 1.3480 Loss_G: 0.8601 D(x): 0.3250 D(G(z)): 0.1132 / 0.4343\n",
      "[1/5][45/95] Loss_D: 1.0557 Loss_G: 1.6008 D(x): 0.7745 D(G(z)): 0.5151 / 0.2153\n",
      "[1/5][46/95] Loss_D: 1.0695 Loss_G: 1.4667 D(x): 0.5695 D(G(z)): 0.3538 / 0.2410\n",
      "[1/5][47/95] Loss_D: 1.3389 Loss_G: 1.1903 D(x): 0.5026 D(G(z)): 0.4128 / 0.3157\n",
      "[1/5][48/95] Loss_D: 1.1052 Loss_G: 1.2279 D(x): 0.5811 D(G(z)): 0.3950 / 0.3049\n",
      "[1/5][49/95] Loss_D: 1.0420 Loss_G: 2.5065 D(x): 0.6761 D(G(z)): 0.4485 / 0.0988\n",
      "[1/5][50/95] Loss_D: 1.2905 Loss_G: 0.1391 D(x): 0.3559 D(G(z)): 0.1642 / 0.8737\n",
      "[1/5][51/95] Loss_D: 2.4767 Loss_G: 4.2441 D(x): 0.9391 D(G(z)): 0.9021 / 0.0327\n",
      "[1/5][52/95] Loss_D: 1.8313 Loss_G: 1.1114 D(x): 0.1923 D(G(z)): 0.0529 / 0.3691\n",
      "[1/5][53/95] Loss_D: 1.0751 Loss_G: 1.2882 D(x): 0.7670 D(G(z)): 0.5322 / 0.3022\n",
      "[1/5][54/95] Loss_D: 0.8767 Loss_G: 2.0354 D(x): 0.6875 D(G(z)): 0.3469 / 0.1665\n",
      "[1/5][55/95] Loss_D: 1.2945 Loss_G: 0.7720 D(x): 0.4798 D(G(z)): 0.3883 / 0.4803\n",
      "[1/5][56/95] Loss_D: 1.4963 Loss_G: 1.8928 D(x): 0.6481 D(G(z)): 0.6101 / 0.1654\n",
      "[1/5][57/95] Loss_D: 1.1272 Loss_G: 1.1117 D(x): 0.5165 D(G(z)): 0.3321 / 0.3722\n",
      "[1/5][58/95] Loss_D: 1.0457 Loss_G: 1.2286 D(x): 0.6078 D(G(z)): 0.3815 / 0.3424\n",
      "[1/5][59/95] Loss_D: 1.0064 Loss_G: 2.3141 D(x): 0.7178 D(G(z)): 0.4634 / 0.1210\n",
      "[1/5][60/95] Loss_D: 1.0067 Loss_G: 0.8115 D(x): 0.5111 D(G(z)): 0.2488 / 0.4765\n",
      "[1/5][61/95] Loss_D: 1.1396 Loss_G: 3.4515 D(x): 0.7908 D(G(z)): 0.5634 / 0.0400\n",
      "[1/5][62/95] Loss_D: 1.5154 Loss_G: 0.3249 D(x): 0.2621 D(G(z)): 0.0989 / 0.7308\n",
      "[1/5][63/95] Loss_D: 1.6739 Loss_G: 3.6859 D(x): 0.8520 D(G(z)): 0.7573 / 0.0319\n",
      "[1/5][64/95] Loss_D: 2.3086 Loss_G: 0.1508 D(x): 0.1354 D(G(z)): 0.1773 / 0.8625\n",
      "[1/5][65/95] Loss_D: 2.0034 Loss_G: 2.3242 D(x): 0.8639 D(G(z)): 0.8208 / 0.1061\n",
      "[1/5][66/95] Loss_D: 1.8516 Loss_G: 0.3564 D(x): 0.2128 D(G(z)): 0.1622 / 0.7086\n",
      "[1/5][67/95] Loss_D: 1.7950 Loss_G: 2.3149 D(x): 0.8796 D(G(z)): 0.7816 / 0.1188\n",
      "[1/5][68/95] Loss_D: 0.8657 Loss_G: 1.9485 D(x): 0.5116 D(G(z)): 0.1359 / 0.1731\n",
      "[1/5][69/95] Loss_D: 1.3543 Loss_G: 0.3178 D(x): 0.4180 D(G(z)): 0.3027 / 0.7350\n",
      "[1/5][70/95] Loss_D: 1.6063 Loss_G: 2.3618 D(x): 0.9021 D(G(z)): 0.7528 / 0.1065\n",
      "[1/5][71/95] Loss_D: 0.8636 Loss_G: 1.8754 D(x): 0.4945 D(G(z)): 0.0930 / 0.1630\n",
      "[1/5][72/95] Loss_D: 0.8146 Loss_G: 1.3108 D(x): 0.7023 D(G(z)): 0.3426 / 0.2928\n",
      "[1/5][73/95] Loss_D: 1.1042 Loss_G: 0.8783 D(x): 0.5617 D(G(z)): 0.3538 / 0.4298\n",
      "[1/5][74/95] Loss_D: 1.1522 Loss_G: 1.3859 D(x): 0.6566 D(G(z)): 0.4942 / 0.2591\n",
      "[1/5][75/95] Loss_D: 1.2808 Loss_G: 0.8873 D(x): 0.4904 D(G(z)): 0.4034 / 0.4330\n",
      "[1/5][76/95] Loss_D: 1.0255 Loss_G: 1.7570 D(x): 0.7213 D(G(z)): 0.4527 / 0.1836\n",
      "[1/5][77/95] Loss_D: 0.9642 Loss_G: 0.8545 D(x): 0.5453 D(G(z)): 0.2504 / 0.4428\n",
      "[1/5][78/95] Loss_D: 1.3028 Loss_G: 3.6330 D(x): 0.7950 D(G(z)): 0.6364 / 0.0353\n",
      "[1/5][79/95] Loss_D: 2.3403 Loss_G: 0.1276 D(x): 0.1212 D(G(z)): 0.0523 / 0.8833\n",
      "[1/5][80/95] Loss_D: 2.2366 Loss_G: 2.5482 D(x): 0.9463 D(G(z)): 0.8709 / 0.0944\n",
      "[1/5][81/95] Loss_D: 1.4755 Loss_G: 0.7375 D(x): 0.3054 D(G(z)): 0.1780 / 0.4917\n",
      "[1/5][82/95] Loss_D: 1.3420 Loss_G: 1.7992 D(x): 0.7271 D(G(z)): 0.6067 / 0.1783\n",
      "[1/5][83/95] Loss_D: 1.3513 Loss_G: 0.6278 D(x): 0.3773 D(G(z)): 0.2453 / 0.5581\n",
      "[1/5][84/95] Loss_D: 1.3388 Loss_G: 2.2681 D(x): 0.7534 D(G(z)): 0.6316 / 0.1172\n",
      "[1/5][85/95] Loss_D: 1.4999 Loss_G: 0.3553 D(x): 0.2906 D(G(z)): 0.1779 / 0.7079\n",
      "[1/5][86/95] Loss_D: 1.6903 Loss_G: 2.8661 D(x): 0.8824 D(G(z)): 0.7837 / 0.0711\n",
      "[1/5][87/95] Loss_D: 1.7332 Loss_G: 0.5863 D(x): 0.2168 D(G(z)): 0.1132 / 0.5650\n",
      "[1/5][88/95] Loss_D: 1.3265 Loss_G: 1.3911 D(x): 0.7886 D(G(z)): 0.6536 / 0.2743\n",
      "[1/5][89/95] Loss_D: 0.9943 Loss_G: 1.2683 D(x): 0.5530 D(G(z)): 0.2884 / 0.2969\n",
      "[1/5][90/95] Loss_D: 0.9326 Loss_G: 1.4580 D(x): 0.6728 D(G(z)): 0.3926 / 0.2455\n",
      "[1/5][91/95] Loss_D: 0.9196 Loss_G: 1.5903 D(x): 0.6431 D(G(z)): 0.3618 / 0.2148\n",
      "[1/5][92/95] Loss_D: 1.1778 Loss_G: 0.3571 D(x): 0.4211 D(G(z)): 0.2364 / 0.7056\n",
      "[1/5][93/95] Loss_D: 1.7882 Loss_G: 2.8971 D(x): 0.8573 D(G(z)): 0.7873 / 0.0632\n",
      "[1/5][94/95] Loss_D: 2.2534 Loss_G: 0.0998 D(x): 0.1140 D(G(z)): 0.0637 / 0.9056\n",
      "[2/5][0/95] Loss_D: 1.9265 Loss_G: 2.2270 D(x): 0.9502 D(G(z)): 0.8278 / 0.1338\n",
      "[2/5][1/95] Loss_D: 0.9704 Loss_G: 1.8724 D(x): 0.5604 D(G(z)): 0.2752 / 0.1762\n",
      "[2/5][2/95] Loss_D: 1.8343 Loss_G: 0.2394 D(x): 0.2498 D(G(z)): 0.2128 / 0.7923\n",
      "[2/5][3/95] Loss_D: 3.0063 Loss_G: 2.1514 D(x): 0.8705 D(G(z)): 0.9295 / 0.1310\n",
      "[2/5][4/95] Loss_D: 1.5887 Loss_G: 1.2052 D(x): 0.2616 D(G(z)): 0.1580 / 0.3148\n",
      "[2/5][5/95] Loss_D: 1.2204 Loss_G: 1.1214 D(x): 0.5977 D(G(z)): 0.4751 / 0.3324\n",
      "[2/5][6/95] Loss_D: 1.0408 Loss_G: 1.5748 D(x): 0.6137 D(G(z)): 0.4008 / 0.2155\n",
      "[2/5][7/95] Loss_D: 0.7620 Loss_G: 2.2703 D(x): 0.7189 D(G(z)): 0.3322 / 0.1181\n",
      "[2/5][8/95] Loss_D: 1.2574 Loss_G: 0.5427 D(x): 0.3838 D(G(z)): 0.1998 / 0.5842\n",
      "[2/5][9/95] Loss_D: 1.3979 Loss_G: 2.0831 D(x): 0.7810 D(G(z)): 0.6693 / 0.1472\n",
      "[2/5][10/95] Loss_D: 0.9160 Loss_G: 1.6240 D(x): 0.5382 D(G(z)): 0.2396 / 0.2111\n",
      "[2/5][11/95] Loss_D: 1.6504 Loss_G: 0.3882 D(x): 0.3416 D(G(z)): 0.3945 / 0.6803\n",
      "[2/5][12/95] Loss_D: 1.6362 Loss_G: 3.1029 D(x): 0.7888 D(G(z)): 0.7455 / 0.1032\n",
      "[2/5][13/95] Loss_D: 1.5229 Loss_G: 1.2322 D(x): 0.3439 D(G(z)): 0.1660 / 0.3888\n",
      "[2/5][14/95] Loss_D: 0.9640 Loss_G: 1.0970 D(x): 0.7315 D(G(z)): 0.4291 / 0.3666\n",
      "[2/5][15/95] Loss_D: 0.9642 Loss_G: 2.4687 D(x): 0.8217 D(G(z)): 0.5123 / 0.1088\n",
      "[2/5][16/95] Loss_D: 1.2009 Loss_G: 0.5486 D(x): 0.3995 D(G(z)): 0.1767 / 0.5858\n",
      "[2/5][17/95] Loss_D: 1.4245 Loss_G: 2.7541 D(x): 0.8747 D(G(z)): 0.7030 / 0.0764\n",
      "[2/5][18/95] Loss_D: 1.2476 Loss_G: 0.8653 D(x): 0.3864 D(G(z)): 0.1398 / 0.4315\n",
      "[2/5][19/95] Loss_D: 1.0574 Loss_G: 2.6474 D(x): 0.8698 D(G(z)): 0.5856 / 0.0806\n",
      "[2/5][20/95] Loss_D: 1.4075 Loss_G: 0.4711 D(x): 0.3347 D(G(z)): 0.1227 / 0.6334\n",
      "[2/5][21/95] Loss_D: 1.5127 Loss_G: 2.4549 D(x): 0.8562 D(G(z)): 0.7186 / 0.0951\n",
      "[2/5][22/95] Loss_D: 1.2361 Loss_G: 0.9801 D(x): 0.4124 D(G(z)): 0.1551 / 0.3875\n",
      "[2/5][23/95] Loss_D: 1.2275 Loss_G: 1.9251 D(x): 0.7849 D(G(z)): 0.5980 / 0.1559\n",
      "[2/5][24/95] Loss_D: 1.1707 Loss_G: 1.0074 D(x): 0.4608 D(G(z)): 0.2307 / 0.3825\n",
      "[2/5][25/95] Loss_D: 1.0602 Loss_G: 1.7384 D(x): 0.7256 D(G(z)): 0.4813 / 0.1903\n",
      "[2/5][26/95] Loss_D: 0.7200 Loss_G: 3.1620 D(x): 0.8204 D(G(z)): 0.3855 / 0.0547\n",
      "[2/5][27/95] Loss_D: 2.5222 Loss_G: 0.2064 D(x): 0.1113 D(G(z)): 0.0875 / 0.8163\n",
      "[2/5][28/95] Loss_D: 2.2499 Loss_G: 1.5097 D(x): 0.9178 D(G(z)): 0.8688 / 0.2374\n",
      "[2/5][29/95] Loss_D: 1.3839 Loss_G: 1.4969 D(x): 0.4380 D(G(z)): 0.4009 / 0.2417\n",
      "[2/5][30/95] Loss_D: 1.2184 Loss_G: 0.6014 D(x): 0.4158 D(G(z)): 0.2475 / 0.5689\n",
      "[2/5][31/95] Loss_D: 1.2723 Loss_G: 1.7296 D(x): 0.7951 D(G(z)): 0.6207 / 0.1919\n",
      "[2/5][32/95] Loss_D: 1.0387 Loss_G: 1.7679 D(x): 0.5808 D(G(z)): 0.3254 / 0.2010\n",
      "[2/5][33/95] Loss_D: 0.9274 Loss_G: 1.0948 D(x): 0.5720 D(G(z)): 0.2532 / 0.3756\n",
      "[2/5][34/95] Loss_D: 0.9947 Loss_G: 1.5970 D(x): 0.7364 D(G(z)): 0.4487 / 0.2310\n",
      "[2/5][35/95] Loss_D: 1.0449 Loss_G: 1.8591 D(x): 0.6530 D(G(z)): 0.4004 / 0.1795\n",
      "[2/5][36/95] Loss_D: 1.3378 Loss_G: 0.9790 D(x): 0.4490 D(G(z)): 0.3436 / 0.3978\n",
      "[2/5][37/95] Loss_D: 1.3922 Loss_G: 0.9661 D(x): 0.5285 D(G(z)): 0.4927 / 0.4161\n",
      "[2/5][38/95] Loss_D: 1.3567 Loss_G: 2.1235 D(x): 0.6254 D(G(z)): 0.5611 / 0.1392\n",
      "[2/5][39/95] Loss_D: 1.7133 Loss_G: 0.1790 D(x): 0.2516 D(G(z)): 0.2067 / 0.8375\n",
      "[2/5][40/95] Loss_D: 2.4045 Loss_G: 3.3518 D(x): 0.8873 D(G(z)): 0.8912 / 0.0406\n",
      "[2/5][41/95] Loss_D: 2.1504 Loss_G: 0.5953 D(x): 0.1390 D(G(z)): 0.0614 / 0.5648\n",
      "[2/5][42/95] Loss_D: 1.3456 Loss_G: 1.2246 D(x): 0.7550 D(G(z)): 0.6439 / 0.2993\n",
      "[2/5][43/95] Loss_D: 1.1086 Loss_G: 1.3177 D(x): 0.5353 D(G(z)): 0.3659 / 0.2720\n",
      "[2/5][44/95] Loss_D: 0.8290 Loss_G: 1.5773 D(x): 0.6583 D(G(z)): 0.3246 / 0.2148\n",
      "[2/5][45/95] Loss_D: 1.1793 Loss_G: 0.6110 D(x): 0.4656 D(G(z)): 0.3053 / 0.5470\n",
      "[2/5][46/95] Loss_D: 1.2731 Loss_G: 2.1107 D(x): 0.7681 D(G(z)): 0.6279 / 0.1386\n",
      "[2/5][47/95] Loss_D: 1.3455 Loss_G: 0.6486 D(x): 0.3605 D(G(z)): 0.1930 / 0.5299\n",
      "[2/5][48/95] Loss_D: 1.6001 Loss_G: 1.6119 D(x): 0.7620 D(G(z)): 0.7230 / 0.2182\n",
      "[2/5][49/95] Loss_D: 0.6036 Loss_G: 2.5943 D(x): 0.7155 D(G(z)): 0.2220 / 0.0863\n",
      "[2/5][50/95] Loss_D: 1.6784 Loss_G: 0.2751 D(x): 0.2421 D(G(z)): 0.1406 / 0.7626\n",
      "[2/5][51/95] Loss_D: 2.0245 Loss_G: 1.3598 D(x): 0.8597 D(G(z)): 0.8337 / 0.2798\n",
      "[2/5][52/95] Loss_D: 1.2222 Loss_G: 1.5109 D(x): 0.4823 D(G(z)): 0.3603 / 0.2359\n",
      "[2/5][53/95] Loss_D: 1.3357 Loss_G: 0.5505 D(x): 0.3747 D(G(z)): 0.2643 / 0.5875\n",
      "[2/5][54/95] Loss_D: 1.3758 Loss_G: 1.7322 D(x): 0.7728 D(G(z)): 0.6579 / 0.1873\n",
      "[2/5][55/95] Loss_D: 0.8595 Loss_G: 1.9815 D(x): 0.6159 D(G(z)): 0.2812 / 0.1529\n",
      "[2/5][56/95] Loss_D: 1.0400 Loss_G: 0.8153 D(x): 0.4620 D(G(z)): 0.1671 / 0.4576\n",
      "[2/5][57/95] Loss_D: 1.2165 Loss_G: 1.3848 D(x): 0.7313 D(G(z)): 0.5706 / 0.2754\n",
      "[2/5][58/95] Loss_D: 1.0336 Loss_G: 1.5933 D(x): 0.6012 D(G(z)): 0.3811 / 0.2372\n",
      "[2/5][59/95] Loss_D: 1.2418 Loss_G: 0.9262 D(x): 0.5107 D(G(z)): 0.3911 / 0.4214\n",
      "[2/5][60/95] Loss_D: 1.0275 Loss_G: 1.7751 D(x): 0.7163 D(G(z)): 0.4783 / 0.1958\n",
      "[2/5][61/95] Loss_D: 1.1345 Loss_G: 0.6636 D(x): 0.4583 D(G(z)): 0.2641 / 0.5311\n",
      "[2/5][62/95] Loss_D: 1.0777 Loss_G: 2.1592 D(x): 0.8581 D(G(z)): 0.5855 / 0.1261\n",
      "[2/5][63/95] Loss_D: 1.3133 Loss_G: 0.6434 D(x): 0.3818 D(G(z)): 0.2452 / 0.5345\n",
      "[2/5][64/95] Loss_D: 1.5353 Loss_G: 1.3895 D(x): 0.5613 D(G(z)): 0.5888 / 0.2621\n",
      "[2/5][65/95] Loss_D: 1.3470 Loss_G: 1.5167 D(x): 0.5023 D(G(z)): 0.4635 / 0.2284\n",
      "[2/5][66/95] Loss_D: 1.5529 Loss_G: 0.6365 D(x): 0.3698 D(G(z)): 0.3983 / 0.5327\n",
      "[2/5][67/95] Loss_D: 1.3303 Loss_G: 1.7091 D(x): 0.6518 D(G(z)): 0.5802 / 0.1935\n",
      "[2/5][68/95] Loss_D: 1.4152 Loss_G: 0.5283 D(x): 0.3538 D(G(z)): 0.2902 / 0.5932\n",
      "[2/5][69/95] Loss_D: 1.2990 Loss_G: 1.4610 D(x): 0.6861 D(G(z)): 0.5893 / 0.2411\n",
      "[2/5][70/95] Loss_D: 1.1412 Loss_G: 0.9587 D(x): 0.4983 D(G(z)): 0.3317 / 0.3912\n",
      "[2/5][71/95] Loss_D: 1.1283 Loss_G: 1.2834 D(x): 0.6212 D(G(z)): 0.4625 / 0.2922\n",
      "[2/5][72/95] Loss_D: 1.1250 Loss_G: 0.7466 D(x): 0.5115 D(G(z)): 0.3364 / 0.4826\n",
      "[2/5][73/95] Loss_D: 1.1352 Loss_G: 2.8276 D(x): 0.8093 D(G(z)): 0.5852 / 0.0814\n",
      "[2/5][74/95] Loss_D: 1.5083 Loss_G: 0.4717 D(x): 0.2757 D(G(z)): 0.1262 / 0.6287\n",
      "[2/5][75/95] Loss_D: 1.3883 Loss_G: 1.6423 D(x): 0.7989 D(G(z)): 0.6786 / 0.2094\n",
      "[2/5][76/95] Loss_D: 1.4254 Loss_G: 0.6655 D(x): 0.3925 D(G(z)): 0.3245 / 0.5268\n",
      "[2/5][77/95] Loss_D: 1.2998 Loss_G: 2.2134 D(x): 0.7887 D(G(z)): 0.6415 / 0.1286\n",
      "[2/5][78/95] Loss_D: 1.7860 Loss_G: 0.3117 D(x): 0.2213 D(G(z)): 0.1403 / 0.7470\n",
      "[2/5][79/95] Loss_D: 2.0347 Loss_G: 1.6609 D(x): 0.8902 D(G(z)): 0.8361 / 0.2099\n",
      "[2/5][80/95] Loss_D: 0.8601 Loss_G: 1.9796 D(x): 0.5681 D(G(z)): 0.2276 / 0.1536\n",
      "[2/5][81/95] Loss_D: 1.6886 Loss_G: 0.2055 D(x): 0.2547 D(G(z)): 0.2136 / 0.8170\n",
      "[2/5][82/95] Loss_D: 1.9383 Loss_G: 1.4907 D(x): 0.9226 D(G(z)): 0.8295 / 0.2410\n",
      "[2/5][83/95] Loss_D: 1.1073 Loss_G: 1.6420 D(x): 0.5033 D(G(z)): 0.3120 / 0.2149\n",
      "[2/5][84/95] Loss_D: 1.4750 Loss_G: 0.5062 D(x): 0.3720 D(G(z)): 0.3083 / 0.6103\n",
      "[2/5][85/95] Loss_D: 1.3750 Loss_G: 1.3231 D(x): 0.7316 D(G(z)): 0.6390 / 0.2782\n",
      "[2/5][86/95] Loss_D: 1.3324 Loss_G: 1.2583 D(x): 0.4691 D(G(z)): 0.3928 / 0.2970\n",
      "[2/5][87/95] Loss_D: 1.4064 Loss_G: 0.6419 D(x): 0.4329 D(G(z)): 0.3998 / 0.5397\n",
      "[2/5][88/95] Loss_D: 1.2796 Loss_G: 2.0896 D(x): 0.7675 D(G(z)): 0.6239 / 0.1386\n",
      "[2/5][89/95] Loss_D: 1.3706 Loss_G: 0.4716 D(x): 0.3356 D(G(z)): 0.1958 / 0.6389\n",
      "[2/5][90/95] Loss_D: 1.2521 Loss_G: 1.8367 D(x): 0.7894 D(G(z)): 0.6171 / 0.1736\n",
      "[2/5][91/95] Loss_D: 1.2002 Loss_G: 1.2019 D(x): 0.4848 D(G(z)): 0.3337 / 0.3124\n",
      "[2/5][92/95] Loss_D: 1.5983 Loss_G: 0.5501 D(x): 0.3766 D(G(z)): 0.4124 / 0.5835\n",
      "[2/5][93/95] Loss_D: 1.8215 Loss_G: 3.2027 D(x): 0.7402 D(G(z)): 0.7670 / 0.0512\n",
      "[2/5][94/95] Loss_D: 2.4811 Loss_G: 0.2946 D(x): 0.1006 D(G(z)): 0.0601 / 0.7542\n",
      "[3/5][0/95] Loss_D: 2.3576 Loss_G: 1.7053 D(x): 0.8859 D(G(z)): 0.8826 / 0.2391\n",
      "[3/5][1/95] Loss_D: 1.3828 Loss_G: 1.5404 D(x): 0.4228 D(G(z)): 0.3281 / 0.2464\n",
      "[3/5][2/95] Loss_D: 1.3881 Loss_G: 0.5858 D(x): 0.4042 D(G(z)): 0.3317 / 0.5757\n",
      "[3/5][3/95] Loss_D: 1.5844 Loss_G: 2.0461 D(x): 0.7313 D(G(z)): 0.6921 / 0.1397\n",
      "[3/5][4/95] Loss_D: 1.3614 Loss_G: 1.0179 D(x): 0.3428 D(G(z)): 0.2092 / 0.3728\n",
      "[3/5][5/95] Loss_D: 1.1475 Loss_G: 1.1845 D(x): 0.6308 D(G(z)): 0.4723 / 0.3169\n",
      "[3/5][6/95] Loss_D: 1.1582 Loss_G: 1.4022 D(x): 0.5766 D(G(z)): 0.4302 / 0.2527\n",
      "[3/5][7/95] Loss_D: 0.8752 Loss_G: 1.9560 D(x): 0.6846 D(G(z)): 0.3767 / 0.1481\n",
      "[3/5][8/95] Loss_D: 1.3170 Loss_G: 0.7756 D(x): 0.4227 D(G(z)): 0.3432 / 0.4680\n",
      "[3/5][9/95] Loss_D: 1.6845 Loss_G: 1.7998 D(x): 0.6569 D(G(z)): 0.7088 / 0.2013\n",
      "[3/5][10/95] Loss_D: 0.8510 Loss_G: 2.3746 D(x): 0.6774 D(G(z)): 0.3178 / 0.1159\n",
      "[3/5][11/95] Loss_D: 2.5178 Loss_G: 0.2183 D(x): 0.1482 D(G(z)): 0.1747 / 0.8053\n",
      "[3/5][12/95] Loss_D: 2.1025 Loss_G: 1.7419 D(x): 0.9578 D(G(z)): 0.8657 / 0.2172\n",
      "[3/5][13/95] Loss_D: 1.7775 Loss_G: 1.0185 D(x): 0.3189 D(G(z)): 0.3197 / 0.3891\n",
      "[3/5][14/95] Loss_D: 1.0373 Loss_G: 1.1799 D(x): 0.7082 D(G(z)): 0.4746 / 0.3209\n",
      "[3/5][15/95] Loss_D: 1.3729 Loss_G: 1.5123 D(x): 0.6039 D(G(z)): 0.5512 / 0.2340\n",
      "[3/5][16/95] Loss_D: 1.5414 Loss_G: 0.7689 D(x): 0.4048 D(G(z)): 0.4121 / 0.4812\n",
      "[3/5][17/95] Loss_D: 1.7419 Loss_G: 1.2041 D(x): 0.4914 D(G(z)): 0.5974 / 0.3110\n",
      "[3/5][18/95] Loss_D: 1.3255 Loss_G: 1.6870 D(x): 0.5243 D(G(z)): 0.4571 / 0.1926\n",
      "[3/5][19/95] Loss_D: 1.3385 Loss_G: 0.7521 D(x): 0.4054 D(G(z)): 0.3027 / 0.4802\n",
      "[3/5][20/95] Loss_D: 1.2089 Loss_G: 1.5132 D(x): 0.7093 D(G(z)): 0.5632 / 0.2252\n",
      "[3/5][21/95] Loss_D: 1.1766 Loss_G: 0.8433 D(x): 0.4589 D(G(z)): 0.3077 / 0.4408\n",
      "[3/5][22/95] Loss_D: 1.3229 Loss_G: 1.1106 D(x): 0.6271 D(G(z)): 0.5536 / 0.3435\n",
      "[3/5][23/95] Loss_D: 1.2392 Loss_G: 1.2221 D(x): 0.5636 D(G(z)): 0.4662 / 0.3109\n",
      "[3/5][24/95] Loss_D: 1.3129 Loss_G: 0.6698 D(x): 0.4683 D(G(z)): 0.3944 / 0.5197\n",
      "[3/5][25/95] Loss_D: 1.3947 Loss_G: 1.6389 D(x): 0.6641 D(G(z)): 0.6153 / 0.2144\n",
      "[3/5][26/95] Loss_D: 1.3960 Loss_G: 0.5036 D(x): 0.3771 D(G(z)): 0.2918 / 0.6107\n",
      "[3/5][27/95] Loss_D: 1.6387 Loss_G: 2.5614 D(x): 0.7847 D(G(z)): 0.7369 / 0.0929\n",
      "[3/5][28/95] Loss_D: 2.0940 Loss_G: 0.3598 D(x): 0.1781 D(G(z)): 0.1368 / 0.7006\n",
      "[3/5][29/95] Loss_D: 1.6965 Loss_G: 1.7397 D(x): 0.8138 D(G(z)): 0.7677 / 0.1835\n",
      "[3/5][30/95] Loss_D: 1.7524 Loss_G: 0.9803 D(x): 0.2646 D(G(z)): 0.2969 / 0.3819\n",
      "[3/5][31/95] Loss_D: 1.2949 Loss_G: 0.8675 D(x): 0.5261 D(G(z)): 0.4612 / 0.4266\n",
      "[3/5][32/95] Loss_D: 1.2118 Loss_G: 1.2841 D(x): 0.5682 D(G(z)): 0.4577 / 0.2866\n",
      "[3/5][33/95] Loss_D: 1.0326 Loss_G: 1.5244 D(x): 0.5853 D(G(z)): 0.3738 / 0.2257\n",
      "[3/5][34/95] Loss_D: 1.1420 Loss_G: 1.1239 D(x): 0.5409 D(G(z)): 0.3900 / 0.3322\n",
      "[3/5][35/95] Loss_D: 1.5817 Loss_G: 0.4842 D(x): 0.3973 D(G(z)): 0.4508 / 0.6186\n",
      "[3/5][36/95] Loss_D: 1.4607 Loss_G: 2.1136 D(x): 0.7826 D(G(z)): 0.6977 / 0.1416\n",
      "[3/5][37/95] Loss_D: 1.7002 Loss_G: 0.5680 D(x): 0.2885 D(G(z)): 0.2581 / 0.5717\n",
      "[3/5][38/95] Loss_D: 1.2263 Loss_G: 1.1337 D(x): 0.6966 D(G(z)): 0.5683 / 0.3368\n",
      "[3/5][39/95] Loss_D: 1.4903 Loss_G: 0.6172 D(x): 0.4109 D(G(z)): 0.4157 / 0.5456\n",
      "[3/5][40/95] Loss_D: 1.5702 Loss_G: 0.9932 D(x): 0.6157 D(G(z)): 0.6518 / 0.3840\n",
      "[3/5][41/95] Loss_D: 1.4520 Loss_G: 0.9978 D(x): 0.5117 D(G(z)): 0.5178 / 0.3764\n",
      "[3/5][42/95] Loss_D: 1.2269 Loss_G: 0.8686 D(x): 0.4921 D(G(z)): 0.3880 / 0.4275\n",
      "[3/5][43/95] Loss_D: 1.2733 Loss_G: 1.0608 D(x): 0.5947 D(G(z)): 0.5107 / 0.3562\n",
      "[3/5][44/95] Loss_D: 1.3229 Loss_G: 1.1335 D(x): 0.5614 D(G(z)): 0.5037 / 0.3312\n",
      "[3/5][45/95] Loss_D: 1.2379 Loss_G: 1.1371 D(x): 0.5279 D(G(z)): 0.4249 / 0.3266\n",
      "[3/5][46/95] Loss_D: 1.2108 Loss_G: 0.8837 D(x): 0.5150 D(G(z)): 0.4009 / 0.4272\n",
      "[3/5][47/95] Loss_D: 1.2061 Loss_G: 1.3581 D(x): 0.6168 D(G(z)): 0.4877 / 0.2671\n",
      "[3/5][48/95] Loss_D: 1.0797 Loss_G: 0.9345 D(x): 0.5062 D(G(z)): 0.3015 / 0.4127\n",
      "[3/5][49/95] Loss_D: 1.1561 Loss_G: 1.4817 D(x): 0.6917 D(G(z)): 0.5279 / 0.2439\n",
      "[3/5][50/95] Loss_D: 1.2462 Loss_G: 0.6928 D(x): 0.4399 D(G(z)): 0.3137 / 0.5124\n",
      "[3/5][51/95] Loss_D: 1.3728 Loss_G: 1.6974 D(x): 0.7101 D(G(z)): 0.6248 / 0.1934\n",
      "[3/5][52/95] Loss_D: 0.9862 Loss_G: 1.8310 D(x): 0.5838 D(G(z)): 0.3180 / 0.1747\n",
      "[3/5][53/95] Loss_D: 2.0071 Loss_G: 0.1753 D(x): 0.1936 D(G(z)): 0.2282 / 0.8434\n",
      "[3/5][54/95] Loss_D: 2.3812 Loss_G: 1.9335 D(x): 0.9306 D(G(z)): 0.8893 / 0.1561\n",
      "[3/5][55/95] Loss_D: 2.1069 Loss_G: 0.6275 D(x): 0.1745 D(G(z)): 0.2043 / 0.5418\n",
      "[3/5][56/95] Loss_D: 1.3213 Loss_G: 1.0198 D(x): 0.6220 D(G(z)): 0.5606 / 0.3733\n",
      "[3/5][57/95] Loss_D: 0.7335 Loss_G: 2.8556 D(x): 0.9011 D(G(z)): 0.4566 / 0.0777\n",
      "[3/5][58/95] Loss_D: 2.2334 Loss_G: 0.4727 D(x): 0.1664 D(G(z)): 0.1096 / 0.6274\n",
      "[3/5][59/95] Loss_D: 1.3762 Loss_G: 0.8526 D(x): 0.7445 D(G(z)): 0.6518 / 0.4355\n",
      "[3/5][60/95] Loss_D: 1.4030 Loss_G: 1.1983 D(x): 0.5924 D(G(z)): 0.5669 / 0.3141\n",
      "[3/5][61/95] Loss_D: 1.0870 Loss_G: 1.2530 D(x): 0.5433 D(G(z)): 0.3573 / 0.2984\n",
      "[3/5][62/95] Loss_D: 1.4530 Loss_G: 0.6430 D(x): 0.4178 D(G(z)): 0.4047 / 0.5307\n",
      "[3/5][63/95] Loss_D: 1.2902 Loss_G: 0.8350 D(x): 0.5746 D(G(z)): 0.4983 / 0.4450\n",
      "[3/5][64/95] Loss_D: 1.4674 Loss_G: 0.9019 D(x): 0.5149 D(G(z)): 0.5308 / 0.4134\n",
      "[3/5][65/95] Loss_D: 1.3535 Loss_G: 1.0531 D(x): 0.5326 D(G(z)): 0.4882 / 0.3583\n",
      "[3/5][66/95] Loss_D: 1.5179 Loss_G: 0.5633 D(x): 0.3805 D(G(z)): 0.3960 / 0.5788\n",
      "[3/5][67/95] Loss_D: 1.4497 Loss_G: 1.1573 D(x): 0.6923 D(G(z)): 0.6435 / 0.3197\n",
      "[3/5][68/95] Loss_D: 1.4553 Loss_G: 0.8094 D(x): 0.3959 D(G(z)): 0.3860 / 0.4485\n",
      "[3/5][69/95] Loss_D: 1.3306 Loss_G: 0.7697 D(x): 0.5306 D(G(z)): 0.4846 / 0.4673\n",
      "[3/5][70/95] Loss_D: 1.3692 Loss_G: 0.9402 D(x): 0.5689 D(G(z)): 0.5397 / 0.3959\n",
      "[3/5][71/95] Loss_D: 1.2340 Loss_G: 0.9384 D(x): 0.5155 D(G(z)): 0.4147 / 0.4024\n",
      "[3/5][72/95] Loss_D: 1.1378 Loss_G: 1.0889 D(x): 0.5950 D(G(z)): 0.4434 / 0.3563\n",
      "[3/5][73/95] Loss_D: 1.0961 Loss_G: 0.9299 D(x): 0.5392 D(G(z)): 0.3601 / 0.4054\n",
      "[3/5][74/95] Loss_D: 1.1843 Loss_G: 0.9931 D(x): 0.5890 D(G(z)): 0.4605 / 0.3851\n",
      "[3/5][75/95] Loss_D: 1.3827 Loss_G: 1.2855 D(x): 0.6085 D(G(z)): 0.5675 / 0.2966\n",
      "[3/5][76/95] Loss_D: 1.1330 Loss_G: 0.8587 D(x): 0.4960 D(G(z)): 0.3175 / 0.4535\n",
      "[3/5][77/95] Loss_D: 1.5975 Loss_G: 1.3746 D(x): 0.5949 D(G(z)): 0.6335 / 0.2631\n",
      "[3/5][78/95] Loss_D: 1.0836 Loss_G: 1.1149 D(x): 0.4932 D(G(z)): 0.2961 / 0.3515\n",
      "[3/5][79/95] Loss_D: 1.5029 Loss_G: 0.5023 D(x): 0.4226 D(G(z)): 0.4001 / 0.6133\n",
      "[3/5][80/95] Loss_D: 1.4932 Loss_G: 1.5622 D(x): 0.7013 D(G(z)): 0.6650 / 0.2223\n",
      "[3/5][81/95] Loss_D: 1.6136 Loss_G: 0.4994 D(x): 0.3002 D(G(z)): 0.2920 / 0.6099\n",
      "[3/5][82/95] Loss_D: 1.5190 Loss_G: 1.1838 D(x): 0.6418 D(G(z)): 0.6511 / 0.3120\n",
      "[3/5][83/95] Loss_D: 1.3544 Loss_G: 0.8283 D(x): 0.4319 D(G(z)): 0.3802 / 0.4414\n",
      "[3/5][84/95] Loss_D: 1.2542 Loss_G: 0.8892 D(x): 0.5386 D(G(z)): 0.4547 / 0.4168\n",
      "[3/5][85/95] Loss_D: 1.3897 Loss_G: 1.2834 D(x): 0.5992 D(G(z)): 0.5733 / 0.2877\n",
      "[3/5][86/95] Loss_D: 1.5129 Loss_G: 0.3837 D(x): 0.3215 D(G(z)): 0.2640 / 0.6840\n",
      "[3/5][87/95] Loss_D: 1.6371 Loss_G: 1.5690 D(x): 0.7536 D(G(z)): 0.7239 / 0.2241\n",
      "[3/5][88/95] Loss_D: 1.3889 Loss_G: 0.8344 D(x): 0.3746 D(G(z)): 0.2737 / 0.4575\n",
      "[3/5][89/95] Loss_D: 1.3136 Loss_G: 1.1438 D(x): 0.6612 D(G(z)): 0.5729 / 0.3423\n",
      "[3/5][90/95] Loss_D: 1.0471 Loss_G: 1.8298 D(x): 0.6503 D(G(z)): 0.4380 / 0.1871\n",
      "[3/5][91/95] Loss_D: 1.8642 Loss_G: 0.3491 D(x): 0.2517 D(G(z)): 0.3041 / 0.7088\n",
      "[3/5][92/95] Loss_D: 1.6213 Loss_G: 1.1157 D(x): 0.7609 D(G(z)): 0.7286 / 0.3337\n",
      "[3/5][93/95] Loss_D: 1.4820 Loss_G: 0.9367 D(x): 0.3877 D(G(z)): 0.3752 / 0.3981\n",
      "[3/5][94/95] Loss_D: 1.5897 Loss_G: 0.2927 D(x): 0.3522 D(G(z)): 0.4026 / 0.7466\n",
      "[4/5][0/95] Loss_D: 1.8440 Loss_G: 1.4260 D(x): 0.8580 D(G(z)): 0.7930 / 0.2591\n",
      "[4/5][1/95] Loss_D: 1.4809 Loss_G: 0.9359 D(x): 0.3525 D(G(z)): 0.3111 / 0.4051\n",
      "[4/5][2/95] Loss_D: 1.1907 Loss_G: 0.9277 D(x): 0.5744 D(G(z)): 0.4400 / 0.4052\n",
      "[4/5][3/95] Loss_D: 1.3128 Loss_G: 1.2602 D(x): 0.5932 D(G(z)): 0.5021 / 0.2976\n",
      "[4/5][4/95] Loss_D: 1.2072 Loss_G: 0.8302 D(x): 0.4703 D(G(z)): 0.3347 / 0.4498\n",
      "[4/5][5/95] Loss_D: 1.3354 Loss_G: 1.0982 D(x): 0.6318 D(G(z)): 0.5662 / 0.3459\n",
      "[4/5][6/95] Loss_D: 1.2342 Loss_G: 0.9287 D(x): 0.4902 D(G(z)): 0.3780 / 0.4050\n",
      "[4/5][7/95] Loss_D: 1.2880 Loss_G: 0.8889 D(x): 0.5778 D(G(z)): 0.5011 / 0.4183\n",
      "[4/5][8/95] Loss_D: 1.2378 Loss_G: 1.2081 D(x): 0.5818 D(G(z)): 0.4829 / 0.3053\n",
      "[4/5][9/95] Loss_D: 1.2442 Loss_G: 1.6961 D(x): 0.5496 D(G(z)): 0.4616 / 0.1955\n",
      "[4/5][10/95] Loss_D: 1.8925 Loss_G: 0.3247 D(x): 0.2269 D(G(z)): 0.2761 / 0.7305\n",
      "[4/5][11/95] Loss_D: 1.7264 Loss_G: 0.9459 D(x): 0.7161 D(G(z)): 0.7240 / 0.4053\n",
      "[4/5][12/95] Loss_D: 1.4053 Loss_G: 1.0394 D(x): 0.4557 D(G(z)): 0.4312 / 0.3575\n",
      "[4/5][13/95] Loss_D: 1.5436 Loss_G: 0.6875 D(x): 0.4097 D(G(z)): 0.4627 / 0.5079\n",
      "[4/5][14/95] Loss_D: 1.4348 Loss_G: 0.8535 D(x): 0.5055 D(G(z)): 0.5149 / 0.4349\n",
      "[4/5][15/95] Loss_D: 1.2448 Loss_G: 1.1088 D(x): 0.5772 D(G(z)): 0.4829 / 0.3442\n",
      "[4/5][16/95] Loss_D: 1.1215 Loss_G: 0.8909 D(x): 0.5223 D(G(z)): 0.3586 / 0.4180\n",
      "[4/5][17/95] Loss_D: 1.2572 Loss_G: 1.1656 D(x): 0.6412 D(G(z)): 0.5413 / 0.3284\n",
      "[4/5][18/95] Loss_D: 1.4181 Loss_G: 0.8908 D(x): 0.4427 D(G(z)): 0.4101 / 0.4202\n",
      "[4/5][19/95] Loss_D: 1.0001 Loss_G: 1.8931 D(x): 0.7314 D(G(z)): 0.4818 / 0.1695\n",
      "[4/5][20/95] Loss_D: 1.5200 Loss_G: 0.5927 D(x): 0.3235 D(G(z)): 0.2554 / 0.5601\n",
      "[4/5][21/95] Loss_D: 1.5750 Loss_G: 0.8380 D(x): 0.6294 D(G(z)): 0.6578 / 0.4380\n",
      "[4/5][22/95] Loss_D: 1.3685 Loss_G: 0.9894 D(x): 0.4596 D(G(z)): 0.4292 / 0.3774\n",
      "[4/5][23/95] Loss_D: 1.4391 Loss_G: 1.1388 D(x): 0.5692 D(G(z)): 0.5660 / 0.3291\n",
      "[4/5][24/95] Loss_D: 1.4210 Loss_G: 0.6092 D(x): 0.3842 D(G(z)): 0.3518 / 0.5479\n",
      "[4/5][25/95] Loss_D: 1.7665 Loss_G: 0.7569 D(x): 0.5846 D(G(z)): 0.6972 / 0.4847\n",
      "[4/5][26/95] Loss_D: 1.4014 Loss_G: 0.6357 D(x): 0.4294 D(G(z)): 0.4149 / 0.5407\n",
      "[4/5][27/95] Loss_D: 1.3867 Loss_G: 1.0094 D(x): 0.6530 D(G(z)): 0.5975 / 0.3829\n",
      "[4/5][28/95] Loss_D: 0.9828 Loss_G: 1.2077 D(x): 0.5966 D(G(z)): 0.3555 / 0.3071\n",
      "[4/5][29/95] Loss_D: 1.0928 Loss_G: 1.1953 D(x): 0.6250 D(G(z)): 0.4462 / 0.3186\n",
      "[4/5][30/95] Loss_D: 1.2279 Loss_G: 1.2458 D(x): 0.5619 D(G(z)): 0.4560 / 0.3069\n",
      "[4/5][31/95] Loss_D: 1.1569 Loss_G: 0.8159 D(x): 0.5168 D(G(z)): 0.3686 / 0.4707\n",
      "[4/5][32/95] Loss_D: 1.8114 Loss_G: 1.4788 D(x): 0.5660 D(G(z)): 0.6883 / 0.2459\n",
      "[4/5][33/95] Loss_D: 1.6460 Loss_G: 0.4424 D(x): 0.3251 D(G(z)): 0.3235 / 0.6572\n",
      "[4/5][34/95] Loss_D: 1.3771 Loss_G: 2.1344 D(x): 0.8034 D(G(z)): 0.6673 / 0.1296\n",
      "[4/5][35/95] Loss_D: 1.9283 Loss_G: 0.3541 D(x): 0.1917 D(G(z)): 0.1859 / 0.7097\n",
      "[4/5][36/95] Loss_D: 2.0008 Loss_G: 1.3209 D(x): 0.6974 D(G(z)): 0.7825 / 0.2753\n",
      "[4/5][37/95] Loss_D: 1.4708 Loss_G: 0.9808 D(x): 0.3517 D(G(z)): 0.3097 / 0.3821\n",
      "[4/5][38/95] Loss_D: 1.5433 Loss_G: 0.9530 D(x): 0.5305 D(G(z)): 0.5785 / 0.3906\n",
      "[4/5][39/95] Loss_D: 1.0815 Loss_G: 2.1645 D(x): 0.6801 D(G(z)): 0.4784 / 0.1297\n",
      "[4/5][40/95] Loss_D: 1.8680 Loss_G: 0.5269 D(x): 0.2138 D(G(z)): 0.1644 / 0.5940\n",
      "[4/5][41/95] Loss_D: 1.9798 Loss_G: 0.3195 D(x): 0.4244 D(G(z)): 0.6565 / 0.7322\n",
      "[4/5][42/95] Loss_D: 1.6516 Loss_G: 2.0683 D(x): 0.7753 D(G(z)): 0.7446 / 0.1524\n",
      "[4/5][43/95] Loss_D: 1.5710 Loss_G: 0.9601 D(x): 0.2891 D(G(z)): 0.1610 / 0.4020\n",
      "[4/5][44/95] Loss_D: 1.0138 Loss_G: 1.2910 D(x): 0.7916 D(G(z)): 0.5124 / 0.3043\n",
      "[4/5][45/95] Loss_D: 1.1800 Loss_G: 1.2411 D(x): 0.6276 D(G(z)): 0.4544 / 0.3133\n",
      "[4/5][46/95] Loss_D: 0.8249 Loss_G: 1.9150 D(x): 0.7478 D(G(z)): 0.3954 / 0.1617\n",
      "[4/5][47/95] Loss_D: 1.7451 Loss_G: 0.4198 D(x): 0.2767 D(G(z)): 0.2943 / 0.6657\n",
      "[4/5][48/95] Loss_D: 1.9592 Loss_G: 0.9708 D(x): 0.6511 D(G(z)): 0.7520 / 0.3992\n",
      "[4/5][49/95] Loss_D: 1.4499 Loss_G: 1.1728 D(x): 0.4229 D(G(z)): 0.4026 / 0.3283\n",
      "[4/5][50/95] Loss_D: 1.3953 Loss_G: 0.9326 D(x): 0.5013 D(G(z)): 0.4581 / 0.4057\n",
      "[4/5][51/95] Loss_D: 1.3582 Loss_G: 1.0054 D(x): 0.5358 D(G(z)): 0.4953 / 0.3723\n",
      "[4/5][52/95] Loss_D: 1.3299 Loss_G: 0.8313 D(x): 0.4817 D(G(z)): 0.4272 / 0.4450\n",
      "[4/5][53/95] Loss_D: 1.6182 Loss_G: 0.6149 D(x): 0.4894 D(G(z)): 0.5817 / 0.5442\n",
      "[4/5][54/95] Loss_D: 1.3553 Loss_G: 1.1397 D(x): 0.6361 D(G(z)): 0.5867 / 0.3321\n",
      "[4/5][55/95] Loss_D: 1.7006 Loss_G: 0.5204 D(x): 0.2918 D(G(z)): 0.3227 / 0.5988\n",
      "[4/5][56/95] Loss_D: 1.0258 Loss_G: 1.5425 D(x): 0.9139 D(G(z)): 0.5944 / 0.2253\n",
      "[4/5][57/95] Loss_D: 1.5679 Loss_G: 0.7317 D(x): 0.3178 D(G(z)): 0.3025 / 0.4900\n",
      "[4/5][58/95] Loss_D: 1.4703 Loss_G: 0.6358 D(x): 0.5483 D(G(z)): 0.5630 / 0.5380\n",
      "[4/5][59/95] Loss_D: 1.0832 Loss_G: 1.8412 D(x): 0.8860 D(G(z)): 0.5989 / 0.1758\n",
      "[4/5][60/95] Loss_D: 1.8052 Loss_G: 0.7842 D(x): 0.2309 D(G(z)): 0.2037 / 0.4736\n",
      "[4/5][61/95] Loss_D: 1.5157 Loss_G: 0.7065 D(x): 0.6206 D(G(z)): 0.6198 / 0.5114\n",
      "[4/5][62/95] Loss_D: 1.4845 Loss_G: 1.1866 D(x): 0.5863 D(G(z)): 0.5851 / 0.3179\n",
      "[4/5][63/95] Loss_D: 1.3249 Loss_G: 0.8164 D(x): 0.4120 D(G(z)): 0.3273 / 0.4589\n",
      "[4/5][64/95] Loss_D: 1.2336 Loss_G: 1.1161 D(x): 0.6522 D(G(z)): 0.5291 / 0.3415\n",
      "[4/5][65/95] Loss_D: 0.7980 Loss_G: 1.9640 D(x): 0.7405 D(G(z)): 0.3786 / 0.1522\n",
      "[4/5][66/95] Loss_D: 2.1118 Loss_G: 0.3742 D(x): 0.1689 D(G(z)): 0.1750 / 0.6946\n",
      "[4/5][67/95] Loss_D: 1.6627 Loss_G: 0.8026 D(x): 0.7327 D(G(z)): 0.7182 / 0.4587\n",
      "[4/5][68/95] Loss_D: 1.4021 Loss_G: 1.3794 D(x): 0.5465 D(G(z)): 0.5372 / 0.2608\n",
      "[4/5][69/95] Loss_D: 1.4265 Loss_G: 1.0331 D(x): 0.4034 D(G(z)): 0.3396 / 0.3644\n",
      "[4/5][70/95] Loss_D: 1.5113 Loss_G: 0.6280 D(x): 0.4142 D(G(z)): 0.4278 / 0.5376\n",
      "[4/5][71/95] Loss_D: 1.3817 Loss_G: 0.9357 D(x): 0.5778 D(G(z)): 0.5529 / 0.3991\n",
      "[4/5][72/95] Loss_D: 1.1385 Loss_G: 1.2025 D(x): 0.5810 D(G(z)): 0.4320 / 0.3081\n",
      "[4/5][73/95] Loss_D: 1.4822 Loss_G: 0.5272 D(x): 0.3595 D(G(z)): 0.3406 / 0.5948\n",
      "[4/5][74/95] Loss_D: 1.4496 Loss_G: 1.1780 D(x): 0.6670 D(G(z)): 0.6368 / 0.3190\n",
      "[4/5][75/95] Loss_D: 0.9626 Loss_G: 1.6130 D(x): 0.6300 D(G(z)): 0.3835 / 0.2155\n",
      "[4/5][76/95] Loss_D: 1.3906 Loss_G: 0.6182 D(x): 0.3891 D(G(z)): 0.2995 / 0.5441\n",
      "[4/5][77/95] Loss_D: 1.3092 Loss_G: 1.0696 D(x): 0.7367 D(G(z)): 0.6258 / 0.3569\n",
      "[4/5][78/95] Loss_D: 1.2696 Loss_G: 1.1188 D(x): 0.5252 D(G(z)): 0.4445 / 0.3384\n",
      "[4/5][79/95] Loss_D: 1.7468 Loss_G: 0.5321 D(x): 0.3216 D(G(z)): 0.4284 / 0.5939\n",
      "[4/5][80/95] Loss_D: 1.4196 Loss_G: 1.3493 D(x): 0.8064 D(G(z)): 0.6908 / 0.2802\n",
      "[4/5][81/95] Loss_D: 1.0496 Loss_G: 1.3850 D(x): 0.5154 D(G(z)): 0.2951 / 0.2663\n",
      "[4/5][82/95] Loss_D: 1.4505 Loss_G: 0.5656 D(x): 0.3690 D(G(z)): 0.3094 / 0.5861\n",
      "[4/5][83/95] Loss_D: 1.4634 Loss_G: 0.9969 D(x): 0.7414 D(G(z)): 0.6461 / 0.3909\n",
      "[4/5][84/95] Loss_D: 1.1129 Loss_G: 1.3047 D(x): 0.5877 D(G(z)): 0.4093 / 0.2825\n",
      "[4/5][85/95] Loss_D: 1.6587 Loss_G: 0.7766 D(x): 0.3593 D(G(z)): 0.3907 / 0.4764\n",
      "[4/5][86/95] Loss_D: 1.5692 Loss_G: 1.0397 D(x): 0.5603 D(G(z)): 0.5849 / 0.3671\n",
      "[4/5][87/95] Loss_D: 1.4432 Loss_G: 0.9415 D(x): 0.4249 D(G(z)): 0.4206 / 0.4046\n",
      "[4/5][88/95] Loss_D: 1.5285 Loss_G: 0.7897 D(x): 0.4689 D(G(z)): 0.5198 / 0.4657\n",
      "[4/5][89/95] Loss_D: 1.4006 Loss_G: 1.0717 D(x): 0.5734 D(G(z)): 0.5558 / 0.3638\n",
      "[4/5][90/95] Loss_D: 0.8898 Loss_G: 1.6448 D(x): 0.6800 D(G(z)): 0.3802 / 0.2196\n",
      "[4/5][91/95] Loss_D: 1.6337 Loss_G: 0.5350 D(x): 0.3444 D(G(z)): 0.3531 / 0.6014\n",
      "[4/5][92/95] Loss_D: 1.5204 Loss_G: 0.7960 D(x): 0.6525 D(G(z)): 0.6414 / 0.4653\n",
      "[4/5][93/95] Loss_D: 1.4733 Loss_G: 1.1319 D(x): 0.5416 D(G(z)): 0.5449 / 0.3452\n",
      "[4/5][94/95] Loss_D: 1.2904 Loss_G: 0.8760 D(x): 0.4082 D(G(z)): 0.2985 / 0.4268\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(batchSize, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "for epoch in range(niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label, device=device)\n",
    "\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, niter, i, len(dataloader),\n",
    "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu,\n",
    "                    '%s/real_samples.png' % outf,\n",
    "                    normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.detach(),\n",
    "                    '%s/fake_samples_epoch_%03d.png' % (outf, epoch),\n",
    "                    normalize=True)\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (outf, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
