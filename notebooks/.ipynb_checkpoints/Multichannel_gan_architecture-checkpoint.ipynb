{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiwavelength GAN architecture\n",
    "Added by Shooby Oct 24th<br>\n",
    "Last edited: Oct 25th<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.functional as F\n",
    "from photutils import create_matching_kernel\n",
    "\n",
    "from astropy.convolution import convolve_fft as convolve\n",
    "import astropy.io.fits as pyfits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  1214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x122c12cd0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\") # If GPU then use \"cuda:0\"\n",
    "ngpu = 3 #number of GPUs to use \n",
    "ngf = 64\n",
    "ndf = 64\n",
    "workers = 8 #number of data loading workers\n",
    "batchSize = 64 #input batch size\n",
    "imageSize = 64 #the height / width of the input image to network\n",
    "\n",
    "nz = 16 #size of the latent z vector\n",
    "niter = 5 #number of epochs to train for\n",
    "lr = 0.001 #learning rate, default=0.0002\n",
    "beta1 = 0.7 #beta1 for adam. default=0.5\n",
    "outf='../outputs' #folder to output images and model checkpoints\n",
    "ngc = 7\n",
    "\n",
    "manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 41)\n",
      "(41, 41)\n",
      "torch.Size([1, 1, 41, 41])\n"
     ]
    }
   ],
   "source": [
    "from skimage.transform import downscale_local_mean\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "psf = pyfits.getdata('../psfs/psf_i.fits')\n",
    "psf = downscale_local_mean(psf,(3,3))\n",
    "psf = psf[7:-8,7:-8]#[22:-22,22:-22]\n",
    "\n",
    "\n",
    "psf_hsc = pyfits.getdata('../psfs/PSF_subaru_i.fits')\n",
    "psf_hsc = psf_hsc[0:41,1:42]\n",
    "#psf_hsc = zoom(psf_hsc, 3, order=1)\n",
    "print(psf_hsc.shape)\n",
    "\n",
    "kern = create_matching_kernel(psf,psf_hsc)\n",
    "print(kern.shape)\n",
    "\n",
    "psfh = np.repeat(kern[:,:, np.newaxis], 1, axis=2)\n",
    "psfh = np.repeat(psfh[:,:,:,np.newaxis],1,axis = 3)\n",
    "kernel = torch.Tensor(psfh)\n",
    "kernel = kernel.permute(2,3,0,1)\n",
    "kernel =  kernel.float()\n",
    "print(kernel.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from galaxy_hdf5loader import galaxydata\n",
    "\n",
    "dataset = galaxydata('../Sample.hdf5')\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize,shuffle=True, num_workers=int(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(7, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(ngc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "#if netD != '':\n",
    "#    netD.load_state_dict(torch.load(netD))\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shoobygen(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(7, 256, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): ConvTranspose2d(256, 512, kernel_size=(6, 6), stride=(3, 3), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), bias=False)\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(256, 7, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (10): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Shoobygen(nn.Module):\n",
    "\n",
    "    def __init__(self,ngpu):\n",
    "        super(Shoobygen, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(ngc, ngf * 4, 7, 2, 2, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            \n",
    "            nn.ConvTranspose2d( ngf*4, ngf * 8, 6, 3, 2,dilation=2, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 3, 2, 2, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "  \n",
    "            nn.ConvTranspose2d(ngf*4, ngc, 4, 1, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "            output1 = output[:,:,:,:]\n",
    "\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "            output1 = output[:,:,:,:]\n",
    "\n",
    "        return output1\n",
    "\n",
    "netS = Shoobygen(ngpu).to(device)\n",
    "netS.apply(weights_init)\n",
    "print(netS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5][0/50] Loss_D: 0.3343 Loss_G: 8.5060 D(x): 0.9744 D(G(z)): 0.0002 / 0.0012\n",
      "0\n",
      "[0/5][1/50] Loss_D: 6.3221 Loss_G: 24.7039 D(x): 1.0000 D(G(z)): 0.8170 / 0.0000\n",
      "[0/5][2/50] Loss_D: 0.0011 Loss_G: 23.4534 D(x): 0.9989 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][3/50] Loss_D: 1.3389 Loss_G: 15.0027 D(x): 0.8031 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][4/50] Loss_D: 0.3345 Loss_G: 8.8724 D(x): 0.9445 D(G(z)): 0.0019 / 0.0073\n",
      "[0/5][5/50] Loss_D: 1.1228 Loss_G: 10.2552 D(x): 0.8491 D(G(z)): 0.2546 / 0.0001\n",
      "[0/5][6/50] Loss_D: 0.0086 Loss_G: 11.8365 D(x): 0.9935 D(G(z)): 0.0014 / 0.0000\n",
      "[0/5][7/50] Loss_D: 0.0118 Loss_G: 8.4429 D(x): 0.9944 D(G(z)): 0.0055 / 0.0013\n",
      "[0/5][8/50] Loss_D: 4.6488 Loss_G: 16.7566 D(x): 0.9952 D(G(z)): 0.6667 / 0.0000\n",
      "[0/5][9/50] Loss_D: 0.2269 Loss_G: 18.0932 D(x): 0.9192 D(G(z)): 0.0118 / 0.0000\n",
      "[0/5][10/50] Loss_D: 1.3769 Loss_G: 14.3325 D(x): 0.7297 D(G(z)): 0.0001 / 0.0000\n",
      "[0/5][11/50] Loss_D: 0.5237 Loss_G: 7.3164 D(x): 0.9097 D(G(z)): 0.0436 / 0.0149\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-18cbeafe9441>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0merrD_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0merrD_fake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mD_G_z1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0merrD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrD_real\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0merrD_fake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerS = optim.Adam(netS.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "for epoch in range(niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label, device=device)\n",
    "        \n",
    "        real_cpu = real_cpu.float()\n",
    "        real_cp = real_cpu[:,3,:,:].reshape(batchSize,1,64,64)\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with resampled, lower res, noise added images\n",
    "        kernel = kernel.to(device)\n",
    "        im = real_cpu+0.25*torch.rand_like(real_cpu)\n",
    "        downsampled = F.upsample(im,scale_factor=1/3,mode='bilinear')\n",
    "        downsampled_resh = downsampled.view(-1,1,21,21)\n",
    "        img2 = F.conv2d(downsampled_resh, kernel,padding=int(((kernel.shape[3])-1)/2))\n",
    "        img = img2.view(-1,7,21,21)\n",
    "        img = img[:,:,:,:]\n",
    "        \n",
    "        fake = netS(img)\n",
    "        label.fill_(fake_label)\n",
    "        fd = fake.detach()\n",
    "        output = netD(fd.float())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netS.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerS.step()\n",
    "\n",
    "        \n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, niter, i, len(dataloader),\n",
    "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "            vutils.save_image(real_cp,'%s/real_samples.png' % outf, normalize=True)\n",
    "            fake = netS(img)\n",
    "            fak = fake[:,3,:,:].reshape(batchSize,1,66,66)\n",
    "            vutils.save_image(fak.detach(),'%s/fake_samples_epoch_%03d.png' % (outf, epoch),normalize=True)\n",
    "            grid = torchvision.utils.make_grid(fak.detach())\n",
    "            \n",
    "    # do checkpointing\n",
    "    torch.save(netS.state_dict(), '%s/netG_epoch_%d.pth' % (outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (outf, epoch))\n",
    "    stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
